wandb: Agent Starting Run: wuw8h1eg with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 2.0240935774795044e-05
wandb: 	lr: 4.440064380413456e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Currently logged in as: michaelzhao (michaelzhao-harvard-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_090717-wuw8h1eg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/wuw8h1eg
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–…â–…â–…â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ˆâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–
wandb:   best_epoch â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–‚â–ƒâ–…â–†â–‡â–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–
wandb:       val_f1 â–â–ˆâ–ˆâ–‡â–…â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.29315
wandb:   best_epoch 2
wandb:  best_val_f1 0.7285
wandb:        epoch 7
wandb:      test_f1 0.72417
wandb:  test_f1_few 0.72011
wandb: test_f1_zero 0.72781
wandb:   train_loss 0.33372
wandb:       val_f1 0.71765
wandb: 
wandb: ğŸš€ View run revived-sweep-1 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/wuw8h1eg
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_090717-wuw8h1eg/logs
wandb: Agent Starting Run: 9g3ejvcn with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 7.529313001274593e-05
wandb: 	lr: 6.6526567268354995e-06
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_091950-9g3ejvcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/9g3ejvcn
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–„â–†â–†â–ƒâ–‚â–ƒâ–‚â–ƒâ–…â–…â–‚â–…â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–ƒâ–ƒâ–ƒâ–‚â–â–â–â–„â–ƒâ–„â–‚â–‚â–‚â–â–â–â–â–‚â–â–ƒ
wandb:   best_epoch â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–…â–…â–ˆâ–…â–„â–†â–„â–â–†â–…â–„â–„â–„â–…â–…â–„â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.37597
wandb:   best_epoch 3
wandb:  best_val_f1 0.75428
wandb:        epoch 18
wandb:      test_f1 0.74735
wandb:  test_f1_few 0.74984
wandb: test_f1_zero 0.74449
wandb:   train_loss 0.15973
wandb:       val_f1 0.71341
wandb: 
wandb: ğŸš€ View run peachy-sweep-2 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/9g3ejvcn
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_091950-9g3ejvcn/logs
wandb: Agent Starting Run: xwkyfiy3 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 9.726919465451052e-05
wandb: 	lr: 2.083556190556011e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_095905-xwkyfiy3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xwkyfiy3
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–†â–…â–†â–ƒâ–…â–…â–„â–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–
wandb:       val_f1 â–â–ˆâ–ˆâ–†â–…â–†â–‡â–†â–†â–†â–†â–‡â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.07942
wandb:   best_epoch 3
wandb:  best_val_f1 0.74589
wandb:        epoch 13
wandb:      test_f1 0.74791
wandb:  test_f1_few 0.75269
wandb: test_f1_zero 0.74285
wandb:   train_loss 0.19356
wandb:       val_f1 0.70283
wandb: 
wandb: ğŸš€ View run woven-sweep-3 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xwkyfiy3
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_095905-xwkyfiy3/logs
wandb: Agent Starting Run: s21nzehj with config:
wandb: 	batch_size: 64
wandb: 	epochs: 25
wandb: 	l2_reg: 3.782367280491364e-05
wandb: 	lr: 3.6262635271367106e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_102008-s21nzehj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/s21nzehj
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–†â–„â–ƒâ–‚â–ƒâ–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–„â–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–â–ˆâ–ˆâ–‚â–†â–ˆâ–ˆâ–‡â–‡â–‡â–†â–ˆâ–†â–†â–‡â–†â–‡â–†
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.1959
wandb:   best_epoch 3
wandb:  best_val_f1 0.74378
wandb:        epoch 18
wandb:      test_f1 0.73943
wandb:  test_f1_few 0.73033
wandb: test_f1_zero 0.74895
wandb:   train_loss 0.15031
wandb:       val_f1 0.71455
wandb: 
wandb: ğŸš€ View run pleasant-sweep-4 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/s21nzehj
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_102008-s21nzehj/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: sfj5gm3z with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 5.8211225455219336e-05
wandb: 	lr: 8.265386647580895e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_105321-sfj5gm3z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/sfj5gm3z
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–†â–†â–…â–…â–…â–…â–„â–„â–ƒâ–‡â–„â–†â–„â–…â–„â–„â–…â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–‚â–‚â–â–ƒâ–â–â–‚â–…â–â–„â–‚â–ƒâ–ƒâ–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:       val_f1 â–â–†â–ˆâ–†â–„â–†â–…â–ƒâ–‡â–†â–…â–…â–„â–…â–ƒâ–…â–†â–†
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.27039
wandb:   best_epoch 3
wandb:  best_val_f1 0.7542
wandb:        epoch 18
wandb:      test_f1 0.75732
wandb:  test_f1_few 0.75176
wandb: test_f1_zero 0.76309
wandb:   train_loss 0.1728
wandb:       val_f1 0.73331
wandb: 
wandb: ğŸš€ View run dainty-sweep-5 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/sfj5gm3z
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_105321-sfj5gm3z/logs
wandb: Agent Starting Run: ehzlj8om with config:
wandb: 	batch_size: 64
wandb: 	epochs: 50
wandb: 	l2_reg: 1.8474002783055737e-05
wandb: 	lr: 4.061028479031612e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_112652-ehzlj8om
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ehzlj8om
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–†â–ˆâ–…â–„â–†â–„â–„â–„â–ƒâ–…â–ƒâ–„â–„â–„â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–â–„â–â–â–‚â–‚
wandb:   best_epoch â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–ˆâ–†â–ˆâ–â–†â–‡â–‡â–†â–„â–†â–…â–…â–…â–„â–…â–„â–…â–†
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.16938
wandb:   best_epoch 3
wandb:  best_val_f1 0.73561
wandb:        epoch 18
wandb:      test_f1 0.73453
wandb:  test_f1_few 0.73458
wandb: test_f1_zero 0.7345
wandb:   train_loss 0.14833
wandb:       val_f1 0.71414
wandb: 
wandb: ğŸš€ View run genial-sweep-6 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ehzlj8om
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_112652-ehzlj8om/logs
wandb: Agent Starting Run: 67k6m19f with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 4.006096466842862e-05
wandb: 	lr: 3.4500941010057065e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_115638-67k6m19f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/67k6m19f
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–…â–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–â–â–‚
wandb:   best_epoch â–â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:       val_f1 â–†â–†â–‡â–â–…â–ˆâ–‡â–†â–„â–…â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.29677
wandb:   best_epoch 6
wandb:  best_val_f1 0.74438
wandb:        epoch 11
wandb:      test_f1 0.73695
wandb:  test_f1_few 0.74375
wandb: test_f1_zero 0.72977
wandb:   train_loss 0.23925
wandb:       val_f1 0.71617
wandb: 
wandb: ğŸš€ View run glad-sweep-7 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/67k6m19f
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_115638-67k6m19f/logs
wandb: Agent Starting Run: ekae8wi5 with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 3.8216083619372326e-05
wandb: 	lr: 1.689152262594528e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_121605-ekae8wi5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ekae8wi5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–‡â–ˆâ–†â–†â–„â–…â–…â–„â–ƒâ–†â–ƒâ–†â–…â–ƒâ–ƒâ–…â–†â–ƒâ–…â–ƒâ–„â–‚â–…â–ƒâ–ƒâ–‚â–ƒâ–â–ˆâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–…â–‚
wandb:   best_epoch â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–„â–„â–ƒâ–‚â–‚â–â–
wandb:       val_f1 â–†â–…â–ˆâ–†â–…â–‡â–†â–
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.12985
wandb:   best_epoch 3
wandb:  best_val_f1 0.74398
wandb:        epoch 8
wandb:      test_f1 0.73726
wandb:  test_f1_few 0.74575
wandb: test_f1_zero 0.72823
wandb:   train_loss 0.28936
wandb:       val_f1 0.69194
wandb: 
wandb: ğŸš€ View run restful-sweep-8 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ekae8wi5
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_121605-ekae8wi5/logs
wandb: Agent Starting Run: 2i2i2ber with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 5.131904591420647e-05
wandb: 	lr: 1.6171069900857887e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_123208-2i2i2ber
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2i2i2ber
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–…â–†â–…â–„â–„â–„â–„â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–â–‡â–ˆâ–†â–…â–‡â–‡â–†â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–†â–‡
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.08432
wandb:   best_epoch 3
wandb:  best_val_f1 0.7467
wandb:        epoch 18
wandb:      test_f1 0.73684
wandb:  test_f1_few 0.73781
wandb: test_f1_zero 0.73593
wandb:   train_loss 0.15357
wandb:       val_f1 0.72711
wandb: 
wandb: ğŸš€ View run wild-sweep-9 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2i2i2ber
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_123208-2i2i2ber/logs
wandb: Agent Starting Run: 6l4s07pl with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 5.462003399331975e-05
wandb: 	lr: 1.7367209525932223e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_130301-6l4s07pl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6l4s07pl
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–„â–„â–…â–„â–ƒâ–ƒâ–„â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–„â–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–â–â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚
wandb:   best_epoch â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–…â–…â–ˆâ–„â–ƒâ–…â–„â–â–„â–„â–ƒâ–ƒâ–‚â–â–ƒâ–ƒâ–„â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.22213
wandb:   best_epoch 3
wandb:  best_val_f1 0.7557
wandb:        epoch 18
wandb:      test_f1 0.746
wandb:  test_f1_few 0.74842
wandb: test_f1_zero 0.74346
wandb:   train_loss 0.14728
wandb:       val_f1 0.72654
wandb: 
wandb: ğŸš€ View run swift-sweep-10 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6l4s07pl
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_130301-6l4s07pl/logs
wandb: Agent Starting Run: i82u4j0n with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 4.034038569205083e-05
wandb: 	lr: 8.240720456484983e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_133441-i82u4j0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i82u4j0n
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–‡â–ˆâ–†â–„â–„â–†â–„â–…â–…â–â–‚â–‚â–ƒâ–„â–‚â–ƒâ–â–‚â–â–â–‚â–„â–ƒâ–â–‚â–‚â–„â–‚â–â–‚â–ƒâ–â–‚â–‚â–â–‚â–â–â–‚â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:       val_f1 â–â–†â–ˆâ–…â–…â–†â–ƒâ–ƒâ–†â–†â–…â–…â–…â–…â–…â–…â–„â–†
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.15989
wandb:   best_epoch 3
wandb:  best_val_f1 0.76501
wandb:        epoch 18
wandb:      test_f1 0.75138
wandb:  test_f1_few 0.75661
wandb: test_f1_zero 0.74558
wandb:   train_loss 0.18319
wandb:       val_f1 0.73744
wandb: 
wandb: ğŸš€ View run winter-sweep-11 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i82u4j0n
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_133441-i82u4j0n/logs
wandb: Agent Starting Run: xl3a8qk8 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 5.582900695782314e-05
wandb: 	lr: 5.681025442160858e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_140824-xl3a8qk8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xl3a8qk8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–†â–ˆâ–‡â–†â–ˆâ–…â–†â–†â–‡â–…â–„â–„â–…â–…â–…â–…â–ƒâ–„â–„â–…â–„â–ƒâ–‚â–…â–„â–‚â–„â–„â–ƒâ–…â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–â–ƒâ–‚â–
wandb:   best_epoch â–â–‚â–ƒâ–ƒâ–ƒâ–…â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.25077
wandb:   best_epoch 10
wandb:  best_val_f1 0.74294
wandb:        epoch 25
wandb:      test_f1 0.72233
wandb:  test_f1_few 0.71785
wandb: test_f1_zero 0.72717
wandb:   train_loss 0.23483
wandb:       val_f1 0.72647
wandb: 
wandb: ğŸš€ View run lively-sweep-12 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xl3a8qk8
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_140824-xl3a8qk8/logs
wandb: Agent Starting Run: xoglizmo with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 3.0566695176830026e-05
wandb: 	lr: 1.579341769330365e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_144942-xoglizmo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xoglizmo
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ƒâ–ƒâ–ƒâ–‚â–ˆâ–‚â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–ƒâ–â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–„â–‚â–‚â–
wandb:   best_epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       val_f1 â–‡â–†â–…â–ƒâ–„â–†â–ƒâ–â–„â–„â–…â–†â–†â–‡â–†â–„â–ˆâ–†â–…â–‚â–„â–†â–…â–„â–…â–†â–†â–„â–†â–„â–†â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.28655
wandb:   best_epoch 17
wandb:  best_val_f1 0.73749
wandb:        epoch 32
wandb:      test_f1 0.73017
wandb:  test_f1_few 0.74674
wandb: test_f1_zero 0.71242
wandb:   train_loss 0.10779
wandb:       val_f1 0.71305
wandb: 
wandb: ğŸš€ View run graceful-sweep-13 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xoglizmo
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_144942-xoglizmo/logs
wandb: Agent Starting Run: xvlyo502 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 3.434774207140354e-05
wandb: 	lr: 5.518688732730609e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_155331-xvlyo502
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xvlyo502
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–‡â–ˆâ–‡â–„â–…â–…â–ƒâ–…â–„â–ƒâ–„â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–â–ƒâ–â–â–‚â–ƒâ–‚â–â–â–‚â–â–
wandb:   best_epoch â–â–‚â–‚â–‚â–„â–…â–…â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:       val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.25793
wandb:   best_epoch 11
wandb:  best_val_f1 0.73602
wandb:        epoch 26
wandb:      test_f1 0.7409
wandb:  test_f1_few 0.73634
wandb: test_f1_zero 0.74588
wandb:   train_loss 0.24726
wandb:       val_f1 0.7203
wandb: 
wandb: ğŸš€ View run faithful-sweep-14 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xvlyo502
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_155331-xvlyo502/logs
wandb: Agent Starting Run: i3e1mxan with config:
wandb: 	batch_size: 4
wandb: 	epochs: 75
wandb: 	l2_reg: 7.796991299028495e-05
wandb: 	lr: 5.032678327176544e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_163400-i3e1mxan
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i3e1mxan
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–„â–ˆâ–†â–ƒâ–„â–„â–†â–‚â–‚â–‚â–‚â–„â–‚â–ƒâ–â–„â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–ƒâ–†â–â–…â–‚â–ƒâ–ƒâ–â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–†â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–…â–†â–ˆâ–‡â–…â–†â–â–…â–†â–…â–ƒâ–„â–„â–â–‚â–„â–…â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.00304
wandb:   best_epoch 3
wandb:  best_val_f1 0.74274
wandb:        epoch 18
wandb:      test_f1 0.74466
wandb:  test_f1_few 0.74421
wandb: test_f1_zero 0.74516
wandb:   train_loss 0.16937
wandb:       val_f1 0.71599
wandb: 
wandb: ğŸš€ View run usual-sweep-15 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i3e1mxan
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_163400-i3e1mxan/logs
wandb: Agent Starting Run: g6uirg6b with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 3.208310278659775e-05
wandb: 	lr: 6.399862485604724e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_171701-g6uirg6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g6uirg6b
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–†â–†â–„â–„â–„â–„â–„â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–‚
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–â–ˆâ–ˆâ–†â–†â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.40157
wandb:   best_epoch 3
wandb:  best_val_f1 0.74387
wandb:        epoch 18
wandb:      test_f1 0.74374
wandb:  test_f1_few 0.73859
wandb: test_f1_zero 0.74918
wandb:   train_loss 0.24078
wandb:       val_f1 0.73338
wandb: 
wandb: ğŸš€ View run dry-sweep-16 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g6uirg6b
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_171701-g6uirg6b/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: l2vst00s with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 4.64787936312592e-05
wandb: 	lr: 5.723049692144186e-06
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_174805-l2vst00s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/l2vst00s
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–…â–…â–†â–„â–…â–ƒâ–„â–…â–„â–ƒâ–„â–‚â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–â–‚â–‚â–„â–ƒâ–â–‚â–‚â–ƒâ–‚â–â–â–â–…â–‚
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–â–†â–ˆâ–‡â–…â–‡â–†â–…â–‡â–ˆâ–†â–‡â–†â–‡â–†â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.14279
wandb:   best_epoch 3
wandb:  best_val_f1 0.74635
wandb:        epoch 18
wandb:      test_f1 0.74927
wandb:  test_f1_few 0.74589
wandb: test_f1_zero 0.75275
wandb:   train_loss 0.20108
wandb:       val_f1 0.7293
wandb: 
wandb: ğŸš€ View run sweepy-sweep-17 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/l2vst00s
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_174805-l2vst00s/logs
wandb: Agent Starting Run: rprj8dsz with config:
wandb: 	batch_size: 64
wandb: 	epochs: 50
wandb: 	l2_reg: 7.100413728874671e-05
wandb: 	lr: 9.797141633819474e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_182346-rprj8dsz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/rprj8dsz
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–„â–…â–„â–ƒâ–„â–„â–„â–…â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–â–„â–ˆâ–‚â–ˆâ–‡â–†â–…â–†â–‡â–†â–†â–…â–„â–…â–ƒâ–…â–…
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.30201
wandb:   best_epoch 3
wandb:  best_val_f1 0.7378
wandb:        epoch 18
wandb:      test_f1 0.74097
wandb:  test_f1_few 0.73524
wandb: test_f1_zero 0.7472
wandb:   train_loss 0.23071
wandb:       val_f1 0.71666
wandb: 
wandb: ğŸš€ View run fragrant-sweep-18 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/rprj8dsz
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_182346-rprj8dsz/logs
wandb: Agent Starting Run: ienkldp4 with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 6.944414012776496e-05
wandb: 	lr: 5.182462246759728e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_185159-ienkldp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ienkldp4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–‡â–†â–„â–…â–†â–ƒâ–…â–…â–„â–…â–„â–„â–…â–ƒâ–„â–„â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚â–ƒâ–ƒâ–â–â–â–â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:       val_f1 â–â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.24065
wandb:   best_epoch 3
wandb:  best_val_f1 0.74842
wandb:        epoch 18
wandb:      test_f1 0.74976
wandb:  test_f1_few 0.74465
wandb: test_f1_zero 0.75507
wandb:   train_loss 0.21789
wandb:       val_f1 0.73011
wandb: 
wandb: ğŸš€ View run giddy-sweep-19 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ienkldp4
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_185159-ienkldp4/logs
wandb: Agent Starting Run: 63pnnuzy with config:
wandb: 	batch_size: 4
wandb: 	epochs: 25
wandb: 	l2_reg: 9.308580747959692e-05
wandb: 	lr: 4.985979820315001e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_192548-63pnnuzy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/63pnnuzy
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ƒâ–ƒâ–‚â–†â–‚â–ƒâ–ƒâ–â–„â–„â–„â–ˆâ–„â–„â–„â–â–„â–ƒâ–ƒâ–†â–â–„â–‚â–ƒâ–„â–â–â–â–â–ƒâ–â–„â–ƒâ–â–„â–„â–†â–„â–â–‚
wandb:   best_epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  best_val_f1 â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–‡â–„â–…â–„â–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚
wandb:       val_f1 â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.96266
wandb:   best_epoch 1
wandb:  best_val_f1 0.16606
wandb:        epoch 16
wandb:      test_f1 0.16865
wandb:  test_f1_few 0.16972
wandb: test_f1_zero 0.16752
wandb:   train_loss 1.04802
wandb:       val_f1 0.16606
wandb: 
wandb: ğŸš€ View run fast-sweep-20 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/63pnnuzy
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_192548-63pnnuzy/logs
wandb: Agent Starting Run: f302p7lu with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 1.437528295315638e-05
wandb: 	lr: 1.6351942547645442e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_201437-f302p7lu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f302p7lu
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–ˆâ–…â–†â–„â–„â–„â–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–â–
wandb:   best_epoch â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–
wandb:       val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.26926
wandb:   best_epoch 3
wandb:  best_val_f1 0.7449
wandb:        epoch 13
wandb:      test_f1 0.74517
wandb:  test_f1_few 0.73972
wandb: test_f1_zero 0.75087
wandb:   train_loss 0.27633
wandb:       val_f1 0.70995
wandb: 
wandb: ğŸš€ View run sweet-sweep-21 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f302p7lu
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_201437-f302p7lu/logs
wandb: Agent Starting Run: 6e0117fm with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 4.434033172753466e-05
wandb: 	lr: 8.12936395143613e-06
wandb: 	n_layers_freeze: 1
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_203554-6e0117fm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6e0117fm
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:   batch_loss â–ˆâ–†â–…â–…â–„â–„â–…â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–â–‚â–
wandb:   best_epoch â–â–‚â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  best_val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:        epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:      test_f1 â–
wandb:  test_f1_few â–
wandb: test_f1_zero â–
wandb:   train_loss â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:       val_f1 â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:   batch_loss 0.18663
wandb:   best_epoch 6
wandb:  best_val_f1 0.7453
wandb:        epoch 21
wandb:      test_f1 0.73234
wandb:  test_f1_few 0.73458
wandb: test_f1_zero 0.72999
wandb:   train_loss 0.2515
wandb:       val_f1 0.73082
wandb: 
wandb: ğŸš€ View run dutiful-sweep-22 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6e0117fm
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_203554-6e0117fm/logs
wandb: Agent Starting Run: 6zlv40l6 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 1.3553048023825074e-05
wandb: 	lr: 5.389384726339378e-06
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211344-6zlv40l6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6zlv40l6
wandb:                                                                                
wandb: ğŸš€ View run snowy-sweep-23 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6zlv40l6
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211344-6zlv40l6/logs
Run 6zlv40l6 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 36, in sweep_train
    engine.train()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 74, in train
    train_loss = self.train_epoch()
                 ^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 130, in train_epoch
    logits = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    return self.module(*inputs[0], **module_kwargs[0])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_models.py", line 86, in forward
    outputs = self.bert(input_ids=input_ids,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 544, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 334, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 293, in forward
    self_attention_outputs = self.attention(
                             ^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 234, in forward
    self_outputs = self.attn(
                   ^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 183, in forward
    attention_probs = self.dropout(attention_probs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 330.62 MiB is free. Including non-PyTorch memory, this process has 78.76 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 136.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6zlv40l6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 36, in sweep_train
wandb: ERROR     engine.train()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 74, in train
wandb: ERROR     train_loss = self.train_epoch()
wandb: ERROR                  ^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 130, in train_epoch
wandb: ERROR     logits = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
wandb: ERROR              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
wandb: ERROR     return self.module(*inputs[0], **module_kwargs[0])
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_models.py", line 86, in forward
wandb: ERROR     outputs = self.bert(input_ids=input_ids,
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 544, in forward
wandb: ERROR     encoder_outputs = self.encoder(
wandb: ERROR                       ^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 334, in forward
wandb: ERROR     layer_outputs = layer_module(
wandb: ERROR                     ^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 293, in forward
wandb: ERROR     self_attention_outputs = self.attention(
wandb: ERROR                              ^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 234, in forward
wandb: ERROR     self_outputs = self.attn(
wandb: ERROR                    ^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/transformers/models/mpnet/modeling_mpnet.py", line 183, in forward
wandb: ERROR     attention_probs = self.dropout(attention_probs)
wandb: ERROR                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/dropout.py", line 59, in forward
wandb: ERROR     return F.dropout(input, self.p, self.training, self.inplace)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/functional.py", line 1295, in dropout
wandb: ERROR     return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
wandb: ERROR                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 330.62 MiB is free. Including non-PyTorch memory, this process has 78.76 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 136.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: nfvlt0lv with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 5.378536752028828e-05
wandb: 	lr: 2.4824335680138062e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211414-nfvlt0lv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/nfvlt0lv
wandb:                                                                                
wandb: ğŸš€ View run warm-sweep-24 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/nfvlt0lv
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211414-nfvlt0lv/logs
Run nfvlt0lv errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 36, in sweep_train
    engine.train()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 71, in train
    best_state_dict = copy.deepcopy(self.model.state_dict())
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 162, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 285, in _reconstruct
    value = deepcopy(value, memo)
            ^^^^^^^^^^^^^^^^^^^^^
  File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 143, in deepcopy
    y = copier(memo)
        ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/_tensor.py", line 123, in __deepcopy__
    new_storage = self._typed_storage()._deepcopy(memo)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 908, in _deepcopy
    return self._new_wrapped_storage(copy.deepcopy(self._untyped_storage, memo))
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 143, in deepcopy
    y = copier(memo)
        ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 144, in __deepcopy__
    new_storage = self.clone()
                  ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 158, in clone
    return type(self)(self.nbytes(), device=self.device).copy_(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nfvlt0lv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 36, in sweep_train
wandb: ERROR     engine.train()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 71, in train
wandb: ERROR     best_state_dict = copy.deepcopy(self.model.state_dict())
wandb: ERROR                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 162, in deepcopy
wandb: ERROR     y = _reconstruct(x, memo, *rv)
wandb: ERROR         ^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 285, in _reconstruct
wandb: ERROR     value = deepcopy(value, memo)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 143, in deepcopy
wandb: ERROR     y = copier(memo)
wandb: ERROR         ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/_tensor.py", line 123, in __deepcopy__
wandb: ERROR     new_storage = self._typed_storage()._deepcopy(memo)
wandb: ERROR                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 908, in _deepcopy
wandb: ERROR     return self._new_wrapped_storage(copy.deepcopy(self._untyped_storage, memo))
wandb: ERROR                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/sw/Miniforge3-24.7.1-0/lib/python3.12/copy.py", line 143, in deepcopy
wandb: ERROR     y = copier(memo)
wandb: ERROR         ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 144, in __deepcopy__
wandb: ERROR     new_storage = self.clone()
wandb: ERROR                   ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/storage.py", line 158, in clone
wandb: ERROR     return type(self)(self.nbytes(), device=self.device).copy_(self)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lvosnkee with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 8.534944535288185e-05
wandb: 	lr: 7.310939918407751e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211435-lvosnkee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lvosnkee
wandb:                                                                                
wandb: ğŸš€ View run dutiful-sweep-25 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lvosnkee
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211435-lvosnkee/logs
Run lvosnkee errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lvosnkee errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9q0bg5jd with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 3.5083475746921346e-05
wandb: 	lr: 7.506492088838504e-06
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211456-9q0bg5jd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/9q0bg5jd
wandb:                                                                                
wandb: ğŸš€ View run genial-sweep-26 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/9q0bg5jd
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211456-9q0bg5jd/logs
Run 9q0bg5jd errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9q0bg5jd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vyjqh1dg with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 5.453762379514319e-05
wandb: 	lr: 7.76086292392101e-06
wandb: 	n_layers_freeze: 1
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211517-vyjqh1dg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/vyjqh1dg
wandb:                                                                                
wandb: ğŸš€ View run daily-sweep-27 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/vyjqh1dg
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211517-vyjqh1dg/logs
Run vyjqh1dg errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vyjqh1dg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gf0suy08 with config:
wandb: 	batch_size: 16
wandb: 	epochs: 25
wandb: 	l2_reg: 2.7745588704973584e-05
wandb: 	lr: 1.631123887121325e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211537-gf0suy08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/gf0suy08
wandb:                                                                                
wandb: ğŸš€ View run fallen-sweep-28 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/gf0suy08
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211537-gf0suy08/logs
Run gf0suy08 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gf0suy08 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: o4mq35ix with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 5.412874318000286e-05
wandb: 	lr: 1.6972231469040918e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211558-o4mq35ix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/o4mq35ix
wandb:                                                                                
wandb: ğŸš€ View run polar-sweep-29 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/o4mq35ix
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211558-o4mq35ix/logs
Run o4mq35ix errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run o4mq35ix errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 79kc457s with config:
wandb: 	batch_size: 32
wandb: 	epochs: 25
wandb: 	l2_reg: 9.088765133422395e-05
wandb: 	lr: 3.8240681946957705e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211618-79kc457s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/79kc457s
wandb:                                                                                
wandb: ğŸš€ View run fresh-sweep-30 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/79kc457s
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211618-79kc457s/logs
Run 79kc457s errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 79kc457s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qpwcg5kh with config:
wandb: 	batch_size: 4
wandb: 	epochs: 50
wandb: 	l2_reg: 6.657256690053658e-05
wandb: 	lr: 5.521209579453673e-06
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211639-qpwcg5kh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/qpwcg5kh
wandb:                                                                                
wandb: ğŸš€ View run lyric-sweep-31 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/qpwcg5kh
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211639-qpwcg5kh/logs
Run qpwcg5kh errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qpwcg5kh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: u7khoy2j with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 9.965958906012944e-05
wandb: 	lr: 3.979281687067081e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211700-u7khoy2j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/u7khoy2j
wandb:                                                                                
wandb: ğŸš€ View run wobbly-sweep-32 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/u7khoy2j
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211700-u7khoy2j/logs
Run u7khoy2j errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run u7khoy2j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: onelj2ul with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 8.643213318824036e-05
wandb: 	lr: 2.252259298186523e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211721-onelj2ul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/onelj2ul
wandb:                                                                                
wandb: ğŸš€ View run legendary-sweep-33 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/onelj2ul
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211721-onelj2ul/logs
Run onelj2ul errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run onelj2ul errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7s6clwl1 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 50
wandb: 	l2_reg: 9.09951323520668e-05
wandb: 	lr: 2.6027074095902408e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211741-7s6clwl1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/7s6clwl1
wandb:                                                                                
wandb: ğŸš€ View run glamorous-sweep-34 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/7s6clwl1
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211741-7s6clwl1/logs
Run 7s6clwl1 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7s6clwl1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: g4s3x3b1 with config:
wandb: 	batch_size: 4
wandb: 	epochs: 50
wandb: 	l2_reg: 2.0087139097913457e-05
wandb: 	lr: 2.9697840259034332e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211802-g4s3x3b1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g4s3x3b1
wandb:                                                                                
wandb: ğŸš€ View run sweet-sweep-35 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g4s3x3b1
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211802-g4s3x3b1/logs
Run g4s3x3b1 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run g4s3x3b1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4gfeteru with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 3.769127702547605e-05
wandb: 	lr: 2.4204228134810827e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211822-4gfeteru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/4gfeteru
wandb:                                                                                
wandb: ğŸš€ View run prime-sweep-36 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/4gfeteru
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211822-4gfeteru/logs
Run 4gfeteru errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4gfeteru errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4qsi0r6r with config:
wandb: 	batch_size: 4
wandb: 	epochs: 50
wandb: 	l2_reg: 7.525122728956934e-05
wandb: 	lr: 4.094089037262557e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211843-4qsi0r6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/4qsi0r6r
wandb:                                                                                
wandb: ğŸš€ View run honest-sweep-37 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/4qsi0r6r
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211843-4qsi0r6r/logs
Run 4qsi0r6r errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4qsi0r6r errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: t5wue4n7 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 8.20548967071845e-05
wandb: 	lr: 1.3105694749699105e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211904-t5wue4n7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/t5wue4n7
wandb:                                                                                
wandb: ğŸš€ View run flowing-sweep-38 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/t5wue4n7
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211904-t5wue4n7/logs
Run t5wue4n7 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run t5wue4n7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: q96tvkeg with config:
wandb: 	batch_size: 16
wandb: 	epochs: 25
wandb: 	l2_reg: 8.110277182898062e-05
wandb: 	lr: 4.263459496235173e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211924-q96tvkeg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/q96tvkeg
wandb:                                                                                
wandb: ğŸš€ View run easy-sweep-39 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/q96tvkeg
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211924-q96tvkeg/logs
Run q96tvkeg errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run q96tvkeg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ufp6hcv0 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 5.49728991983748e-05
wandb: 	lr: 2.7942188131684996e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_211945-ufp6hcv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-40
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ufp6hcv0
wandb:                                                                                
wandb: ğŸš€ View run dainty-sweep-40 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ufp6hcv0
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_211945-ufp6hcv0/logs
Run ufp6hcv0 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ufp6hcv0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bkcsn923 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 7.988976927140796e-05
wandb: 	lr: 2.9824556762008744e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212006-bkcsn923
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-41
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bkcsn923
wandb:                                                                                
wandb: ğŸš€ View run rosy-sweep-41 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bkcsn923
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212006-bkcsn923/logs
Run bkcsn923 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bkcsn923 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ma91fryt with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 2.0995160159668884e-05
wandb: 	lr: 7.112374665834755e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212027-ma91fryt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-42
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ma91fryt
wandb:                                                                                
wandb: ğŸš€ View run pious-sweep-42 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ma91fryt
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212027-ma91fryt/logs
Run ma91fryt errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ma91fryt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1g3biqh2 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 50
wandb: 	l2_reg: 8.892858314363998e-05
wandb: 	lr: 1.5450024353488782e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212047-1g3biqh2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-43
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/1g3biqh2
wandb:                                                                                
wandb: ğŸš€ View run leafy-sweep-43 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/1g3biqh2
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212047-1g3biqh2/logs
Run 1g3biqh2 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1g3biqh2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: db24oj9n with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 5.3996770466089185e-05
wandb: 	lr: 2.861624739920091e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212108-db24oj9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-44
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/db24oj9n
wandb:                                                                                
wandb: ğŸš€ View run fragrant-sweep-44 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/db24oj9n
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212108-db24oj9n/logs
Run db24oj9n errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run db24oj9n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8xz0s6xl with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 9.442934479064562e-05
wandb: 	lr: 4.2697383315439666e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212129-8xz0s6xl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-45
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8xz0s6xl
wandb:                                                                                
wandb: ğŸš€ View run firm-sweep-45 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8xz0s6xl
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212129-8xz0s6xl/logs
Run 8xz0s6xl errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8xz0s6xl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xrm2qx15 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 8.762499551841136e-05
wandb: 	lr: 3.873719435945431e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212149-xrm2qx15
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-46
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xrm2qx15
wandb:                                                                                
wandb: ğŸš€ View run dark-sweep-46 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xrm2qx15
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212149-xrm2qx15/logs
Run xrm2qx15 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xrm2qx15 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f1wqutb6 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 9.508209731411314e-05
wandb: 	lr: 3.7320858029099616e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212210-f1wqutb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-47
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f1wqutb6
wandb:                                                                                
wandb: ğŸš€ View run sage-sweep-47 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f1wqutb6
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212210-f1wqutb6/logs
Run f1wqutb6 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f1wqutb6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 60sor7j6 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 7.154757717697567e-05
wandb: 	lr: 2.0023394109685145e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212231-60sor7j6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-48
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/60sor7j6
wandb:                                                                                
wandb: ğŸš€ View run sweepy-sweep-48 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/60sor7j6
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212231-60sor7j6/logs
Run 60sor7j6 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 60sor7j6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a40qubxp with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 3.458657117781888e-05
wandb: 	lr: 2.2277222082437015e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212251-a40qubxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-49
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/a40qubxp
wandb:                                                                                
wandb: ğŸš€ View run true-sweep-49 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/a40qubxp
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212251-a40qubxp/logs
Run a40qubxp errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a40qubxp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6j8f1u3j with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 6.4516314257963e-05
wandb: 	lr: 4.0302538765320024e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212312-6j8f1u3j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-50
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6j8f1u3j
wandb:                                                                                
wandb: ğŸš€ View run rural-sweep-50 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6j8f1u3j
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212312-6j8f1u3j/logs
Run 6j8f1u3j errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6j8f1u3j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2ebbxoo5 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 8.101552068190298e-05
wandb: 	lr: 4.598691815304714e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212333-2ebbxoo5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-51
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2ebbxoo5
wandb:                                                                                
wandb: ğŸš€ View run sleek-sweep-51 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2ebbxoo5
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212333-2ebbxoo5/logs
Run 2ebbxoo5 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2ebbxoo5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: oiz53dq8 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 6.610101646539949e-05
wandb: 	lr: 3.592422970007838e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212353-oiz53dq8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-52
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/oiz53dq8
wandb:                                                                                
wandb: ğŸš€ View run dandy-sweep-52 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/oiz53dq8
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212353-oiz53dq8/logs
Run oiz53dq8 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run oiz53dq8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: i0vnw7lb with config:
wandb: 	batch_size: 4
wandb: 	epochs: 25
wandb: 	l2_reg: 4.223595274974337e-05
wandb: 	lr: 4.8758380411120705e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212414-i0vnw7lb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-53
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i0vnw7lb
wandb:                                                                                
wandb: ğŸš€ View run playful-sweep-53 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/i0vnw7lb
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212414-i0vnw7lb/logs
Run i0vnw7lb errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i0vnw7lb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: pxfn8w38 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 4.9703772224031155e-05
wandb: 	lr: 3.896892923632482e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212435-pxfn8w38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-54
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/pxfn8w38
wandb:                                                                                
wandb: ğŸš€ View run glowing-sweep-54 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/pxfn8w38
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212435-pxfn8w38/logs
Run pxfn8w38 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pxfn8w38 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ugci2fur with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 9.599832565307095e-05
wandb: 	lr: 4.487897448504871e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212456-ugci2fur
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-55
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ugci2fur
wandb:                                                                                
wandb: ğŸš€ View run sage-sweep-55 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ugci2fur
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212456-ugci2fur/logs
Run ugci2fur errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ugci2fur errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lnkvrtc1 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 8.804867745655106e-05
wandb: 	lr: 2.51815865937778e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212516-lnkvrtc1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-56
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lnkvrtc1
wandb:                                                                                
wandb: ğŸš€ View run autumn-sweep-56 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lnkvrtc1
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212516-lnkvrtc1/logs
Run lnkvrtc1 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lnkvrtc1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: l21upnzo with config:
wandb: 	batch_size: 4
wandb: 	epochs: 25
wandb: 	l2_reg: 8.730647716160163e-05
wandb: 	lr: 1.5856642071656275e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212537-l21upnzo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-57
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/l21upnzo
wandb:                                                                                
wandb: ğŸš€ View run polished-sweep-57 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/l21upnzo
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212537-l21upnzo/logs
Run l21upnzo errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l21upnzo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a9ny3la7 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 75
wandb: 	l2_reg: 3.2521436887605936e-05
wandb: 	lr: 2.3887721291412337e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212557-a9ny3la7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-58
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/a9ny3la7
wandb:                                                                                
wandb: ğŸš€ View run smooth-sweep-58 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/a9ny3la7
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212557-a9ny3la7/logs
Run a9ny3la7 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a9ny3la7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gtmvqttv with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 6.948251635593858e-05
wandb: 	lr: 7.054067542193316e-06
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212619-gtmvqttv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-59
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/gtmvqttv
wandb:                                                                                
wandb: ğŸš€ View run twilight-sweep-59 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/gtmvqttv
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212619-gtmvqttv/logs
Run gtmvqttv errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gtmvqttv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f4x8zndu with config:
wandb: 	batch_size: 4
wandb: 	epochs: 50
wandb: 	l2_reg: 3.742581291248168e-05
wandb: 	lr: 3.582381504568585e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212639-f4x8zndu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-60
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f4x8zndu
wandb:                                                                                
wandb: ğŸš€ View run helpful-sweep-60 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/f4x8zndu
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212639-f4x8zndu/logs
Run f4x8zndu errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f4x8zndu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sngsxcoe with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 1.4119428059123077e-05
wandb: 	lr: 3.871980945577228e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212700-sngsxcoe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-61
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/sngsxcoe
wandb:                                                                                
wandb: ğŸš€ View run cerulean-sweep-61 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/sngsxcoe
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212700-sngsxcoe/logs
Run sngsxcoe errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run sngsxcoe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bwmyrxox with config:
wandb: 	batch_size: 64
wandb: 	epochs: 25
wandb: 	l2_reg: 8.700993836767399e-05
wandb: 	lr: 5.335189223120284e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212721-bwmyrxox
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-62
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bwmyrxox
wandb:                                                                                
wandb: ğŸš€ View run sleek-sweep-62 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bwmyrxox
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212721-bwmyrxox/logs
Run bwmyrxox errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bwmyrxox errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xdybmitc with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 9.715668273170954e-05
wandb: 	lr: 4.0920148954538165e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212741-xdybmitc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-63
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xdybmitc
wandb:                                                                                
wandb: ğŸš€ View run robust-sweep-63 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/xdybmitc
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212741-xdybmitc/logs
Run xdybmitc errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xdybmitc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lwep82eo with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 5.8806092059456886e-05
wandb: 	lr: 2.9549792007571815e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212802-lwep82eo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-64
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lwep82eo
wandb:                                                                                
wandb: ğŸš€ View run lemon-sweep-64 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/lwep82eo
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212802-lwep82eo/logs
Run lwep82eo errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lwep82eo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ah2at8zw with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 1.4027681446049613e-05
wandb: 	lr: 2.5255639784329472e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212822-ah2at8zw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-65
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ah2at8zw
wandb:                                                                                
wandb: ğŸš€ View run azure-sweep-65 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ah2at8zw
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212822-ah2at8zw/logs
Run ah2at8zw errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ah2at8zw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ofdag0hq with config:
wandb: 	batch_size: 4
wandb: 	epochs: 25
wandb: 	l2_reg: 6.436577637956605e-05
wandb: 	lr: 1.6500878439841133e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212843-ofdag0hq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-66
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ofdag0hq
wandb:                                                                                
wandb: ğŸš€ View run glorious-sweep-66 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ofdag0hq
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212843-ofdag0hq/logs
Run ofdag0hq errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ofdag0hq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: anf0tv52 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 6.810362910336849e-05
wandb: 	lr: 2.2628116732256508e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212904-anf0tv52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-67
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/anf0tv52
wandb:                                                                                
wandb: ğŸš€ View run unique-sweep-67 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/anf0tv52
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212904-anf0tv52/logs
Run anf0tv52 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run anf0tv52 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dpbu33it with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 1.4765494775394128e-05
wandb: 	lr: 1.4177698479903888e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212924-dpbu33it
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-68
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/dpbu33it
wandb:                                                                                
wandb: ğŸš€ View run comfy-sweep-68 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/dpbu33it
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212924-dpbu33it/logs
Run dpbu33it errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dpbu33it errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: uh0j8l8h with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 9.595878399883756e-05
wandb: 	lr: 2.039873960706727e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_212945-uh0j8l8h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-69
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/uh0j8l8h
wandb:                                                                                
wandb: ğŸš€ View run youthful-sweep-69 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/uh0j8l8h
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_212945-uh0j8l8h/logs
Run uh0j8l8h errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run uh0j8l8h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8dadxyio with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 9.135790050706343e-05
wandb: 	lr: 5.956167912815196e-06
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213006-8dadxyio
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-70
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8dadxyio
wandb:                                                                                
wandb: ğŸš€ View run lively-sweep-70 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8dadxyio
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213006-8dadxyio/logs
Run 8dadxyio errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8dadxyio errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2verr720 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 25
wandb: 	l2_reg: 8.137752952889594e-05
wandb: 	lr: 4.430377540189464e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213026-2verr720
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-71
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2verr720
wandb:                                                                                
wandb: ğŸš€ View run peach-sweep-71 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2verr720
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213026-2verr720/logs
Run 2verr720 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2verr720 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y8n9a7ms with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 8.454867862990377e-05
wandb: 	lr: 1.8752524551986987e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213047-y8n9a7ms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-72
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/y8n9a7ms
wandb:                                                                                
wandb: ğŸš€ View run wandering-sweep-72 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/y8n9a7ms
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213047-y8n9a7ms/logs
Run y8n9a7ms errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y8n9a7ms errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: h5yw63zd with config:
wandb: 	batch_size: 64
wandb: 	epochs: 50
wandb: 	l2_reg: 8.449147445379052e-05
wandb: 	lr: 3.821703559540823e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213108-h5yw63zd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-73
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/h5yw63zd
wandb:                                                                                
wandb: ğŸš€ View run laced-sweep-73 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/h5yw63zd
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213108-h5yw63zd/logs
Run h5yw63zd errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run h5yw63zd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: g8xhtkux with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 8.143411051482276e-05
wandb: 	lr: 4.085995929992756e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213128-g8xhtkux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-74
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g8xhtkux
wandb:                                                                                
wandb: ğŸš€ View run noble-sweep-74 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/g8xhtkux
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213128-g8xhtkux/logs
Run g8xhtkux errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run g8xhtkux errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7f3aydl8 with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 3.97488537567464e-05
wandb: 	lr: 4.45293780279408e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213149-7f3aydl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-75
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/7f3aydl8
wandb:                                                                                
wandb: ğŸš€ View run rose-sweep-75 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/7f3aydl8
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213149-7f3aydl8/logs
Run 7f3aydl8 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7f3aydl8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ujbh9yze with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 5.1542914484537834e-05
wandb: 	lr: 2.2631791882810932e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213210-ujbh9yze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-76
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ujbh9yze
wandb:                                                                                
wandb: ğŸš€ View run distinctive-sweep-76 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ujbh9yze
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213210-ujbh9yze/logs
Run ujbh9yze errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ujbh9yze errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ejbqy4wr with config:
wandb: 	batch_size: 4
wandb: 	epochs: 50
wandb: 	l2_reg: 4.643049782979197e-05
wandb: 	lr: 2.6556345177768544e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213230-ejbqy4wr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-77
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ejbqy4wr
wandb:                                                                                
wandb: ğŸš€ View run sweepy-sweep-77 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ejbqy4wr
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213230-ejbqy4wr/logs
Run ejbqy4wr errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ejbqy4wr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 3bk5bd45 with config:
wandb: 	batch_size: 32
wandb: 	epochs: 25
wandb: 	l2_reg: 6.045208803534989e-05
wandb: 	lr: 2.1024864409713564e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213251-3bk5bd45
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-78
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/3bk5bd45
wandb:                                                                                
wandb: ğŸš€ View run prime-sweep-78 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/3bk5bd45
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213251-3bk5bd45/logs
Run 3bk5bd45 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3bk5bd45 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nyo12fb5 with config:
wandb: 	batch_size: 4
wandb: 	epochs: 25
wandb: 	l2_reg: 7.246006920923282e-05
wandb: 	lr: 1.7872555071562035e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213312-nyo12fb5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-79
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/nyo12fb5
wandb:                                                                                
wandb: ğŸš€ View run happy-sweep-79 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/nyo12fb5
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213312-nyo12fb5/logs
Run nyo12fb5 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nyo12fb5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vek9g9r4 with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 2.164442650038909e-05
wandb: 	lr: 2.590173518803814e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213332-vek9g9r4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-80
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/vek9g9r4
wandb:                                                                                
wandb: ğŸš€ View run ethereal-sweep-80 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/vek9g9r4
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213332-vek9g9r4/logs
Run vek9g9r4 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vek9g9r4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: yny4dzuf with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 4.754465867554969e-05
wandb: 	lr: 8.807284273096733e-06
wandb: 	n_layers_freeze: 0
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213353-yny4dzuf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-81
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/yny4dzuf
wandb:                                                                                
wandb: ğŸš€ View run bumbling-sweep-81 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/yny4dzuf
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213353-yny4dzuf/logs
Run yny4dzuf errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run yny4dzuf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5ya8uvsq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 25
wandb: 	l2_reg: 7.426209799335307e-05
wandb: 	lr: 9.67069784475196e-06
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213413-5ya8uvsq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-82
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/5ya8uvsq
wandb:                                                                                
wandb: ğŸš€ View run lunar-sweep-82 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/5ya8uvsq
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213413-5ya8uvsq/logs
Run 5ya8uvsq errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5ya8uvsq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: zsayp2ai with config:
wandb: 	batch_size: 4
wandb: 	epochs: 75
wandb: 	l2_reg: 3.646564513294047e-05
wandb: 	lr: 3.006596483749496e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213434-zsayp2ai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-83
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/zsayp2ai
wandb:                                                                                
wandb: ğŸš€ View run stoic-sweep-83 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/zsayp2ai
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213434-zsayp2ai/logs
Run zsayp2ai errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zsayp2ai errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ni7y3ghb with config:
wandb: 	batch_size: 64
wandb: 	epochs: 50
wandb: 	l2_reg: 9.9216227935587e-05
wandb: 	lr: 3.2961566194487886e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213455-ni7y3ghb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-84
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ni7y3ghb
wandb:                                                                                
wandb: ğŸš€ View run olive-sweep-84 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ni7y3ghb
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213455-ni7y3ghb/logs
Run ni7y3ghb errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ni7y3ghb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 48v4q54e with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 5.3516965763974035e-05
wandb: 	lr: 4.102605135415334e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213518-48v4q54e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-85
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/48v4q54e
wandb:                                                                                
wandb: ğŸš€ View run splendid-sweep-85 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/48v4q54e
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213518-48v4q54e/logs
Run 48v4q54e errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 48v4q54e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bbld4z7z with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 7.122999949737384e-05
wandb: 	lr: 2.303969481147342e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213539-bbld4z7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-86
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bbld4z7z
wandb:                                                                                
wandb: ğŸš€ View run desert-sweep-86 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/bbld4z7z
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213539-bbld4z7z/logs
Run bbld4z7z errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bbld4z7z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cz41ycfw with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 9.434118732215724e-05
wandb: 	lr: 8.625789501737118e-06
wandb: 	n_layers_freeze: 1
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213559-cz41ycfw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-87
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/cz41ycfw
wandb:                                                                                
wandb: ğŸš€ View run resilient-sweep-87 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/cz41ycfw
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213559-cz41ycfw/logs
Run cz41ycfw errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run cz41ycfw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6wopgqcq with config:
wandb: 	batch_size: 32
wandb: 	epochs: 25
wandb: 	l2_reg: 6.316025649590111e-05
wandb: 	lr: 3.993481379702697e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213620-6wopgqcq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-88
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6wopgqcq
wandb:                                                                                
wandb: ğŸš€ View run comfy-sweep-88 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6wopgqcq
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213620-6wopgqcq/logs
Run 6wopgqcq errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6wopgqcq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8s8q453q with config:
wandb: 	batch_size: 32
wandb: 	epochs: 50
wandb: 	l2_reg: 1.3765520751389116e-05
wandb: 	lr: 4.384536951029869e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213641-8s8q453q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-89
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8s8q453q
wandb:                                                                                
wandb: ğŸš€ View run major-sweep-89 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8s8q453q
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213641-8s8q453q/logs
Run 8s8q453q errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8s8q453q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6jl9vgtx with config:
wandb: 	batch_size: 4
wandb: 	epochs: 75
wandb: 	l2_reg: 8.187582561500884e-05
wandb: 	lr: 2.9271606214178585e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213701-6jl9vgtx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-90
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6jl9vgtx
wandb:                                                                                
wandb: ğŸš€ View run fine-sweep-90 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/6jl9vgtx
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213701-6jl9vgtx/logs
Run 6jl9vgtx errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6jl9vgtx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y02qxe3x with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 3.2776317499681676e-05
wandb: 	lr: 2.4248963852706343e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213722-y02qxe3x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-91
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/y02qxe3x
wandb:                                                                                
wandb: ğŸš€ View run visionary-sweep-91 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/y02qxe3x
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213722-y02qxe3x/logs
Run y02qxe3x errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y02qxe3x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: thainyom with config:
wandb: 	batch_size: 128
wandb: 	epochs: 25
wandb: 	l2_reg: 8.800468185777216e-05
wandb: 	lr: 4.40446337395838e-05
wandb: 	n_layers_freeze: 2
wandb: 	patience: 5
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213742-thainyom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-92
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/thainyom
wandb:                                                                                
wandb: ğŸš€ View run balmy-sweep-92 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/thainyom
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213742-thainyom/logs
Run thainyom errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run thainyom errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2w6weoj9 with config:
wandb: 	batch_size: 64
wandb: 	epochs: 75
wandb: 	l2_reg: 6.6934159660223e-05
wandb: 	lr: 3.7566645936384525e-05
wandb: 	n_layers_freeze: 3
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213803-2w6weoj9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-93
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2w6weoj9
wandb:                                                                                
wandb: ğŸš€ View run likely-sweep-93 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/2w6weoj9
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213803-2w6weoj9/logs
Run 2w6weoj9 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2w6weoj9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: hbsvv5mb with config:
wandb: 	batch_size: 4
wandb: 	epochs: 75
wandb: 	l2_reg: 8.76680148538092e-05
wandb: 	lr: 3.689929931417024e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213824-hbsvv5mb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-94
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/hbsvv5mb
wandb:                                                                                
wandb: ğŸš€ View run warm-sweep-94 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/hbsvv5mb
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213824-hbsvv5mb/logs
Run hbsvv5mb errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run hbsvv5mb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: k0dqg2no with config:
wandb: 	batch_size: 4
wandb: 	epochs: 75
wandb: 	l2_reg: 7.298661470006947e-05
wandb: 	lr: 3.2205677338186225e-05
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213844-k0dqg2no
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-95
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/k0dqg2no
wandb:                                                                                
wandb: ğŸš€ View run flowing-sweep-95 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/k0dqg2no
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213844-k0dqg2no/logs
Run k0dqg2no errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run k0dqg2no errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: x23ggwbx with config:
wandb: 	batch_size: 64
wandb: 	epochs: 50
wandb: 	l2_reg: 2.719576210275753e-05
wandb: 	lr: 9.86754105818486e-06
wandb: 	n_layers_freeze: 4
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213905-x23ggwbx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-96
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/x23ggwbx
wandb:                                                                                
wandb: ğŸš€ View run dainty-sweep-96 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/x23ggwbx
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213905-x23ggwbx/logs
Run x23ggwbx errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run x23ggwbx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ly5dc0p1 with config:
wandb: 	batch_size: 128
wandb: 	epochs: 75
wandb: 	l2_reg: 3.019297413845636e-05
wandb: 	lr: 2.0778334321897196e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213925-ly5dc0p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-97
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ly5dc0p1
wandb:                                                                                
wandb: ğŸš€ View run fluent-sweep-97 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/ly5dc0p1
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213925-ly5dc0p1/logs
Run ly5dc0p1 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ly5dc0p1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: if2jl9ne with config:
wandb: 	batch_size: 8
wandb: 	epochs: 75
wandb: 	l2_reg: 2.053526614820333e-05
wandb: 	lr: 2.3958839864575272e-05
wandb: 	n_layers_freeze: 1
wandb: 	patience: 10
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_213946-if2jl9ne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-98
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/if2jl9ne
wandb:                                                                                
wandb: ğŸš€ View run firm-sweep-98 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/if2jl9ne
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_213946-if2jl9ne/logs
Run if2jl9ne errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run if2jl9ne errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8jib566k with config:
wandb: 	batch_size: 16
wandb: 	epochs: 50
wandb: 	l2_reg: 5.423071499791766e-05
wandb: 	lr: 1.618654131765119e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_214007-8jib566k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-99
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8jib566k
wandb:                                                                                
wandb: ğŸš€ View run fanciful-sweep-99 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/8jib566k
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_214007-8jib566k/logs
Run 8jib566k errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8jib566k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: h65ze6v6 with config:
wandb: 	batch_size: 16
wandb: 	epochs: 75
wandb: 	l2_reg: 2.219425469197279e-05
wandb: 	lr: 3.551814138704574e-05
wandb: 	n_layers_freeze: 0
wandb: 	patience: 15
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /n/home09/michaelzhao/Downloads/thesis/vast/wandb/run-20250218_214028-h65ze6v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-100
wandb: â­ï¸ View project at https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: ğŸ§¹ View sweep at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
wandb: ğŸš€ View run at https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/h65ze6v6
wandb:                                                                                
wandb: ğŸš€ View run trim-sweep-100 at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/runs/h65ze6v6
wandb: â­ï¸ View project at: https://wandb.ai/michaelzhao-harvard-university/wiki-larger
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250218_214028-h65ze6v6/logs
Run h65ze6v6 errored:
Traceback (most recent call last):
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
    engine = Engine(args)
             ^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
    model = nn.DataParallel(model)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
    self.module.to(self.src_device_obj)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run h65ze6v6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/hyper_sweep_larger.py", line 35, in sweep_train
wandb: ERROR     engine = Engine(args)
wandb: ERROR              ^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/thesis/vast/he_engine.py", line 43, in __init__
wandb: ERROR     model = nn.DataParallel(model)
wandb: ERROR             ^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 163, in __init__
wandb: ERROR     self.module.to(self.src_device_obj)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/n/home09/michaelzhao/Downloads/topicexpan/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 78.32 GiB is allocated by PyTorch, and 39.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
