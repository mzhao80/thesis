Starting with configuration:
data: vast
topic: 
model: sentence-transformers/all-mpnet-base-v2
wiki_model: sentence-transformers/all-mpnet-base-v2
lr: 8e-06
batch_size: 16
epochs: 75
patience: 15
n_layers_freeze: 3
l2_reg: 4e-05
gpu: 0
inference: 0
n_workers: 4
seed: 42
n_layers_freeze_wiki: 3
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.083
Batch: 85/843	Loss: 1.170
Batch: 169/843	Loss: 1.220
Batch: 253/843	Loss: 1.031
Batch: 337/843	Loss: 1.089
Batch: 421/843	Loss: 0.921
Batch: 505/843	Loss: 1.145
Batch: 589/843	Loss: 1.086
Batch: 673/843	Loss: 1.135
Batch: 757/843	Loss: 1.023
Batch: 841/843	Loss: 0.993
Batch: 843/843	Loss: 0.914
