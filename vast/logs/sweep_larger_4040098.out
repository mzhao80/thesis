Create sweep with ID: wpze2ye0
Sweep URL: https://wandb.ai/michaelzhao-harvard-university/wiki-larger/sweeps/wpze2ye0
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 2.0240935774795044e-05, 'lr': 4.440064380413456e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/422	Loss: 1.084
Batch: 43/422	Loss: 1.064
Batch: 85/422	Loss: 1.138
Batch: 127/422	Loss: 1.027
Batch: 169/422	Loss: 1.042
Batch: 211/422	Loss: 0.696
Batch: 253/422	Loss: 0.830
Batch: 295/422	Loss: 0.622
Batch: 337/422	Loss: 0.801
Batch: 379/422	Loss: 0.525
Batch: 421/422	Loss: 0.755
Batch: 422/422	Loss: 0.531
Epoch: 1	Train Loss: 0.839	Val F1: 0.634
Best Epoch: 1	Best Val F1: 0.634

****************************** Epoch: 2 ******************************
Batch: 1/422	Loss: 0.686
Batch: 43/422	Loss: 0.741
Batch: 85/422	Loss: 0.754
Batch: 127/422	Loss: 0.619
Batch: 169/422	Loss: 0.594
Batch: 211/422	Loss: 0.668
Batch: 253/422	Loss: 0.587
Batch: 295/422	Loss: 0.585
Batch: 337/422	Loss: 0.728
Batch: 379/422	Loss: 0.550
Batch: 421/422	Loss: 0.426
Batch: 422/422	Loss: 0.376
Epoch: 2	Train Loss: 0.595	Val F1: 0.729
Best Epoch: 2	Best Val F1: 0.729

****************************** Epoch: 3 ******************************
Batch: 1/422	Loss: 0.623
Batch: 43/422	Loss: 0.534
Batch: 85/422	Loss: 0.687
Batch: 127/422	Loss: 0.387
Batch: 169/422	Loss: 0.380
Batch: 211/422	Loss: 0.737
Batch: 253/422	Loss: 0.504
Batch: 295/422	Loss: 0.667
Batch: 337/422	Loss: 0.460
Batch: 379/422	Loss: 0.632
Batch: 421/422	Loss: 0.442
Batch: 422/422	Loss: 0.620
Epoch: 3	Train Loss: 0.519	Val F1: 0.723
Best Epoch: 2	Best Val F1: 0.729

****************************** Epoch: 4 ******************************
Batch: 1/422	Loss: 0.486
Batch: 43/422	Loss: 0.609
Batch: 85/422	Loss: 0.477
Batch: 127/422	Loss: 0.522
Batch: 169/422	Loss: 0.471
Batch: 211/422	Loss: 0.457
Batch: 253/422	Loss: 0.420
Batch: 295/422	Loss: 0.347
Batch: 337/422	Loss: 0.622
Batch: 379/422	Loss: 0.679
Batch: 421/422	Loss: 0.444
Batch: 422/422	Loss: 1.783
Epoch: 4	Train Loss: 0.461	Val F1: 0.709
Best Epoch: 2	Best Val F1: 0.729

****************************** Epoch: 5 ******************************
Batch: 1/422	Loss: 0.399
Batch: 43/422	Loss: 0.386
Batch: 85/422	Loss: 0.341
Batch: 127/422	Loss: 0.401
Batch: 169/422	Loss: 0.600
Batch: 211/422	Loss: 0.262
Batch: 253/422	Loss: 0.280
Batch: 295/422	Loss: 0.409
Batch: 337/422	Loss: 0.435
Batch: 379/422	Loss: 0.477
Batch: 421/422	Loss: 0.395
Batch: 422/422	Loss: 0.913
Epoch: 5	Train Loss: 0.417	Val F1: 0.690
Best Epoch: 2	Best Val F1: 0.729

****************************** Epoch: 6 ******************************
Batch: 1/422	Loss: 0.303
Batch: 43/422	Loss: 0.202
Batch: 85/422	Loss: 0.343
Batch: 127/422	Loss: 0.377
Batch: 169/422	Loss: 0.448
Batch: 211/422	Loss: 0.450
Batch: 253/422	Loss: 0.250
Batch: 295/422	Loss: 0.354
Batch: 337/422	Loss: 0.512
Batch: 379/422	Loss: 0.443
Batch: 421/422	Loss: 0.240
Batch: 422/422	Loss: 0.344
Epoch: 6	Train Loss: 0.375	Val F1: 0.728
Best Epoch: 2	Best Val F1: 0.729

****************************** Epoch: 7 ******************************
Batch: 1/422	Loss: 0.422
Batch: 43/422	Loss: 0.436
Batch: 85/422	Loss: 0.263
Batch: 127/422	Loss: 0.131
Batch: 169/422	Loss: 0.510
Batch: 211/422	Loss: 0.414
Batch: 253/422	Loss: 0.379
Batch: 295/422	Loss: 0.392
Batch: 337/422	Loss: 0.416
Batch: 379/422	Loss: 0.444
Batch: 421/422	Loss: 0.413
Batch: 422/422	Loss: 0.293
Epoch: 7	Train Loss: 0.334	Val F1: 0.718
Best Epoch: 2	Best Val F1: 0.729

Saving the best checkpoint....
Inference...
Test F1: 0.724	Test F1_Few: 0.720	Test F1_Zero: 0.728
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 7.529313001274593e-05, 'lr': 6.6526567268354995e-06, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/1685	Loss: 1.078
Batch: 169/1685	Loss: 1.087
Batch: 337/1685	Loss: 1.159
Batch: 505/1685	Loss: 0.983
Batch: 673/1685	Loss: 0.865
Batch: 841/1685	Loss: 0.713
Batch: 1009/1685	Loss: 0.568
Batch: 1177/1685	Loss: 0.359
Batch: 1345/1685	Loss: 0.836
Batch: 1513/1685	Loss: 0.753
Batch: 1681/1685	Loss: 0.734
Batch: 1685/1685	Loss: 0.656
Epoch: 1	Train Loss: 0.834	Val F1: 0.730
Best Epoch: 1	Best Val F1: 0.730

****************************** Epoch: 2 ******************************
Batch: 1/1685	Loss: 0.458
Batch: 169/1685	Loss: 1.203
Batch: 337/1685	Loss: 1.196
Batch: 505/1685	Loss: 0.375
Batch: 673/1685	Loss: 0.346
Batch: 841/1685	Loss: 1.311
Batch: 1009/1685	Loss: 0.438
Batch: 1177/1685	Loss: 0.447
Batch: 1345/1685	Loss: 0.458
Batch: 1513/1685	Loss: 0.859
Batch: 1681/1685	Loss: 0.299
Batch: 1685/1685	Loss: 0.487
Epoch: 2	Train Loss: 0.600	Val F1: 0.726
Best Epoch: 1	Best Val F1: 0.730

****************************** Epoch: 3 ******************************
Batch: 1/1685	Loss: 0.554
Batch: 169/1685	Loss: 0.460
Batch: 337/1685	Loss: 0.569
Batch: 505/1685	Loss: 0.300
Batch: 673/1685	Loss: 0.441
Batch: 841/1685	Loss: 1.069
Batch: 1009/1685	Loss: 0.485
Batch: 1177/1685	Loss: 0.419
Batch: 1345/1685	Loss: 0.213
Batch: 1513/1685	Loss: 0.838
Batch: 1681/1685	Loss: 0.419
Batch: 1685/1685	Loss: 0.183
Epoch: 3	Train Loss: 0.546	Val F1: 0.754
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 4 ******************************
Batch: 1/1685	Loss: 0.291
Batch: 169/1685	Loss: 0.829
Batch: 337/1685	Loss: 0.664
Batch: 505/1685	Loss: 0.774
Batch: 673/1685	Loss: 0.285
Batch: 841/1685	Loss: 0.490
Batch: 1009/1685	Loss: 0.778
Batch: 1177/1685	Loss: 0.686
Batch: 1345/1685	Loss: 0.794
Batch: 1513/1685	Loss: 0.635
Batch: 1681/1685	Loss: 0.685
Batch: 1685/1685	Loss: 1.530
Epoch: 4	Train Loss: 0.488	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 5 ******************************
Batch: 1/1685	Loss: 0.871
Batch: 169/1685	Loss: 0.325
Batch: 337/1685	Loss: 0.486
Batch: 505/1685	Loss: 0.086
Batch: 673/1685	Loss: 0.853
Batch: 841/1685	Loss: 0.481
Batch: 1009/1685	Loss: 0.349
Batch: 1177/1685	Loss: 0.922
Batch: 1345/1685	Loss: 0.593
Batch: 1513/1685	Loss: 0.309
Batch: 1681/1685	Loss: 0.483
Batch: 1685/1685	Loss: 0.732
Epoch: 5	Train Loss: 0.441	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 6 ******************************
Batch: 1/1685	Loss: 0.146
Batch: 169/1685	Loss: 0.294
Batch: 337/1685	Loss: 0.294
Batch: 505/1685	Loss: 0.596
Batch: 673/1685	Loss: 0.554
Batch: 841/1685	Loss: 0.360
Batch: 1009/1685	Loss: 0.598
Batch: 1177/1685	Loss: 0.498
Batch: 1345/1685	Loss: 0.564
Batch: 1513/1685	Loss: 0.359
Batch: 1681/1685	Loss: 0.195
Batch: 1685/1685	Loss: 0.048
Epoch: 6	Train Loss: 0.404	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 7 ******************************
Batch: 1/1685	Loss: 0.312
Batch: 169/1685	Loss: 0.144
Batch: 337/1685	Loss: 0.184
Batch: 505/1685	Loss: 0.523
Batch: 673/1685	Loss: 0.671
Batch: 841/1685	Loss: 0.289
Batch: 1009/1685	Loss: 0.237
Batch: 1177/1685	Loss: 0.376
Batch: 1345/1685	Loss: 0.397
Batch: 1513/1685	Loss: 0.862
Batch: 1681/1685	Loss: 0.142
Batch: 1685/1685	Loss: 0.166
Epoch: 7	Train Loss: 0.368	Val F1: 0.720
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 8 ******************************
Batch: 1/1685	Loss: 0.275
Batch: 169/1685	Loss: 0.599
Batch: 337/1685	Loss: 0.033
Batch: 505/1685	Loss: 0.627
Batch: 673/1685	Loss: 0.821
Batch: 841/1685	Loss: 0.212
Batch: 1009/1685	Loss: 0.457
Batch: 1177/1685	Loss: 0.488
Batch: 1345/1685	Loss: 0.201
Batch: 1513/1685	Loss: 0.439
Batch: 1681/1685	Loss: 0.209
Batch: 1685/1685	Loss: 0.267
Epoch: 8	Train Loss: 0.336	Val F1: 0.693
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 9 ******************************
Batch: 1/1685	Loss: 0.487
Batch: 169/1685	Loss: 0.374
Batch: 337/1685	Loss: 0.307
Batch: 505/1685	Loss: 0.512
Batch: 673/1685	Loss: 0.385
Batch: 841/1685	Loss: 0.484
Batch: 1009/1685	Loss: 0.466
Batch: 1177/1685	Loss: 0.235
Batch: 1345/1685	Loss: 1.050
Batch: 1513/1685	Loss: 0.099
Batch: 1681/1685	Loss: 0.124
Batch: 1685/1685	Loss: 0.166
Epoch: 9	Train Loss: 0.303	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 10 ******************************
Batch: 1/1685	Loss: 0.292
Batch: 169/1685	Loss: 0.400
Batch: 337/1685	Loss: 0.197
Batch: 505/1685	Loss: 0.065
Batch: 673/1685	Loss: 0.339
Batch: 841/1685	Loss: 0.103
Batch: 1009/1685	Loss: 0.288
Batch: 1177/1685	Loss: 0.412
Batch: 1345/1685	Loss: 0.048
Batch: 1513/1685	Loss: 0.321
Batch: 1681/1685	Loss: 0.182
Batch: 1685/1685	Loss: 0.026
Epoch: 10	Train Loss: 0.282	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 11 ******************************
Batch: 1/1685	Loss: 0.142
Batch: 169/1685	Loss: 0.680
Batch: 337/1685	Loss: 0.291
Batch: 505/1685	Loss: 0.138
Batch: 673/1685	Loss: 0.130
Batch: 841/1685	Loss: 0.395
Batch: 1009/1685	Loss: 0.691
Batch: 1177/1685	Loss: 0.204
Batch: 1345/1685	Loss: 0.383
Batch: 1513/1685	Loss: 0.100
Batch: 1681/1685	Loss: 0.603
Batch: 1685/1685	Loss: 0.134
Epoch: 11	Train Loss: 0.258	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 12 ******************************
Batch: 1/1685	Loss: 0.408
Batch: 169/1685	Loss: 0.048
Batch: 337/1685	Loss: 0.215
Batch: 505/1685	Loss: 0.726
Batch: 673/1685	Loss: 0.079
Batch: 841/1685	Loss: 0.713
Batch: 1009/1685	Loss: 0.290
Batch: 1177/1685	Loss: 0.032
Batch: 1345/1685	Loss: 0.155
Batch: 1513/1685	Loss: 0.032
Batch: 1681/1685	Loss: 0.213
Batch: 1685/1685	Loss: 0.008
Epoch: 12	Train Loss: 0.241	Val F1: 0.720
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 13 ******************************
Batch: 1/1685	Loss: 0.201
Batch: 169/1685	Loss: 0.014
Batch: 337/1685	Loss: 0.623
Batch: 505/1685	Loss: 0.037
Batch: 673/1685	Loss: 0.330
Batch: 841/1685	Loss: 0.148
Batch: 1009/1685	Loss: 0.095
Batch: 1177/1685	Loss: 0.116
Batch: 1345/1685	Loss: 0.058
Batch: 1513/1685	Loss: 0.180
Batch: 1681/1685	Loss: 0.300
Batch: 1685/1685	Loss: 0.105
Epoch: 13	Train Loss: 0.220	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 14 ******************************
Batch: 1/1685	Loss: 0.221
Batch: 169/1685	Loss: 0.088
Batch: 337/1685	Loss: 0.110
Batch: 505/1685	Loss: 0.292
Batch: 673/1685	Loss: 0.216
Batch: 841/1685	Loss: 0.406
Batch: 1009/1685	Loss: 0.171
Batch: 1177/1685	Loss: 0.036
Batch: 1345/1685	Loss: 0.505
Batch: 1513/1685	Loss: 0.180
Batch: 1681/1685	Loss: 0.375
Batch: 1685/1685	Loss: 0.534
Epoch: 14	Train Loss: 0.211	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 15 ******************************
Batch: 1/1685	Loss: 0.050
Batch: 169/1685	Loss: 0.035
Batch: 337/1685	Loss: 0.069
Batch: 505/1685	Loss: 0.270
Batch: 673/1685	Loss: 0.178
Batch: 841/1685	Loss: 0.013
Batch: 1009/1685	Loss: 0.054
Batch: 1177/1685	Loss: 0.099
Batch: 1345/1685	Loss: 0.686
Batch: 1513/1685	Loss: 0.207
Batch: 1681/1685	Loss: 0.133
Batch: 1685/1685	Loss: 0.045
Epoch: 15	Train Loss: 0.189	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 16 ******************************
Batch: 1/1685	Loss: 0.093
Batch: 169/1685	Loss: 0.203
Batch: 337/1685	Loss: 0.299
Batch: 505/1685	Loss: 0.313
Batch: 673/1685	Loss: 0.459
Batch: 841/1685	Loss: 0.491
Batch: 1009/1685	Loss: 0.161
Batch: 1177/1685	Loss: 0.006
Batch: 1345/1685	Loss: 0.216
Batch: 1513/1685	Loss: 0.101
Batch: 1681/1685	Loss: 0.106
Batch: 1685/1685	Loss: 0.072
Epoch: 16	Train Loss: 0.180	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 17 ******************************
Batch: 1/1685	Loss: 0.339
Batch: 169/1685	Loss: 0.179
Batch: 337/1685	Loss: 0.022
Batch: 505/1685	Loss: 0.053
Batch: 673/1685	Loss: 0.089
Batch: 841/1685	Loss: 0.057
Batch: 1009/1685	Loss: 0.075
Batch: 1177/1685	Loss: 0.187
Batch: 1345/1685	Loss: 0.235
Batch: 1513/1685	Loss: 0.095
Batch: 1681/1685	Loss: 0.146
Batch: 1685/1685	Loss: 0.005
Epoch: 17	Train Loss: 0.168	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 18 ******************************
Batch: 1/1685	Loss: 0.017
Batch: 169/1685	Loss: 0.009
Batch: 337/1685	Loss: 0.331
Batch: 505/1685	Loss: 0.052
Batch: 673/1685	Loss: 0.117
Batch: 841/1685	Loss: 0.497
Batch: 1009/1685	Loss: 0.005
Batch: 1177/1685	Loss: 0.248
Batch: 1345/1685	Loss: 0.052
Batch: 1513/1685	Loss: 0.065
Batch: 1681/1685	Loss: 0.163
Batch: 1685/1685	Loss: 0.376
Epoch: 18	Train Loss: 0.160	Val F1: 0.713
Best Epoch: 3	Best Val F1: 0.754

Saving the best checkpoint....
Inference...
Test F1: 0.747	Test F1_Few: 0.750	Test F1_Zero: 0.744
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 9.726919465451052e-05, 'lr': 2.083556190556011e-05, 'n_layers_freeze': 4, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/422	Loss: 1.084
Batch: 43/422	Loss: 1.089
Batch: 85/422	Loss: 1.131
Batch: 127/422	Loss: 1.054
Batch: 169/422	Loss: 1.092
Batch: 211/422	Loss: 0.817
Batch: 253/422	Loss: 0.736
Batch: 295/422	Loss: 0.682
Batch: 337/422	Loss: 0.871
Batch: 379/422	Loss: 0.570
Batch: 421/422	Loss: 0.804
Batch: 422/422	Loss: 0.546
Epoch: 1	Train Loss: 0.852	Val F1: 0.653
Best Epoch: 1	Best Val F1: 0.653

****************************** Epoch: 2 ******************************
Batch: 1/422	Loss: 0.631
Batch: 43/422	Loss: 0.772
Batch: 85/422	Loss: 0.697
Batch: 127/422	Loss: 0.602
Batch: 169/422	Loss: 0.507
Batch: 211/422	Loss: 0.697
Batch: 253/422	Loss: 0.595
Batch: 295/422	Loss: 0.634
Batch: 337/422	Loss: 0.683
Batch: 379/422	Loss: 0.547
Batch: 421/422	Loss: 0.551
Batch: 422/422	Loss: 0.332
Epoch: 2	Train Loss: 0.597	Val F1: 0.741
Best Epoch: 2	Best Val F1: 0.741

****************************** Epoch: 3 ******************************
Batch: 1/422	Loss: 0.622
Batch: 43/422	Loss: 0.481
Batch: 85/422	Loss: 0.601
Batch: 127/422	Loss: 0.428
Batch: 169/422	Loss: 0.405
Batch: 211/422	Loss: 0.661
Batch: 253/422	Loss: 0.446
Batch: 295/422	Loss: 0.759
Batch: 337/422	Loss: 0.395
Batch: 379/422	Loss: 0.475
Batch: 421/422	Loss: 0.437
Batch: 422/422	Loss: 0.655
Epoch: 3	Train Loss: 0.522	Val F1: 0.746
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 4 ******************************
Batch: 1/422	Loss: 0.411
Batch: 43/422	Loss: 0.486
Batch: 85/422	Loss: 0.554
Batch: 127/422	Loss: 0.544
Batch: 169/422	Loss: 0.336
Batch: 211/422	Loss: 0.470
Batch: 253/422	Loss: 0.380
Batch: 295/422	Loss: 0.375
Batch: 337/422	Loss: 0.580
Batch: 379/422	Loss: 0.756
Batch: 421/422	Loss: 0.480
Batch: 422/422	Loss: 1.431
Epoch: 4	Train Loss: 0.461	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 5 ******************************
Batch: 1/422	Loss: 0.503
Batch: 43/422	Loss: 0.388
Batch: 85/422	Loss: 0.365
Batch: 127/422	Loss: 0.417
Batch: 169/422	Loss: 0.353
Batch: 211/422	Loss: 0.398
Batch: 253/422	Loss: 0.314
Batch: 295/422	Loss: 0.402
Batch: 337/422	Loss: 0.388
Batch: 379/422	Loss: 0.411
Batch: 421/422	Loss: 0.512
Batch: 422/422	Loss: 0.651
Epoch: 5	Train Loss: 0.413	Val F1: 0.702
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 6 ******************************
Batch: 1/422	Loss: 0.315
Batch: 43/422	Loss: 0.184
Batch: 85/422	Loss: 0.366
Batch: 127/422	Loss: 0.375
Batch: 169/422	Loss: 0.435
Batch: 211/422	Loss: 0.458
Batch: 253/422	Loss: 0.217
Batch: 295/422	Loss: 0.336
Batch: 337/422	Loss: 0.553
Batch: 379/422	Loss: 0.401
Batch: 421/422	Loss: 0.191
Batch: 422/422	Loss: 0.037
Epoch: 6	Train Loss: 0.374	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 7 ******************************
Batch: 1/422	Loss: 0.421
Batch: 43/422	Loss: 0.224
Batch: 85/422	Loss: 0.241
Batch: 127/422	Loss: 0.198
Batch: 169/422	Loss: 0.665
Batch: 211/422	Loss: 0.414
Batch: 253/422	Loss: 0.479
Batch: 295/422	Loss: 0.316
Batch: 337/422	Loss: 0.418
Batch: 379/422	Loss: 0.417
Batch: 421/422	Loss: 0.405
Batch: 422/422	Loss: 0.105
Epoch: 7	Train Loss: 0.332	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 8 ******************************
Batch: 1/422	Loss: 0.295
Batch: 43/422	Loss: 0.326
Batch: 85/422	Loss: 0.192
Batch: 127/422	Loss: 0.586
Batch: 169/422	Loss: 0.496
Batch: 211/422	Loss: 0.201
Batch: 253/422	Loss: 0.467
Batch: 295/422	Loss: 0.294
Batch: 337/422	Loss: 0.305
Batch: 379/422	Loss: 0.189
Batch: 421/422	Loss: 0.370
Batch: 422/422	Loss: 0.127
Epoch: 8	Train Loss: 0.299	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 9 ******************************
Batch: 1/422	Loss: 0.379
Batch: 43/422	Loss: 0.201
Batch: 85/422	Loss: 0.187
Batch: 127/422	Loss: 0.321
Batch: 169/422	Loss: 0.233
Batch: 211/422	Loss: 0.254
Batch: 253/422	Loss: 0.332
Batch: 295/422	Loss: 0.302
Batch: 337/422	Loss: 0.270
Batch: 379/422	Loss: 0.162
Batch: 421/422	Loss: 0.280
Batch: 422/422	Loss: 0.191
Epoch: 9	Train Loss: 0.274	Val F1: 0.715
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 10 ******************************
Batch: 1/422	Loss: 0.263
Batch: 43/422	Loss: 0.294
Batch: 85/422	Loss: 0.205
Batch: 127/422	Loss: 0.043
Batch: 169/422	Loss: 0.155
Batch: 211/422	Loss: 0.335
Batch: 253/422	Loss: 0.268
Batch: 295/422	Loss: 0.118
Batch: 337/422	Loss: 0.170
Batch: 379/422	Loss: 0.168
Batch: 421/422	Loss: 0.145
Batch: 422/422	Loss: 0.037
Epoch: 10	Train Loss: 0.249	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 11 ******************************
Batch: 1/422	Loss: 0.272
Batch: 43/422	Loss: 0.472
Batch: 85/422	Loss: 0.177
Batch: 127/422	Loss: 0.210
Batch: 169/422	Loss: 0.230
Batch: 211/422	Loss: 0.283
Batch: 253/422	Loss: 0.325
Batch: 295/422	Loss: 0.165
Batch: 337/422	Loss: 0.389
Batch: 379/422	Loss: 0.255
Batch: 421/422	Loss: 0.246
Batch: 422/422	Loss: 0.033
Epoch: 11	Train Loss: 0.227	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 12 ******************************
Batch: 1/422	Loss: 0.192
Batch: 43/422	Loss: 0.133
Batch: 85/422	Loss: 0.186
Batch: 127/422	Loss: 0.120
Batch: 169/422	Loss: 0.156
Batch: 211/422	Loss: 0.230
Batch: 253/422	Loss: 0.270
Batch: 295/422	Loss: 0.261
Batch: 337/422	Loss: 0.159
Batch: 379/422	Loss: 0.287
Batch: 421/422	Loss: 0.298
Batch: 422/422	Loss: 0.003
Epoch: 12	Train Loss: 0.213	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 13 ******************************
Batch: 1/422	Loss: 0.123
Batch: 43/422	Loss: 0.207
Batch: 85/422	Loss: 0.176
Batch: 127/422	Loss: 0.152
Batch: 169/422	Loss: 0.257
Batch: 211/422	Loss: 0.099
Batch: 253/422	Loss: 0.250
Batch: 295/422	Loss: 0.402
Batch: 337/422	Loss: 0.262
Batch: 379/422	Loss: 0.196
Batch: 421/422	Loss: 0.159
Batch: 422/422	Loss: 0.079
Epoch: 13	Train Loss: 0.194	Val F1: 0.703
Best Epoch: 3	Best Val F1: 0.746

Saving the best checkpoint....
Inference...
Test F1: 0.748	Test F1_Few: 0.753	Test F1_Zero: 0.743
Starting training with config: {'batch_size': 64, 'epochs': 25, 'l2_reg': 3.782367280491364e-05, 'lr': 3.6262635271367106e-05, 'n_layers_freeze': 1, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.026
Batch: 43/211	Loss: 1.107
Batch: 64/211	Loss: 0.953
Batch: 85/211	Loss: 0.748
Batch: 106/211	Loss: 0.691
Batch: 127/211	Loss: 0.681
Batch: 148/211	Loss: 0.712
Batch: 169/211	Loss: 0.751
Batch: 190/211	Loss: 0.615
Batch: 211/211	Loss: 0.680
Epoch: 1	Train Loss: 0.830	Val F1: 0.637
Best Epoch: 1	Best Val F1: 0.637

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 0.693
Batch: 22/211	Loss: 0.712
Batch: 43/211	Loss: 0.710
Batch: 64/211	Loss: 0.538
Batch: 85/211	Loss: 0.576
Batch: 106/211	Loss: 0.783
Batch: 127/211	Loss: 0.645
Batch: 148/211	Loss: 0.589
Batch: 169/211	Loss: 0.595
Batch: 190/211	Loss: 0.538
Batch: 211/211	Loss: 0.509
Epoch: 2	Train Loss: 0.606	Val F1: 0.743
Best Epoch: 2	Best Val F1: 0.743

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.543
Batch: 22/211	Loss: 0.527
Batch: 43/211	Loss: 0.580
Batch: 64/211	Loss: 0.408
Batch: 85/211	Loss: 0.489
Batch: 106/211	Loss: 0.491
Batch: 127/211	Loss: 0.442
Batch: 148/211	Loss: 0.569
Batch: 169/211	Loss: 0.535
Batch: 190/211	Loss: 0.511
Batch: 211/211	Loss: 0.414
Epoch: 3	Train Loss: 0.522	Val F1: 0.744
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.370
Batch: 22/211	Loss: 0.590
Batch: 43/211	Loss: 0.493
Batch: 64/211	Loss: 0.391
Batch: 85/211	Loss: 0.414
Batch: 106/211	Loss: 0.415
Batch: 127/211	Loss: 0.341
Batch: 148/211	Loss: 0.419
Batch: 169/211	Loss: 0.468
Batch: 190/211	Loss: 0.489
Batch: 211/211	Loss: 0.540
Epoch: 4	Train Loss: 0.462	Val F1: 0.659
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.550
Batch: 22/211	Loss: 0.402
Batch: 43/211	Loss: 0.237
Batch: 64/211	Loss: 0.339
Batch: 85/211	Loss: 0.394
Batch: 106/211	Loss: 0.308
Batch: 127/211	Loss: 0.278
Batch: 148/211	Loss: 0.364
Batch: 169/211	Loss: 0.299
Batch: 190/211	Loss: 0.451
Batch: 211/211	Loss: 0.517
Epoch: 5	Train Loss: 0.413	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.418
Batch: 22/211	Loss: 0.350
Batch: 43/211	Loss: 0.419
Batch: 64/211	Loss: 0.291
Batch: 85/211	Loss: 0.332
Batch: 106/211	Loss: 0.510
Batch: 127/211	Loss: 0.444
Batch: 148/211	Loss: 0.296
Batch: 169/211	Loss: 0.414
Batch: 190/211	Loss: 0.395
Batch: 211/211	Loss: 0.204
Epoch: 6	Train Loss: 0.371	Val F1: 0.739
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.402
Batch: 22/211	Loss: 0.354
Batch: 43/211	Loss: 0.364
Batch: 64/211	Loss: 0.327
Batch: 85/211	Loss: 0.457
Batch: 106/211	Loss: 0.349
Batch: 127/211	Loss: 0.482
Batch: 148/211	Loss: 0.296
Batch: 169/211	Loss: 0.412
Batch: 190/211	Loss: 0.549
Batch: 211/211	Loss: 0.445
Epoch: 7	Train Loss: 0.334	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.269
Batch: 22/211	Loss: 0.228
Batch: 43/211	Loss: 0.239
Batch: 64/211	Loss: 0.375
Batch: 85/211	Loss: 0.487
Batch: 106/211	Loss: 0.277
Batch: 127/211	Loss: 0.324
Batch: 148/211	Loss: 0.368
Batch: 169/211	Loss: 0.416
Batch: 190/211	Loss: 0.264
Batch: 211/211	Loss: 0.357
Epoch: 8	Train Loss: 0.299	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.190
Batch: 22/211	Loss: 0.298
Batch: 43/211	Loss: 0.227
Batch: 64/211	Loss: 0.396
Batch: 85/211	Loss: 0.225
Batch: 106/211	Loss: 0.177
Batch: 127/211	Loss: 0.208
Batch: 148/211	Loss: 0.268
Batch: 169/211	Loss: 0.234
Batch: 190/211	Loss: 0.353
Batch: 211/211	Loss: 0.212
Epoch: 9	Train Loss: 0.265	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.213
Batch: 22/211	Loss: 0.194
Batch: 43/211	Loss: 0.314
Batch: 64/211	Loss: 0.098
Batch: 85/211	Loss: 0.248
Batch: 106/211	Loss: 0.304
Batch: 127/211	Loss: 0.320
Batch: 148/211	Loss: 0.156
Batch: 169/211	Loss: 0.228
Batch: 190/211	Loss: 0.198
Batch: 211/211	Loss: 0.165
Epoch: 10	Train Loss: 0.245	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.192
Batch: 22/211	Loss: 0.219
Batch: 43/211	Loss: 0.151
Batch: 64/211	Loss: 0.287
Batch: 85/211	Loss: 0.185
Batch: 106/211	Loss: 0.248
Batch: 127/211	Loss: 0.315
Batch: 148/211	Loss: 0.193
Batch: 169/211	Loss: 0.299
Batch: 190/211	Loss: 0.220
Batch: 211/211	Loss: 0.262
Epoch: 11	Train Loss: 0.233	Val F1: 0.720
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 12 ******************************
Batch: 1/211	Loss: 0.146
Batch: 22/211	Loss: 0.187
Batch: 43/211	Loss: 0.172
Batch: 64/211	Loss: 0.196
Batch: 85/211	Loss: 0.169
Batch: 106/211	Loss: 0.284
Batch: 127/211	Loss: 0.228
Batch: 148/211	Loss: 0.328
Batch: 169/211	Loss: 0.160
Batch: 190/211	Loss: 0.183
Batch: 211/211	Loss: 0.223
Epoch: 12	Train Loss: 0.215	Val F1: 0.739
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 13 ******************************
Batch: 1/211	Loss: 0.198
Batch: 22/211	Loss: 0.179
Batch: 43/211	Loss: 0.144
Batch: 64/211	Loss: 0.133
Batch: 85/211	Loss: 0.108
Batch: 106/211	Loss: 0.148
Batch: 127/211	Loss: 0.167
Batch: 148/211	Loss: 0.226
Batch: 169/211	Loss: 0.192
Batch: 190/211	Loss: 0.160
Batch: 211/211	Loss: 0.170
Epoch: 13	Train Loss: 0.189	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 14 ******************************
Batch: 1/211	Loss: 0.174
Batch: 22/211	Loss: 0.248
Batch: 43/211	Loss: 0.103
Batch: 64/211	Loss: 0.137
Batch: 85/211	Loss: 0.102
Batch: 106/211	Loss: 0.135
Batch: 127/211	Loss: 0.123
Batch: 148/211	Loss: 0.106
Batch: 169/211	Loss: 0.115
Batch: 190/211	Loss: 0.180
Batch: 211/211	Loss: 0.131
Epoch: 14	Train Loss: 0.181	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 15 ******************************
Batch: 1/211	Loss: 0.106
Batch: 22/211	Loss: 0.079
Batch: 43/211	Loss: 0.231
Batch: 64/211	Loss: 0.147
Batch: 85/211	Loss: 0.076
Batch: 106/211	Loss: 0.109
Batch: 127/211	Loss: 0.181
Batch: 148/211	Loss: 0.239
Batch: 169/211	Loss: 0.142
Batch: 190/211	Loss: 0.180
Batch: 211/211	Loss: 0.154
Epoch: 15	Train Loss: 0.169	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 16 ******************************
Batch: 1/211	Loss: 0.104
Batch: 22/211	Loss: 0.212
Batch: 43/211	Loss: 0.122
Batch: 64/211	Loss: 0.096
Batch: 85/211	Loss: 0.157
Batch: 106/211	Loss: 0.171
Batch: 127/211	Loss: 0.177
Batch: 148/211	Loss: 0.107
Batch: 169/211	Loss: 0.068
Batch: 190/211	Loss: 0.283
Batch: 211/211	Loss: 0.200
Epoch: 16	Train Loss: 0.157	Val F1: 0.710
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 17 ******************************
Batch: 1/211	Loss: 0.085
Batch: 22/211	Loss: 0.136
Batch: 43/211	Loss: 0.137
Batch: 64/211	Loss: 0.124
Batch: 85/211	Loss: 0.069
Batch: 106/211	Loss: 0.105
Batch: 127/211	Loss: 0.221
Batch: 148/211	Loss: 0.047
Batch: 169/211	Loss: 0.126
Batch: 190/211	Loss: 0.226
Batch: 211/211	Loss: 0.241
Epoch: 17	Train Loss: 0.149	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 18 ******************************
Batch: 1/211	Loss: 0.188
Batch: 22/211	Loss: 0.032
Batch: 43/211	Loss: 0.128
Batch: 64/211	Loss: 0.144
Batch: 85/211	Loss: 0.388
Batch: 106/211	Loss: 0.170
Batch: 127/211	Loss: 0.192
Batch: 148/211	Loss: 0.183
Batch: 169/211	Loss: 0.106
Batch: 190/211	Loss: 0.135
Batch: 211/211	Loss: 0.196
Epoch: 18	Train Loss: 0.150	Val F1: 0.715
Best Epoch: 3	Best Val F1: 0.744

Saving the best checkpoint....
Inference...
Test F1: 0.739	Test F1_Few: 0.730	Test F1_Zero: 0.749
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 5.8211225455219336e-05, 'lr': 8.265386647580895e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.126
Batch: 169/843	Loss: 1.200
Batch: 253/843	Loss: 0.999
Batch: 337/843	Loss: 1.054
Batch: 421/843	Loss: 0.747
Batch: 505/843	Loss: 0.687
Batch: 589/843	Loss: 0.601
Batch: 673/843	Loss: 0.747
Batch: 757/843	Loss: 0.659
Batch: 841/843	Loss: 0.687
Batch: 843/843	Loss: 0.550
Epoch: 1	Train Loss: 0.854	Val F1: 0.692
Best Epoch: 1	Best Val F1: 0.692

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.626
Batch: 85/843	Loss: 0.865
Batch: 169/843	Loss: 0.786
Batch: 253/843	Loss: 0.505
Batch: 337/843	Loss: 0.590
Batch: 421/843	Loss: 0.795
Batch: 505/843	Loss: 0.665
Batch: 589/843	Loss: 0.840
Batch: 673/843	Loss: 0.628
Batch: 757/843	Loss: 0.595
Batch: 841/843	Loss: 0.536
Batch: 843/843	Loss: 0.494
Epoch: 2	Train Loss: 0.613	Val F1: 0.737
Best Epoch: 2	Best Val F1: 0.737

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.439
Batch: 85/843	Loss: 0.597
Batch: 169/843	Loss: 0.521
Batch: 253/843	Loss: 0.447
Batch: 337/843	Loss: 0.419
Batch: 421/843	Loss: 0.818
Batch: 505/843	Loss: 0.392
Batch: 589/843	Loss: 0.741
Batch: 673/843	Loss: 0.337
Batch: 757/843	Loss: 0.896
Batch: 841/843	Loss: 0.694
Batch: 843/843	Loss: 0.852
Epoch: 3	Train Loss: 0.559	Val F1: 0.754
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.337
Batch: 85/843	Loss: 0.508
Batch: 169/843	Loss: 0.501
Batch: 253/843	Loss: 0.663
Batch: 337/843	Loss: 0.393
Batch: 421/843	Loss: 0.713
Batch: 505/843	Loss: 0.739
Batch: 589/843	Loss: 0.410
Batch: 673/843	Loss: 0.548
Batch: 757/843	Loss: 0.514
Batch: 841/843	Loss: 0.384
Batch: 843/843	Loss: 1.381
Epoch: 4	Train Loss: 0.502	Val F1: 0.740
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.572
Batch: 85/843	Loss: 0.454
Batch: 169/843	Loss: 0.467
Batch: 253/843	Loss: 0.387
Batch: 337/843	Loss: 0.639
Batch: 421/843	Loss: 0.423
Batch: 505/843	Loss: 0.497
Batch: 589/843	Loss: 0.743
Batch: 673/843	Loss: 0.455
Batch: 757/843	Loss: 0.546
Batch: 841/843	Loss: 0.328
Batch: 843/843	Loss: 0.716
Epoch: 5	Train Loss: 0.453	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.309
Batch: 85/843	Loss: 0.337
Batch: 169/843	Loss: 0.420
Batch: 253/843	Loss: 0.587
Batch: 337/843	Loss: 0.508
Batch: 421/843	Loss: 0.262
Batch: 505/843	Loss: 0.429
Batch: 589/843	Loss: 0.447
Batch: 673/843	Loss: 0.468
Batch: 757/843	Loss: 0.525
Batch: 841/843	Loss: 0.264
Batch: 843/843	Loss: 0.060
Epoch: 6	Train Loss: 0.416	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.346
Batch: 85/843	Loss: 0.160
Batch: 169/843	Loss: 0.360
Batch: 253/843	Loss: 0.329
Batch: 337/843	Loss: 0.721
Batch: 421/843	Loss: 0.141
Batch: 505/843	Loss: 0.228
Batch: 589/843	Loss: 0.209
Batch: 673/843	Loss: 0.501
Batch: 757/843	Loss: 0.474
Batch: 841/843	Loss: 0.487
Batch: 843/843	Loss: 0.246
Epoch: 7	Train Loss: 0.376	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.197
Batch: 85/843	Loss: 0.410
Batch: 169/843	Loss: 0.131
Batch: 253/843	Loss: 0.546
Batch: 337/843	Loss: 0.576
Batch: 421/843	Loss: 0.363
Batch: 505/843	Loss: 0.606
Batch: 589/843	Loss: 0.518
Batch: 673/843	Loss: 0.265
Batch: 757/843	Loss: 0.200
Batch: 841/843	Loss: 0.270
Batch: 843/843	Loss: 0.175
Epoch: 8	Train Loss: 0.350	Val F1: 0.713
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 9 ******************************
Batch: 1/843	Loss: 0.633
Batch: 85/843	Loss: 0.221
Batch: 169/843	Loss: 0.638
Batch: 253/843	Loss: 0.395
Batch: 337/843	Loss: 0.135
Batch: 421/843	Loss: 0.422
Batch: 505/843	Loss: 0.463
Batch: 589/843	Loss: 0.317
Batch: 673/843	Loss: 0.538
Batch: 757/843	Loss: 0.317
Batch: 841/843	Loss: 0.267
Batch: 843/843	Loss: 0.264
Epoch: 9	Train Loss: 0.320	Val F1: 0.743
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 10 ******************************
Batch: 1/843	Loss: 0.406
Batch: 85/843	Loss: 0.368
Batch: 169/843	Loss: 0.275
Batch: 253/843	Loss: 0.168
Batch: 337/843	Loss: 0.188
Batch: 421/843	Loss: 0.170
Batch: 505/843	Loss: 0.346
Batch: 589/843	Loss: 0.266
Batch: 673/843	Loss: 0.143
Batch: 757/843	Loss: 0.275
Batch: 841/843	Loss: 0.066
Batch: 843/843	Loss: 0.059
Epoch: 10	Train Loss: 0.295	Val F1: 0.740
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 11 ******************************
Batch: 1/843	Loss: 0.366
Batch: 85/843	Loss: 0.460
Batch: 169/843	Loss: 0.290
Batch: 253/843	Loss: 0.172
Batch: 337/843	Loss: 0.178
Batch: 421/843	Loss: 0.270
Batch: 505/843	Loss: 0.559
Batch: 589/843	Loss: 0.114
Batch: 673/843	Loss: 0.497
Batch: 757/843	Loss: 0.101
Batch: 841/843	Loss: 0.204
Batch: 843/843	Loss: 0.352
Epoch: 11	Train Loss: 0.270	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 12 ******************************
Batch: 1/843	Loss: 0.262
Batch: 85/843	Loss: 0.081
Batch: 169/843	Loss: 0.159
Batch: 253/843	Loss: 0.240
Batch: 337/843	Loss: 0.243
Batch: 421/843	Loss: 0.618
Batch: 505/843	Loss: 0.216
Batch: 589/843	Loss: 0.120
Batch: 673/843	Loss: 0.059
Batch: 757/843	Loss: 0.190
Batch: 841/843	Loss: 0.110
Batch: 843/843	Loss: 0.018
Epoch: 12	Train Loss: 0.248	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 13 ******************************
Batch: 1/843	Loss: 0.206
Batch: 85/843	Loss: 0.439
Batch: 169/843	Loss: 0.321
Batch: 253/843	Loss: 0.053
Batch: 337/843	Loss: 0.099
Batch: 421/843	Loss: 0.196
Batch: 505/843	Loss: 0.301
Batch: 589/843	Loss: 0.297
Batch: 673/843	Loss: 0.138
Batch: 757/843	Loss: 0.124
Batch: 841/843	Loss: 0.358
Batch: 843/843	Loss: 0.155
Epoch: 13	Train Loss: 0.231	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 14 ******************************
Batch: 1/843	Loss: 0.089
Batch: 85/843	Loss: 0.195
Batch: 169/843	Loss: 0.108
Batch: 253/843	Loss: 0.167
Batch: 337/843	Loss: 0.290
Batch: 421/843	Loss: 0.121
Batch: 505/843	Loss: 0.067
Batch: 589/843	Loss: 0.024
Batch: 673/843	Loss: 0.156
Batch: 757/843	Loss: 0.053
Batch: 841/843	Loss: 0.352
Batch: 843/843	Loss: 0.067
Epoch: 14	Train Loss: 0.213	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 15 ******************************
Batch: 1/843	Loss: 0.125
Batch: 85/843	Loss: 0.082
Batch: 169/843	Loss: 0.098
Batch: 253/843	Loss: 0.279
Batch: 337/843	Loss: 0.282
Batch: 421/843	Loss: 0.058
Batch: 505/843	Loss: 0.076
Batch: 589/843	Loss: 0.178
Batch: 673/843	Loss: 0.243
Batch: 757/843	Loss: 0.183
Batch: 841/843	Loss: 0.215
Batch: 843/843	Loss: 0.021
Epoch: 15	Train Loss: 0.208	Val F1: 0.710
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 16 ******************************
Batch: 1/843	Loss: 0.201
Batch: 85/843	Loss: 0.095
Batch: 169/843	Loss: 0.222
Batch: 253/843	Loss: 0.241
Batch: 337/843	Loss: 0.568
Batch: 421/843	Loss: 0.090
Batch: 505/843	Loss: 0.119
Batch: 589/843	Loss: 0.150
Batch: 673/843	Loss: 0.527
Batch: 757/843	Loss: 0.182
Batch: 841/843	Loss: 0.247
Batch: 843/843	Loss: 0.125
Epoch: 16	Train Loss: 0.189	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 17 ******************************
Batch: 1/843	Loss: 0.154
Batch: 85/843	Loss: 0.148
Batch: 169/843	Loss: 0.345
Batch: 253/843	Loss: 0.352
Batch: 337/843	Loss: 0.058
Batch: 421/843	Loss: 0.065
Batch: 505/843	Loss: 0.278
Batch: 589/843	Loss: 0.110
Batch: 673/843	Loss: 0.170
Batch: 757/843	Loss: 0.193
Batch: 841/843	Loss: 0.265
Batch: 843/843	Loss: 0.018
Epoch: 17	Train Loss: 0.177	Val F1: 0.733
Best Epoch: 3	Best Val F1: 0.754

****************************** Epoch: 18 ******************************
Batch: 1/843	Loss: 0.091
Batch: 85/843	Loss: 0.080
Batch: 169/843	Loss: 0.170
Batch: 253/843	Loss: 0.136
Batch: 337/843	Loss: 0.243
Batch: 421/843	Loss: 0.185
Batch: 505/843	Loss: 0.106
Batch: 589/843	Loss: 0.086
Batch: 673/843	Loss: 0.068
Batch: 757/843	Loss: 0.102
Batch: 841/843	Loss: 0.175
Batch: 843/843	Loss: 0.270
Epoch: 18	Train Loss: 0.173	Val F1: 0.733
Best Epoch: 3	Best Val F1: 0.754

Saving the best checkpoint....
Inference...
Test F1: 0.757	Test F1_Few: 0.752	Test F1_Zero: 0.763
Starting training with config: {'batch_size': 64, 'epochs': 50, 'l2_reg': 1.8474002783055737e-05, 'lr': 4.061028479031612e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.026
Batch: 43/211	Loss: 1.094
Batch: 64/211	Loss: 0.976
Batch: 85/211	Loss: 0.713
Batch: 106/211	Loss: 0.750
Batch: 127/211	Loss: 0.701
Batch: 148/211	Loss: 0.707
Batch: 169/211	Loss: 0.712
Batch: 190/211	Loss: 0.580
Batch: 211/211	Loss: 0.677
Epoch: 1	Train Loss: 0.814	Val F1: 0.733
Best Epoch: 1	Best Val F1: 0.733

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 0.657
Batch: 22/211	Loss: 0.633
Batch: 43/211	Loss: 0.733
Batch: 64/211	Loss: 0.555
Batch: 85/211	Loss: 0.577
Batch: 106/211	Loss: 0.776
Batch: 127/211	Loss: 0.627
Batch: 148/211	Loss: 0.691
Batch: 169/211	Loss: 0.606
Batch: 190/211	Loss: 0.536
Batch: 211/211	Loss: 0.501
Epoch: 2	Train Loss: 0.599	Val F1: 0.717
Best Epoch: 1	Best Val F1: 0.733

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.556
Batch: 22/211	Loss: 0.556
Batch: 43/211	Loss: 0.571
Batch: 64/211	Loss: 0.456
Batch: 85/211	Loss: 0.502
Batch: 106/211	Loss: 0.541
Batch: 127/211	Loss: 0.383
Batch: 148/211	Loss: 0.581
Batch: 169/211	Loss: 0.535
Batch: 190/211	Loss: 0.493
Batch: 211/211	Loss: 0.505
Epoch: 3	Train Loss: 0.529	Val F1: 0.736
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.375
Batch: 22/211	Loss: 0.511
Batch: 43/211	Loss: 0.535
Batch: 64/211	Loss: 0.483
Batch: 85/211	Loss: 0.367
Batch: 106/211	Loss: 0.441
Batch: 127/211	Loss: 0.334
Batch: 148/211	Loss: 0.425
Batch: 169/211	Loss: 0.471
Batch: 190/211	Loss: 0.519
Batch: 211/211	Loss: 0.558
Epoch: 4	Train Loss: 0.467	Val F1: 0.662
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.522
Batch: 22/211	Loss: 0.428
Batch: 43/211	Loss: 0.227
Batch: 64/211	Loss: 0.341
Batch: 85/211	Loss: 0.443
Batch: 106/211	Loss: 0.389
Batch: 127/211	Loss: 0.288
Batch: 148/211	Loss: 0.350
Batch: 169/211	Loss: 0.360
Batch: 190/211	Loss: 0.390
Batch: 211/211	Loss: 0.485
Epoch: 5	Train Loss: 0.414	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.335
Batch: 22/211	Loss: 0.301
Batch: 43/211	Loss: 0.338
Batch: 64/211	Loss: 0.280
Batch: 85/211	Loss: 0.356
Batch: 106/211	Loss: 0.590
Batch: 127/211	Loss: 0.388
Batch: 148/211	Loss: 0.357
Batch: 169/211	Loss: 0.382
Batch: 190/211	Loss: 0.335
Batch: 211/211	Loss: 0.154
Epoch: 6	Train Loss: 0.371	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.499
Batch: 22/211	Loss: 0.295
Batch: 43/211	Loss: 0.352
Batch: 64/211	Loss: 0.336
Batch: 85/211	Loss: 0.431
Batch: 106/211	Loss: 0.397
Batch: 127/211	Loss: 0.495
Batch: 148/211	Loss: 0.253
Batch: 169/211	Loss: 0.376
Batch: 190/211	Loss: 0.521
Batch: 211/211	Loss: 0.391
Epoch: 7	Train Loss: 0.331	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.274
Batch: 22/211	Loss: 0.266
Batch: 43/211	Loss: 0.216
Batch: 64/211	Loss: 0.331
Batch: 85/211	Loss: 0.415
Batch: 106/211	Loss: 0.204
Batch: 127/211	Loss: 0.339
Batch: 148/211	Loss: 0.334
Batch: 169/211	Loss: 0.429
Batch: 190/211	Loss: 0.319
Batch: 211/211	Loss: 0.303
Epoch: 8	Train Loss: 0.299	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.276
Batch: 22/211	Loss: 0.254
Batch: 43/211	Loss: 0.386
Batch: 64/211	Loss: 0.430
Batch: 85/211	Loss: 0.348
Batch: 106/211	Loss: 0.202
Batch: 127/211	Loss: 0.210
Batch: 148/211	Loss: 0.305
Batch: 169/211	Loss: 0.214
Batch: 190/211	Loss: 0.241
Batch: 211/211	Loss: 0.278
Epoch: 9	Train Loss: 0.272	Val F1: 0.698
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.247
Batch: 22/211	Loss: 0.194
Batch: 43/211	Loss: 0.372
Batch: 64/211	Loss: 0.176
Batch: 85/211	Loss: 0.254
Batch: 106/211	Loss: 0.265
Batch: 127/211	Loss: 0.250
Batch: 148/211	Loss: 0.164
Batch: 169/211	Loss: 0.238
Batch: 190/211	Loss: 0.214
Batch: 211/211	Loss: 0.158
Epoch: 10	Train Loss: 0.261	Val F1: 0.715
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.163
Batch: 22/211	Loss: 0.184
Batch: 43/211	Loss: 0.162
Batch: 64/211	Loss: 0.237
Batch: 85/211	Loss: 0.252
Batch: 106/211	Loss: 0.297
Batch: 127/211	Loss: 0.371
Batch: 148/211	Loss: 0.382
Batch: 169/211	Loss: 0.271
Batch: 190/211	Loss: 0.253
Batch: 211/211	Loss: 0.190
Epoch: 11	Train Loss: 0.246	Val F1: 0.702
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 12 ******************************
Batch: 1/211	Loss: 0.203
Batch: 22/211	Loss: 0.248
Batch: 43/211	Loss: 0.161
Batch: 64/211	Loss: 0.291
Batch: 85/211	Loss: 0.102
Batch: 106/211	Loss: 0.450
Batch: 127/211	Loss: 0.277
Batch: 148/211	Loss: 0.376
Batch: 169/211	Loss: 0.196
Batch: 190/211	Loss: 0.157
Batch: 211/211	Loss: 0.288
Epoch: 12	Train Loss: 0.211	Val F1: 0.707
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 13 ******************************
Batch: 1/211	Loss: 0.179
Batch: 22/211	Loss: 0.230
Batch: 43/211	Loss: 0.164
Batch: 64/211	Loss: 0.134
Batch: 85/211	Loss: 0.123
Batch: 106/211	Loss: 0.184
Batch: 127/211	Loss: 0.231
Batch: 148/211	Loss: 0.261
Batch: 169/211	Loss: 0.153
Batch: 190/211	Loss: 0.150
Batch: 211/211	Loss: 0.202
Epoch: 13	Train Loss: 0.190	Val F1: 0.703
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 14 ******************************
Batch: 1/211	Loss: 0.221
Batch: 22/211	Loss: 0.188
Batch: 43/211	Loss: 0.075
Batch: 64/211	Loss: 0.160
Batch: 85/211	Loss: 0.105
Batch: 106/211	Loss: 0.123
Batch: 127/211	Loss: 0.097
Batch: 148/211	Loss: 0.178
Batch: 169/211	Loss: 0.177
Batch: 190/211	Loss: 0.140
Batch: 211/211	Loss: 0.144
Epoch: 14	Train Loss: 0.179	Val F1: 0.698
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 15 ******************************
Batch: 1/211	Loss: 0.178
Batch: 22/211	Loss: 0.080
Batch: 43/211	Loss: 0.264
Batch: 64/211	Loss: 0.293
Batch: 85/211	Loss: 0.171
Batch: 106/211	Loss: 0.129
Batch: 127/211	Loss: 0.245
Batch: 148/211	Loss: 0.245
Batch: 169/211	Loss: 0.148
Batch: 190/211	Loss: 0.121
Batch: 211/211	Loss: 0.159
Epoch: 15	Train Loss: 0.175	Val F1: 0.706
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 16 ******************************
Batch: 1/211	Loss: 0.162
Batch: 22/211	Loss: 0.273
Batch: 43/211	Loss: 0.165
Batch: 64/211	Loss: 0.070
Batch: 85/211	Loss: 0.154
Batch: 106/211	Loss: 0.135
Batch: 127/211	Loss: 0.178
Batch: 148/211	Loss: 0.102
Batch: 169/211	Loss: 0.107
Batch: 190/211	Loss: 0.323
Batch: 211/211	Loss: 0.241
Epoch: 16	Train Loss: 0.164	Val F1: 0.697
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 17 ******************************
Batch: 1/211	Loss: 0.072
Batch: 22/211	Loss: 0.256
Batch: 43/211	Loss: 0.223
Batch: 64/211	Loss: 0.082
Batch: 85/211	Loss: 0.071
Batch: 106/211	Loss: 0.163
Batch: 127/211	Loss: 0.281
Batch: 148/211	Loss: 0.049
Batch: 169/211	Loss: 0.152
Batch: 190/211	Loss: 0.156
Batch: 211/211	Loss: 0.133
Epoch: 17	Train Loss: 0.156	Val F1: 0.700
Best Epoch: 3	Best Val F1: 0.736

****************************** Epoch: 18 ******************************
Batch: 1/211	Loss: 0.207
Batch: 22/211	Loss: 0.056
Batch: 43/211	Loss: 0.117
Batch: 64/211	Loss: 0.168
Batch: 85/211	Loss: 0.181
Batch: 106/211	Loss: 0.157
Batch: 127/211	Loss: 0.214
Batch: 148/211	Loss: 0.208
Batch: 169/211	Loss: 0.070
Batch: 190/211	Loss: 0.103
Batch: 211/211	Loss: 0.169
Epoch: 18	Train Loss: 0.148	Val F1: 0.714
Best Epoch: 3	Best Val F1: 0.736

Saving the best checkpoint....
Inference...
Test F1: 0.735	Test F1_Few: 0.735	Test F1_Zero: 0.735
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 4.006096466842862e-05, 'lr': 3.4500941010057065e-05, 'n_layers_freeze': 2, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.025
Batch: 43/211	Loss: 1.097
Batch: 64/211	Loss: 0.991
Batch: 85/211	Loss: 0.899
Batch: 106/211	Loss: 0.695
Batch: 127/211	Loss: 0.720
Batch: 148/211	Loss: 0.658
Batch: 169/211	Loss: 0.687
Batch: 190/211	Loss: 0.555
Batch: 211/211	Loss: 0.752
Epoch: 1	Train Loss: 0.819	Val F1: 0.725
Best Epoch: 1	Best Val F1: 0.725

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 0.634
Batch: 22/211	Loss: 0.728
Batch: 43/211	Loss: 0.707
Batch: 64/211	Loss: 0.565
Batch: 85/211	Loss: 0.556
Batch: 106/211	Loss: 0.808
Batch: 127/211	Loss: 0.658
Batch: 148/211	Loss: 0.651
Batch: 169/211	Loss: 0.616
Batch: 190/211	Loss: 0.527
Batch: 211/211	Loss: 0.512
Epoch: 2	Train Loss: 0.609	Val F1: 0.726
Best Epoch: 2	Best Val F1: 0.726

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.579
Batch: 22/211	Loss: 0.530
Batch: 43/211	Loss: 0.638
Batch: 64/211	Loss: 0.436
Batch: 85/211	Loss: 0.499
Batch: 106/211	Loss: 0.537
Batch: 127/211	Loss: 0.392
Batch: 148/211	Loss: 0.627
Batch: 169/211	Loss: 0.557
Batch: 190/211	Loss: 0.532
Batch: 211/211	Loss: 0.502
Epoch: 3	Train Loss: 0.533	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.734

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.386
Batch: 22/211	Loss: 0.568
Batch: 43/211	Loss: 0.476
Batch: 64/211	Loss: 0.433
Batch: 85/211	Loss: 0.421
Batch: 106/211	Loss: 0.403
Batch: 127/211	Loss: 0.379
Batch: 148/211	Loss: 0.421
Batch: 169/211	Loss: 0.547
Batch: 190/211	Loss: 0.483
Batch: 211/211	Loss: 0.528
Epoch: 4	Train Loss: 0.468	Val F1: 0.682
Best Epoch: 3	Best Val F1: 0.734

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.486
Batch: 22/211	Loss: 0.408
Batch: 43/211	Loss: 0.274
Batch: 64/211	Loss: 0.340
Batch: 85/211	Loss: 0.424
Batch: 106/211	Loss: 0.352
Batch: 127/211	Loss: 0.325
Batch: 148/211	Loss: 0.402
Batch: 169/211	Loss: 0.256
Batch: 190/211	Loss: 0.425
Batch: 211/211	Loss: 0.433
Epoch: 5	Train Loss: 0.423	Val F1: 0.715
Best Epoch: 3	Best Val F1: 0.734

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.406
Batch: 22/211	Loss: 0.336
Batch: 43/211	Loss: 0.365
Batch: 64/211	Loss: 0.292
Batch: 85/211	Loss: 0.407
Batch: 106/211	Loss: 0.628
Batch: 127/211	Loss: 0.443
Batch: 148/211	Loss: 0.302
Batch: 169/211	Loss: 0.373
Batch: 190/211	Loss: 0.414
Batch: 211/211	Loss: 0.186
Epoch: 6	Train Loss: 0.375	Val F1: 0.744
Best Epoch: 6	Best Val F1: 0.744

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.374
Batch: 22/211	Loss: 0.300
Batch: 43/211	Loss: 0.365
Batch: 64/211	Loss: 0.366
Batch: 85/211	Loss: 0.413
Batch: 106/211	Loss: 0.416
Batch: 127/211	Loss: 0.491
Batch: 148/211	Loss: 0.322
Batch: 169/211	Loss: 0.418
Batch: 190/211	Loss: 0.597
Batch: 211/211	Loss: 0.426
Epoch: 7	Train Loss: 0.336	Val F1: 0.737
Best Epoch: 6	Best Val F1: 0.744

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.251
Batch: 22/211	Loss: 0.242
Batch: 43/211	Loss: 0.259
Batch: 64/211	Loss: 0.431
Batch: 85/211	Loss: 0.490
Batch: 106/211	Loss: 0.246
Batch: 127/211	Loss: 0.407
Batch: 148/211	Loss: 0.349
Batch: 169/211	Loss: 0.399
Batch: 190/211	Loss: 0.264
Batch: 211/211	Loss: 0.389
Epoch: 8	Train Loss: 0.298	Val F1: 0.729
Best Epoch: 6	Best Val F1: 0.744

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.219
Batch: 22/211	Loss: 0.215
Batch: 43/211	Loss: 0.320
Batch: 64/211	Loss: 0.460
Batch: 85/211	Loss: 0.337
Batch: 106/211	Loss: 0.192
Batch: 127/211	Loss: 0.247
Batch: 148/211	Loss: 0.372
Batch: 169/211	Loss: 0.237
Batch: 190/211	Loss: 0.346
Batch: 211/211	Loss: 0.236
Epoch: 9	Train Loss: 0.275	Val F1: 0.713
Best Epoch: 6	Best Val F1: 0.744

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.306
Batch: 22/211	Loss: 0.182
Batch: 43/211	Loss: 0.318
Batch: 64/211	Loss: 0.145
Batch: 85/211	Loss: 0.189
Batch: 106/211	Loss: 0.157
Batch: 127/211	Loss: 0.251
Batch: 148/211	Loss: 0.126
Batch: 169/211	Loss: 0.315
Batch: 190/211	Loss: 0.188
Batch: 211/211	Loss: 0.143
Epoch: 10	Train Loss: 0.248	Val F1: 0.722
Best Epoch: 6	Best Val F1: 0.744

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.222
Batch: 22/211	Loss: 0.193
Batch: 43/211	Loss: 0.151
Batch: 64/211	Loss: 0.306
Batch: 85/211	Loss: 0.212
Batch: 106/211	Loss: 0.324
Batch: 127/211	Loss: 0.356
Batch: 148/211	Loss: 0.250
Batch: 169/211	Loss: 0.291
Batch: 190/211	Loss: 0.221
Batch: 211/211	Loss: 0.297
Epoch: 11	Train Loss: 0.239	Val F1: 0.716
Best Epoch: 6	Best Val F1: 0.744

Saving the best checkpoint....
Inference...
Test F1: 0.737	Test F1_Few: 0.744	Test F1_Zero: 0.730
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 3.8216083619372326e-05, 'lr': 1.689152262594528e-05, 'n_layers_freeze': 2, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.137
Batch: 169/843	Loss: 1.211
Batch: 253/843	Loss: 0.969
Batch: 337/843	Loss: 1.090
Batch: 421/843	Loss: 0.765
Batch: 505/843	Loss: 0.894
Batch: 589/843	Loss: 0.781
Batch: 673/843	Loss: 0.948
Batch: 757/843	Loss: 0.784
Batch: 841/843	Loss: 0.681
Batch: 843/843	Loss: 1.095
Epoch: 1	Train Loss: 0.920	Val F1: 0.729
Best Epoch: 1	Best Val F1: 0.729

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.573
Batch: 85/843	Loss: 0.861
Batch: 169/843	Loss: 0.919
Batch: 253/843	Loss: 0.503
Batch: 337/843	Loss: 0.626
Batch: 421/843	Loss: 0.783
Batch: 505/843	Loss: 0.618
Batch: 589/843	Loss: 0.979
Batch: 673/843	Loss: 0.585
Batch: 757/843	Loss: 0.576
Batch: 841/843	Loss: 0.593
Batch: 843/843	Loss: 0.347
Epoch: 2	Train Loss: 0.605	Val F1: 0.724
Best Epoch: 1	Best Val F1: 0.729

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.405
Batch: 85/843	Loss: 0.513
Batch: 169/843	Loss: 0.420
Batch: 253/843	Loss: 0.412
Batch: 337/843	Loss: 0.449
Batch: 421/843	Loss: 0.749
Batch: 505/843	Loss: 0.407
Batch: 589/843	Loss: 0.752
Batch: 673/843	Loss: 0.268
Batch: 757/843	Loss: 0.738
Batch: 841/843	Loss: 0.399
Batch: 843/843	Loss: 0.685
Epoch: 3	Train Loss: 0.523	Val F1: 0.744
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.337
Batch: 85/843	Loss: 0.414
Batch: 169/843	Loss: 0.406
Batch: 253/843	Loss: 0.658
Batch: 337/843	Loss: 0.212
Batch: 421/843	Loss: 0.756
Batch: 505/843	Loss: 0.565
Batch: 589/843	Loss: 0.374
Batch: 673/843	Loss: 0.595
Batch: 757/843	Loss: 0.419
Batch: 841/843	Loss: 0.157
Batch: 843/843	Loss: 1.600
Epoch: 4	Train Loss: 0.454	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.482
Batch: 85/843	Loss: 0.434
Batch: 169/843	Loss: 0.427
Batch: 253/843	Loss: 0.270
Batch: 337/843	Loss: 0.523
Batch: 421/843	Loss: 0.461
Batch: 505/843	Loss: 0.256
Batch: 589/843	Loss: 0.593
Batch: 673/843	Loss: 0.365
Batch: 757/843	Loss: 0.395
Batch: 841/843	Loss: 0.340
Batch: 843/843	Loss: 0.881
Epoch: 5	Train Loss: 0.407	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.164
Batch: 85/843	Loss: 0.206
Batch: 169/843	Loss: 0.382
Batch: 253/843	Loss: 0.397
Batch: 337/843	Loss: 0.397
Batch: 421/843	Loss: 0.263
Batch: 505/843	Loss: 0.258
Batch: 589/843	Loss: 0.257
Batch: 673/843	Loss: 0.367
Batch: 757/843	Loss: 0.417
Batch: 841/843	Loss: 0.242
Batch: 843/843	Loss: 0.047
Epoch: 6	Train Loss: 0.363	Val F1: 0.736
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.282
Batch: 85/843	Loss: 0.136
Batch: 169/843	Loss: 0.251
Batch: 253/843	Loss: 0.087
Batch: 337/843	Loss: 1.120
Batch: 421/843	Loss: 0.168
Batch: 505/843	Loss: 0.109
Batch: 589/843	Loss: 0.130
Batch: 673/843	Loss: 0.350
Batch: 757/843	Loss: 0.596
Batch: 841/843	Loss: 0.424
Batch: 843/843	Loss: 0.195
Epoch: 7	Train Loss: 0.329	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.185
Batch: 85/843	Loss: 0.302
Batch: 169/843	Loss: 0.100
Batch: 253/843	Loss: 0.393
Batch: 337/843	Loss: 0.331
Batch: 421/843	Loss: 0.262
Batch: 505/843	Loss: 0.653
Batch: 589/843	Loss: 0.287
Batch: 673/843	Loss: 0.206
Batch: 757/843	Loss: 0.188
Batch: 841/843	Loss: 0.337
Batch: 843/843	Loss: 0.130
Epoch: 8	Train Loss: 0.289	Val F1: 0.692
Best Epoch: 3	Best Val F1: 0.744

Saving the best checkpoint....
Inference...
Test F1: 0.737	Test F1_Few: 0.746	Test F1_Zero: 0.728
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 5.131904591420647e-05, 'lr': 1.6171069900857887e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/422	Loss: 1.084
Batch: 43/422	Loss: 1.094
Batch: 85/422	Loss: 1.132
Batch: 127/422	Loss: 1.056
Batch: 169/422	Loss: 1.077
Batch: 211/422	Loss: 0.954
Batch: 253/422	Loss: 0.887
Batch: 295/422	Loss: 0.672
Batch: 337/422	Loss: 0.810
Batch: 379/422	Loss: 0.580
Batch: 421/422	Loss: 0.710
Batch: 422/422	Loss: 0.550
Epoch: 1	Train Loss: 0.891	Val F1: 0.628
Best Epoch: 1	Best Val F1: 0.628

****************************** Epoch: 2 ******************************
Batch: 1/422	Loss: 0.680
Batch: 43/422	Loss: 0.764
Batch: 85/422	Loss: 0.689
Batch: 127/422	Loss: 0.603
Batch: 169/422	Loss: 0.592
Batch: 211/422	Loss: 0.670
Batch: 253/422	Loss: 0.610
Batch: 295/422	Loss: 0.627
Batch: 337/422	Loss: 0.640
Batch: 379/422	Loss: 0.538
Batch: 421/422	Loss: 0.482
Batch: 422/422	Loss: 0.318
Epoch: 2	Train Loss: 0.595	Val F1: 0.738
Best Epoch: 2	Best Val F1: 0.738

****************************** Epoch: 3 ******************************
Batch: 1/422	Loss: 0.609
Batch: 43/422	Loss: 0.491
Batch: 85/422	Loss: 0.595
Batch: 127/422	Loss: 0.405
Batch: 169/422	Loss: 0.360
Batch: 211/422	Loss: 0.670
Batch: 253/422	Loss: 0.455
Batch: 295/422	Loss: 0.708
Batch: 337/422	Loss: 0.423
Batch: 379/422	Loss: 0.502
Batch: 421/422	Loss: 0.495
Batch: 422/422	Loss: 0.708
Epoch: 3	Train Loss: 0.521	Val F1: 0.747
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 4 ******************************
Batch: 1/422	Loss: 0.435
Batch: 43/422	Loss: 0.512
Batch: 85/422	Loss: 0.511
Batch: 127/422	Loss: 0.561
Batch: 169/422	Loss: 0.356
Batch: 211/422	Loss: 0.427
Batch: 253/422	Loss: 0.411
Batch: 295/422	Loss: 0.382
Batch: 337/422	Loss: 0.553
Batch: 379/422	Loss: 0.744
Batch: 421/422	Loss: 0.496
Batch: 422/422	Loss: 1.297
Epoch: 4	Train Loss: 0.463	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 5 ******************************
Batch: 1/422	Loss: 0.417
Batch: 43/422	Loss: 0.388
Batch: 85/422	Loss: 0.329
Batch: 127/422	Loss: 0.499
Batch: 169/422	Loss: 0.433
Batch: 211/422	Loss: 0.460
Batch: 253/422	Loss: 0.325
Batch: 295/422	Loss: 0.368
Batch: 337/422	Loss: 0.461
Batch: 379/422	Loss: 0.361
Batch: 421/422	Loss: 0.447
Batch: 422/422	Loss: 0.616
Epoch: 5	Train Loss: 0.419	Val F1: 0.703
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 6 ******************************
Batch: 1/422	Loss: 0.286
Batch: 43/422	Loss: 0.215
Batch: 85/422	Loss: 0.345
Batch: 127/422	Loss: 0.364
Batch: 169/422	Loss: 0.430
Batch: 211/422	Loss: 0.424
Batch: 253/422	Loss: 0.254
Batch: 295/422	Loss: 0.312
Batch: 337/422	Loss: 0.533
Batch: 379/422	Loss: 0.455
Batch: 421/422	Loss: 0.245
Batch: 422/422	Loss: 0.061
Epoch: 6	Train Loss: 0.377	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 7 ******************************
Batch: 1/422	Loss: 0.426
Batch: 43/422	Loss: 0.298
Batch: 85/422	Loss: 0.306
Batch: 127/422	Loss: 0.207
Batch: 169/422	Loss: 0.588
Batch: 211/422	Loss: 0.420
Batch: 253/422	Loss: 0.437
Batch: 295/422	Loss: 0.411
Batch: 337/422	Loss: 0.421
Batch: 379/422	Loss: 0.486
Batch: 421/422	Loss: 0.402
Batch: 422/422	Loss: 0.144
Epoch: 7	Train Loss: 0.340	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 8 ******************************
Batch: 1/422	Loss: 0.260
Batch: 43/422	Loss: 0.311
Batch: 85/422	Loss: 0.206
Batch: 127/422	Loss: 0.343
Batch: 169/422	Loss: 0.392
Batch: 211/422	Loss: 0.190
Batch: 253/422	Loss: 0.368
Batch: 295/422	Loss: 0.293
Batch: 337/422	Loss: 0.353
Batch: 379/422	Loss: 0.176
Batch: 421/422	Loss: 0.327
Batch: 422/422	Loss: 0.192
Epoch: 8	Train Loss: 0.310	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 9 ******************************
Batch: 1/422	Loss: 0.390
Batch: 43/422	Loss: 0.224
Batch: 85/422	Loss: 0.206
Batch: 127/422	Loss: 0.363
Batch: 169/422	Loss: 0.247
Batch: 211/422	Loss: 0.307
Batch: 253/422	Loss: 0.325
Batch: 295/422	Loss: 0.281
Batch: 337/422	Loss: 0.351
Batch: 379/422	Loss: 0.170
Batch: 421/422	Loss: 0.313
Batch: 422/422	Loss: 0.341
Epoch: 9	Train Loss: 0.286	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 10 ******************************
Batch: 1/422	Loss: 0.238
Batch: 43/422	Loss: 0.273
Batch: 85/422	Loss: 0.320
Batch: 127/422	Loss: 0.066
Batch: 169/422	Loss: 0.260
Batch: 211/422	Loss: 0.237
Batch: 253/422	Loss: 0.289
Batch: 295/422	Loss: 0.118
Batch: 337/422	Loss: 0.103
Batch: 379/422	Loss: 0.241
Batch: 421/422	Loss: 0.167
Batch: 422/422	Loss: 0.101
Epoch: 10	Train Loss: 0.261	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 11 ******************************
Batch: 1/422	Loss: 0.265
Batch: 43/422	Loss: 0.493
Batch: 85/422	Loss: 0.145
Batch: 127/422	Loss: 0.154
Batch: 169/422	Loss: 0.130
Batch: 211/422	Loss: 0.193
Batch: 253/422	Loss: 0.324
Batch: 295/422	Loss: 0.112
Batch: 337/422	Loss: 0.352
Batch: 379/422	Loss: 0.143
Batch: 421/422	Loss: 0.217
Batch: 422/422	Loss: 0.107
Epoch: 11	Train Loss: 0.237	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 12 ******************************
Batch: 1/422	Loss: 0.256
Batch: 43/422	Loss: 0.208
Batch: 85/422	Loss: 0.187
Batch: 127/422	Loss: 0.186
Batch: 169/422	Loss: 0.135
Batch: 211/422	Loss: 0.277
Batch: 253/422	Loss: 0.196
Batch: 295/422	Loss: 0.354
Batch: 337/422	Loss: 0.134
Batch: 379/422	Loss: 0.143
Batch: 421/422	Loss: 0.303
Batch: 422/422	Loss: 0.004
Epoch: 12	Train Loss: 0.218	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 13 ******************************
Batch: 1/422	Loss: 0.243
Batch: 43/422	Loss: 0.207
Batch: 85/422	Loss: 0.173
Batch: 127/422	Loss: 0.103
Batch: 169/422	Loss: 0.192
Batch: 211/422	Loss: 0.116
Batch: 253/422	Loss: 0.517
Batch: 295/422	Loss: 0.332
Batch: 337/422	Loss: 0.241
Batch: 379/422	Loss: 0.217
Batch: 421/422	Loss: 0.200
Batch: 422/422	Loss: 0.156
Epoch: 13	Train Loss: 0.203	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 14 ******************************
Batch: 1/422	Loss: 0.171
Batch: 43/422	Loss: 0.167
Batch: 85/422	Loss: 0.116
Batch: 127/422	Loss: 0.193
Batch: 169/422	Loss: 0.172
Batch: 211/422	Loss: 0.112
Batch: 253/422	Loss: 0.132
Batch: 295/422	Loss: 0.091
Batch: 337/422	Loss: 0.126
Batch: 379/422	Loss: 0.297
Batch: 421/422	Loss: 0.286
Batch: 422/422	Loss: 0.021
Epoch: 14	Train Loss: 0.193	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 15 ******************************
Batch: 1/422	Loss: 0.149
Batch: 43/422	Loss: 0.090
Batch: 85/422	Loss: 0.197
Batch: 127/422	Loss: 0.471
Batch: 169/422	Loss: 0.357
Batch: 211/422	Loss: 0.085
Batch: 253/422	Loss: 0.183
Batch: 295/422	Loss: 0.211
Batch: 337/422	Loss: 0.165
Batch: 379/422	Loss: 0.205
Batch: 421/422	Loss: 0.132
Batch: 422/422	Loss: 0.090
Epoch: 15	Train Loss: 0.180	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 16 ******************************
Batch: 1/422	Loss: 0.140
Batch: 43/422	Loss: 0.214
Batch: 85/422	Loss: 0.164
Batch: 127/422	Loss: 0.161
Batch: 169/422	Loss: 0.235
Batch: 211/422	Loss: 0.053
Batch: 253/422	Loss: 0.129
Batch: 295/422	Loss: 0.092
Batch: 337/422	Loss: 0.060
Batch: 379/422	Loss: 0.292
Batch: 421/422	Loss: 0.227
Batch: 422/422	Loss: 0.125
Epoch: 16	Train Loss: 0.171	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 17 ******************************
Batch: 1/422	Loss: 0.116
Batch: 43/422	Loss: 0.103
Batch: 85/422	Loss: 0.066
Batch: 127/422	Loss: 0.244
Batch: 169/422	Loss: 0.074
Batch: 211/422	Loss: 0.133
Batch: 253/422	Loss: 0.197
Batch: 295/422	Loss: 0.153
Batch: 337/422	Loss: 0.229
Batch: 379/422	Loss: 0.140
Batch: 421/422	Loss: 0.190
Batch: 422/422	Loss: 0.105
Epoch: 17	Train Loss: 0.155	Val F1: 0.714
Best Epoch: 3	Best Val F1: 0.747

****************************** Epoch: 18 ******************************
Batch: 1/422	Loss: 0.267
Batch: 43/422	Loss: 0.061
Batch: 85/422	Loss: 0.266
Batch: 127/422	Loss: 0.113
Batch: 169/422	Loss: 0.151
Batch: 211/422	Loss: 0.209
Batch: 253/422	Loss: 0.098
Batch: 295/422	Loss: 0.122
Batch: 337/422	Loss: 0.069
Batch: 379/422	Loss: 0.255
Batch: 421/422	Loss: 0.332
Batch: 422/422	Loss: 0.084
Epoch: 18	Train Loss: 0.154	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.747

Saving the best checkpoint....
Inference...
Test F1: 0.737	Test F1_Few: 0.738	Test F1_Zero: 0.736
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 5.462003399331975e-05, 'lr': 1.7367209525932223e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.122
Batch: 169/843	Loss: 1.197
Batch: 253/843	Loss: 0.958
Batch: 337/843	Loss: 0.935
Batch: 421/843	Loss: 0.667
Batch: 505/843	Loss: 0.569
Batch: 589/843	Loss: 0.467
Batch: 673/843	Loss: 0.569
Batch: 757/843	Loss: 0.719
Batch: 841/843	Loss: 0.673
Batch: 843/843	Loss: 0.703
Epoch: 1	Train Loss: 0.786	Val F1: 0.732
Best Epoch: 1	Best Val F1: 0.732

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.553
Batch: 85/843	Loss: 0.905
Batch: 169/843	Loss: 0.787
Batch: 253/843	Loss: 0.509
Batch: 337/843	Loss: 0.508
Batch: 421/843	Loss: 0.818
Batch: 505/843	Loss: 0.612
Batch: 589/843	Loss: 0.707
Batch: 673/843	Loss: 0.589
Batch: 757/843	Loss: 0.528
Batch: 841/843	Loss: 0.592
Batch: 843/843	Loss: 0.273
Epoch: 2	Train Loss: 0.584	Val F1: 0.728
Best Epoch: 1	Best Val F1: 0.732

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.374
Batch: 85/843	Loss: 0.529
Batch: 169/843	Loss: 0.429
Batch: 253/843	Loss: 0.400
Batch: 337/843	Loss: 0.410
Batch: 421/843	Loss: 0.680
Batch: 505/843	Loss: 0.378
Batch: 589/843	Loss: 0.699
Batch: 673/843	Loss: 0.274
Batch: 757/843	Loss: 0.773
Batch: 841/843	Loss: 0.422
Batch: 843/843	Loss: 0.598
Epoch: 3	Train Loss: 0.514	Val F1: 0.756
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.289
Batch: 85/843	Loss: 0.356
Batch: 169/843	Loss: 0.394
Batch: 253/843	Loss: 0.548
Batch: 337/843	Loss: 0.184
Batch: 421/843	Loss: 0.725
Batch: 505/843	Loss: 0.542
Batch: 589/843	Loss: 0.289
Batch: 673/843	Loss: 0.619
Batch: 757/843	Loss: 0.471
Batch: 841/843	Loss: 0.327
Batch: 843/843	Loss: 1.336
Epoch: 4	Train Loss: 0.447	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.511
Batch: 85/843	Loss: 0.438
Batch: 169/843	Loss: 0.396
Batch: 253/843	Loss: 0.309
Batch: 337/843	Loss: 0.599
Batch: 421/843	Loss: 0.400
Batch: 505/843	Loss: 0.320
Batch: 589/843	Loss: 0.663
Batch: 673/843	Loss: 0.403
Batch: 757/843	Loss: 0.448
Batch: 841/843	Loss: 0.310
Batch: 843/843	Loss: 0.892
Epoch: 5	Train Loss: 0.405	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.233
Batch: 85/843	Loss: 0.150
Batch: 169/843	Loss: 0.305
Batch: 253/843	Loss: 0.483
Batch: 337/843	Loss: 0.543
Batch: 421/843	Loss: 0.251
Batch: 505/843	Loss: 0.282
Batch: 589/843	Loss: 0.305
Batch: 673/843	Loss: 0.457
Batch: 757/843	Loss: 0.260
Batch: 841/843	Loss: 0.273
Batch: 843/843	Loss: 0.105
Epoch: 6	Train Loss: 0.360	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.258
Batch: 85/843	Loss: 0.110
Batch: 169/843	Loss: 0.346
Batch: 253/843	Loss: 0.079
Batch: 337/843	Loss: 0.843
Batch: 421/843	Loss: 0.122
Batch: 505/843	Loss: 0.150
Batch: 589/843	Loss: 0.133
Batch: 673/843	Loss: 0.264
Batch: 757/843	Loss: 0.449
Batch: 841/843	Loss: 0.421
Batch: 843/843	Loss: 0.308
Epoch: 7	Train Loss: 0.324	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.230
Batch: 85/843	Loss: 0.299
Batch: 169/843	Loss: 0.061
Batch: 253/843	Loss: 0.361
Batch: 337/843	Loss: 0.368
Batch: 421/843	Loss: 0.242
Batch: 505/843	Loss: 0.629
Batch: 589/843	Loss: 0.440
Batch: 673/843	Loss: 0.217
Batch: 757/843	Loss: 0.178
Batch: 841/843	Loss: 0.252
Batch: 843/843	Loss: 0.182
Epoch: 8	Train Loss: 0.295	Val F1: 0.697
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 9 ******************************
Batch: 1/843	Loss: 0.690
Batch: 85/843	Loss: 0.272
Batch: 169/843	Loss: 0.348
Batch: 253/843	Loss: 0.431
Batch: 337/843	Loss: 0.137
Batch: 421/843	Loss: 0.329
Batch: 505/843	Loss: 0.523
Batch: 589/843	Loss: 0.270
Batch: 673/843	Loss: 0.328
Batch: 757/843	Loss: 0.256
Batch: 841/843	Loss: 0.308
Batch: 843/843	Loss: 0.200
Epoch: 9	Train Loss: 0.264	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 10 ******************************
Batch: 1/843	Loss: 0.250
Batch: 85/843	Loss: 0.375
Batch: 169/843	Loss: 0.150
Batch: 253/843	Loss: 0.071
Batch: 337/843	Loss: 0.142
Batch: 421/843	Loss: 0.125
Batch: 505/843	Loss: 0.326
Batch: 589/843	Loss: 0.065
Batch: 673/843	Loss: 0.103
Batch: 757/843	Loss: 0.175
Batch: 841/843	Loss: 0.071
Batch: 843/843	Loss: 0.064
Epoch: 10	Train Loss: 0.241	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 11 ******************************
Batch: 1/843	Loss: 0.288
Batch: 85/843	Loss: 0.452
Batch: 169/843	Loss: 0.200
Batch: 253/843	Loss: 0.085
Batch: 337/843	Loss: 0.135
Batch: 421/843	Loss: 0.299
Batch: 505/843	Loss: 0.395
Batch: 589/843	Loss: 0.196
Batch: 673/843	Loss: 0.318
Batch: 757/843	Loss: 0.103
Batch: 841/843	Loss: 0.137
Batch: 843/843	Loss: 0.053
Epoch: 11	Train Loss: 0.221	Val F1: 0.714
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 12 ******************************
Batch: 1/843	Loss: 0.182
Batch: 85/843	Loss: 0.069
Batch: 169/843	Loss: 0.159
Batch: 253/843	Loss: 0.261
Batch: 337/843	Loss: 0.337
Batch: 421/843	Loss: 0.378
Batch: 505/843	Loss: 0.119
Batch: 589/843	Loss: 0.065
Batch: 673/843	Loss: 0.084
Batch: 757/843	Loss: 0.123
Batch: 841/843	Loss: 0.124
Batch: 843/843	Loss: 0.004
Epoch: 12	Train Loss: 0.213	Val F1: 0.713
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 13 ******************************
Batch: 1/843	Loss: 0.015
Batch: 85/843	Loss: 0.330
Batch: 169/843	Loss: 0.182
Batch: 253/843	Loss: 0.134
Batch: 337/843	Loss: 0.036
Batch: 421/843	Loss: 0.121
Batch: 505/843	Loss: 0.099
Batch: 589/843	Loss: 0.236
Batch: 673/843	Loss: 0.150
Batch: 757/843	Loss: 0.239
Batch: 841/843	Loss: 0.200
Batch: 843/843	Loss: 0.082
Epoch: 13	Train Loss: 0.193	Val F1: 0.706
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 14 ******************************
Batch: 1/843	Loss: 0.098
Batch: 85/843	Loss: 0.160
Batch: 169/843	Loss: 0.046
Batch: 253/843	Loss: 0.164
Batch: 337/843	Loss: 0.136
Batch: 421/843	Loss: 0.124
Batch: 505/843	Loss: 0.061
Batch: 589/843	Loss: 0.091
Batch: 673/843	Loss: 0.113
Batch: 757/843	Loss: 0.029
Batch: 841/843	Loss: 0.305
Batch: 843/843	Loss: 0.035
Epoch: 14	Train Loss: 0.180	Val F1: 0.697
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 15 ******************************
Batch: 1/843	Loss: 0.071
Batch: 85/843	Loss: 0.086
Batch: 169/843	Loss: 0.088
Batch: 253/843	Loss: 0.185
Batch: 337/843	Loss: 0.147
Batch: 421/843	Loss: 0.040
Batch: 505/843	Loss: 0.087
Batch: 589/843	Loss: 0.115
Batch: 673/843	Loss: 0.167
Batch: 757/843	Loss: 0.149
Batch: 841/843	Loss: 0.192
Batch: 843/843	Loss: 0.003
Epoch: 15	Train Loss: 0.175	Val F1: 0.714
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 16 ******************************
Batch: 1/843	Loss: 0.058
Batch: 85/843	Loss: 0.418
Batch: 169/843	Loss: 0.204
Batch: 253/843	Loss: 0.178
Batch: 337/843	Loss: 0.586
Batch: 421/843	Loss: 0.085
Batch: 505/843	Loss: 0.302
Batch: 589/843	Loss: 0.086
Batch: 673/843	Loss: 0.383
Batch: 757/843	Loss: 0.243
Batch: 841/843	Loss: 0.183
Batch: 843/843	Loss: 0.251
Epoch: 16	Train Loss: 0.164	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 17 ******************************
Batch: 1/843	Loss: 0.113
Batch: 85/843	Loss: 0.291
Batch: 169/843	Loss: 0.304
Batch: 253/843	Loss: 0.263
Batch: 337/843	Loss: 0.025
Batch: 421/843	Loss: 0.108
Batch: 505/843	Loss: 0.436
Batch: 589/843	Loss: 0.054
Batch: 673/843	Loss: 0.162
Batch: 757/843	Loss: 0.255
Batch: 841/843	Loss: 0.052
Batch: 843/843	Loss: 0.007
Epoch: 17	Train Loss: 0.156	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.756

****************************** Epoch: 18 ******************************
Batch: 1/843	Loss: 0.077
Batch: 85/843	Loss: 0.009
Batch: 169/843	Loss: 0.156
Batch: 253/843	Loss: 0.096
Batch: 337/843	Loss: 0.128
Batch: 421/843	Loss: 0.177
Batch: 505/843	Loss: 0.100
Batch: 589/843	Loss: 0.081
Batch: 673/843	Loss: 0.398
Batch: 757/843	Loss: 0.116
Batch: 841/843	Loss: 0.126
Batch: 843/843	Loss: 0.222
Epoch: 18	Train Loss: 0.147	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.756

Saving the best checkpoint....
Inference...
Test F1: 0.746	Test F1_Few: 0.748	Test F1_Zero: 0.743
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 4.034038569205083e-05, 'lr': 8.240720456484983e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.124
Batch: 169/843	Loss: 1.181
Batch: 253/843	Loss: 0.986
Batch: 337/843	Loss: 1.068
Batch: 421/843	Loss: 0.799
Batch: 505/843	Loss: 0.649
Batch: 589/843	Loss: 0.537
Batch: 673/843	Loss: 0.698
Batch: 757/843	Loss: 0.690
Batch: 841/843	Loss: 0.795
Batch: 843/843	Loss: 0.597
Epoch: 1	Train Loss: 0.880	Val F1: 0.683
Best Epoch: 1	Best Val F1: 0.683

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.618
Batch: 85/843	Loss: 0.882
Batch: 169/843	Loss: 0.803
Batch: 253/843	Loss: 0.532
Batch: 337/843	Loss: 0.583
Batch: 421/843	Loss: 0.859
Batch: 505/843	Loss: 0.620
Batch: 589/843	Loss: 0.747
Batch: 673/843	Loss: 0.655
Batch: 757/843	Loss: 0.597
Batch: 841/843	Loss: 0.590
Batch: 843/843	Loss: 0.500
Epoch: 2	Train Loss: 0.616	Val F1: 0.736
Best Epoch: 2	Best Val F1: 0.736

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.436
Batch: 85/843	Loss: 0.650
Batch: 169/843	Loss: 0.503
Batch: 253/843	Loss: 0.442
Batch: 337/843	Loss: 0.422
Batch: 421/843	Loss: 0.613
Batch: 505/843	Loss: 0.381
Batch: 589/843	Loss: 0.768
Batch: 673/843	Loss: 0.316
Batch: 757/843	Loss: 0.816
Batch: 841/843	Loss: 0.482
Batch: 843/843	Loss: 0.874
Epoch: 3	Train Loss: 0.561	Val F1: 0.765
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.356
Batch: 85/843	Loss: 0.516
Batch: 169/843	Loss: 0.473
Batch: 253/843	Loss: 0.707
Batch: 337/843	Loss: 0.393
Batch: 421/843	Loss: 0.609
Batch: 505/843	Loss: 0.763
Batch: 589/843	Loss: 0.414
Batch: 673/843	Loss: 0.632
Batch: 757/843	Loss: 0.712
Batch: 841/843	Loss: 0.477
Batch: 843/843	Loss: 1.491
Epoch: 4	Train Loss: 0.511	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.535
Batch: 85/843	Loss: 0.402
Batch: 169/843	Loss: 0.439
Batch: 253/843	Loss: 0.320
Batch: 337/843	Loss: 0.691
Batch: 421/843	Loss: 0.475
Batch: 505/843	Loss: 0.418
Batch: 589/843	Loss: 0.682
Batch: 673/843	Loss: 0.388
Batch: 757/843	Loss: 0.529
Batch: 841/843	Loss: 0.405
Batch: 843/843	Loss: 0.744
Epoch: 5	Train Loss: 0.468	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.263
Batch: 85/843	Loss: 0.390
Batch: 169/843	Loss: 0.377
Batch: 253/843	Loss: 0.670
Batch: 337/843	Loss: 0.659
Batch: 421/843	Loss: 0.319
Batch: 505/843	Loss: 0.266
Batch: 589/843	Loss: 0.433
Batch: 673/843	Loss: 0.563
Batch: 757/843	Loss: 0.651
Batch: 841/843	Loss: 0.248
Batch: 843/843	Loss: 0.051
Epoch: 6	Train Loss: 0.429	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.281
Batch: 85/843	Loss: 0.138
Batch: 169/843	Loss: 0.380
Batch: 253/843	Loss: 0.299
Batch: 337/843	Loss: 0.967
Batch: 421/843	Loss: 0.145
Batch: 505/843	Loss: 0.161
Batch: 589/843	Loss: 0.248
Batch: 673/843	Loss: 0.555
Batch: 757/843	Loss: 0.450
Batch: 841/843	Loss: 0.470
Batch: 843/843	Loss: 0.186
Epoch: 7	Train Loss: 0.390	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.231
Batch: 85/843	Loss: 0.514
Batch: 169/843	Loss: 0.120
Batch: 253/843	Loss: 0.585
Batch: 337/843	Loss: 0.473
Batch: 421/843	Loss: 0.368
Batch: 505/843	Loss: 0.534
Batch: 589/843	Loss: 0.569
Batch: 673/843	Loss: 0.258
Batch: 757/843	Loss: 0.250
Batch: 841/843	Loss: 0.326
Batch: 843/843	Loss: 0.157
Epoch: 8	Train Loss: 0.364	Val F1: 0.707
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 9 ******************************
Batch: 1/843	Loss: 0.653
Batch: 85/843	Loss: 0.194
Batch: 169/843	Loss: 0.394
Batch: 253/843	Loss: 0.398
Batch: 337/843	Loss: 0.092
Batch: 421/843	Loss: 0.296
Batch: 505/843	Loss: 0.514
Batch: 589/843	Loss: 0.318
Batch: 673/843	Loss: 0.371
Batch: 757/843	Loss: 0.230
Batch: 841/843	Loss: 0.270
Batch: 843/843	Loss: 0.232
Epoch: 9	Train Loss: 0.335	Val F1: 0.736
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 10 ******************************
Batch: 1/843	Loss: 0.400
Batch: 85/843	Loss: 0.407
Batch: 169/843	Loss: 0.306
Batch: 253/843	Loss: 0.075
Batch: 337/843	Loss: 0.165
Batch: 421/843	Loss: 0.226
Batch: 505/843	Loss: 0.406
Batch: 589/843	Loss: 0.268
Batch: 673/843	Loss: 0.142
Batch: 757/843	Loss: 0.265
Batch: 841/843	Loss: 0.086
Batch: 843/843	Loss: 0.219
Epoch: 10	Train Loss: 0.313	Val F1: 0.739
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 11 ******************************
Batch: 1/843	Loss: 0.266
Batch: 85/843	Loss: 0.537
Batch: 169/843	Loss: 0.369
Batch: 253/843	Loss: 0.152
Batch: 337/843	Loss: 0.143
Batch: 421/843	Loss: 0.212
Batch: 505/843	Loss: 0.515
Batch: 589/843	Loss: 0.172
Batch: 673/843	Loss: 0.510
Batch: 757/843	Loss: 0.167
Batch: 841/843	Loss: 0.144
Batch: 843/843	Loss: 0.513
Epoch: 11	Train Loss: 0.286	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 12 ******************************
Batch: 1/843	Loss: 0.351
Batch: 85/843	Loss: 0.122
Batch: 169/843	Loss: 0.159
Batch: 253/843	Loss: 0.497
Batch: 337/843	Loss: 0.152
Batch: 421/843	Loss: 0.812
Batch: 505/843	Loss: 0.308
Batch: 589/843	Loss: 0.189
Batch: 673/843	Loss: 0.155
Batch: 757/843	Loss: 0.167
Batch: 841/843	Loss: 0.143
Batch: 843/843	Loss: 0.023
Epoch: 12	Train Loss: 0.265	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 13 ******************************
Batch: 1/843	Loss: 0.104
Batch: 85/843	Loss: 0.289
Batch: 169/843	Loss: 0.240
Batch: 253/843	Loss: 0.206
Batch: 337/843	Loss: 0.099
Batch: 421/843	Loss: 0.315
Batch: 505/843	Loss: 0.247
Batch: 589/843	Loss: 0.485
Batch: 673/843	Loss: 0.092
Batch: 757/843	Loss: 0.208
Batch: 841/843	Loss: 0.532
Batch: 843/843	Loss: 0.199
Epoch: 13	Train Loss: 0.246	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 14 ******************************
Batch: 1/843	Loss: 0.096
Batch: 85/843	Loss: 0.126
Batch: 169/843	Loss: 0.135
Batch: 253/843	Loss: 0.204
Batch: 337/843	Loss: 0.301
Batch: 421/843	Loss: 0.154
Batch: 505/843	Loss: 0.082
Batch: 589/843	Loss: 0.085
Batch: 673/843	Loss: 0.229
Batch: 757/843	Loss: 0.038
Batch: 841/843	Loss: 0.655
Batch: 843/843	Loss: 0.049
Epoch: 14	Train Loss: 0.230	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 15 ******************************
Batch: 1/843	Loss: 0.151
Batch: 85/843	Loss: 0.141
Batch: 169/843	Loss: 0.124
Batch: 253/843	Loss: 0.193
Batch: 337/843	Loss: 0.200
Batch: 421/843	Loss: 0.026
Batch: 505/843	Loss: 0.154
Batch: 589/843	Loss: 0.157
Batch: 673/843	Loss: 0.333
Batch: 757/843	Loss: 0.253
Batch: 841/843	Loss: 0.277
Batch: 843/843	Loss: 0.011
Epoch: 15	Train Loss: 0.221	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 16 ******************************
Batch: 1/843	Loss: 0.114
Batch: 85/843	Loss: 0.084
Batch: 169/843	Loss: 0.233
Batch: 253/843	Loss: 0.294
Batch: 337/843	Loss: 0.600
Batch: 421/843	Loss: 0.179
Batch: 505/843	Loss: 0.160
Batch: 589/843	Loss: 0.043
Batch: 673/843	Loss: 0.133
Batch: 757/843	Loss: 0.145
Batch: 841/843	Loss: 0.315
Batch: 843/843	Loss: 0.151
Epoch: 16	Train Loss: 0.203	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 17 ******************************
Batch: 1/843	Loss: 0.119
Batch: 85/843	Loss: 0.125
Batch: 169/843	Loss: 0.195
Batch: 253/843	Loss: 0.236
Batch: 337/843	Loss: 0.051
Batch: 421/843	Loss: 0.196
Batch: 505/843	Loss: 0.379
Batch: 589/843	Loss: 0.116
Batch: 673/843	Loss: 0.317
Batch: 757/843	Loss: 0.148
Batch: 841/843	Loss: 0.188
Batch: 843/843	Loss: 0.109
Epoch: 17	Train Loss: 0.193	Val F1: 0.713
Best Epoch: 3	Best Val F1: 0.765

****************************** Epoch: 18 ******************************
Batch: 1/843	Loss: 0.160
Batch: 85/843	Loss: 0.069
Batch: 169/843	Loss: 0.188
Batch: 253/843	Loss: 0.083
Batch: 337/843	Loss: 0.202
Batch: 421/843	Loss: 0.285
Batch: 505/843	Loss: 0.134
Batch: 589/843	Loss: 0.138
Batch: 673/843	Loss: 0.069
Batch: 757/843	Loss: 0.129
Batch: 841/843	Loss: 0.214
Batch: 843/843	Loss: 0.160
Epoch: 18	Train Loss: 0.183	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.765

Saving the best checkpoint....
Inference...
Test F1: 0.751	Test F1_Few: 0.757	Test F1_Zero: 0.746
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 5.582900695782314e-05, 'lr': 5.681025442160858e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.048
Batch: 43/211	Loss: 1.117
Batch: 64/211	Loss: 1.039
Batch: 85/211	Loss: 1.064
Batch: 106/211	Loss: 1.029
Batch: 127/211	Loss: 1.030
Batch: 148/211	Loss: 1.057
Batch: 169/211	Loss: 1.007
Batch: 190/211	Loss: 1.131
Batch: 211/211	Loss: 1.019
Epoch: 1	Train Loss: 1.042	Val F1: 0.322
Best Epoch: 1	Best Val F1: 0.322

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 1.030
Batch: 22/211	Loss: 0.979
Batch: 43/211	Loss: 0.960
Batch: 64/211	Loss: 0.899
Batch: 85/211	Loss: 0.811
Batch: 106/211	Loss: 0.872
Batch: 127/211	Loss: 0.800
Batch: 148/211	Loss: 0.728
Batch: 169/211	Loss: 0.677
Batch: 190/211	Loss: 0.660
Batch: 211/211	Loss: 0.675
Epoch: 2	Train Loss: 0.828	Val F1: 0.707
Best Epoch: 2	Best Val F1: 0.707

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.679
Batch: 22/211	Loss: 0.687
Batch: 43/211	Loss: 0.672
Batch: 64/211	Loss: 0.546
Batch: 85/211	Loss: 0.620
Batch: 106/211	Loss: 0.573
Batch: 127/211	Loss: 0.542
Batch: 148/211	Loss: 0.722
Batch: 169/211	Loss: 0.690
Batch: 190/211	Loss: 0.719
Batch: 211/211	Loss: 0.658
Epoch: 3	Train Loss: 0.650	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.500
Batch: 22/211	Loss: 0.674
Batch: 43/211	Loss: 0.619
Batch: 64/211	Loss: 0.567
Batch: 85/211	Loss: 0.577
Batch: 106/211	Loss: 0.492
Batch: 127/211	Loss: 0.563
Batch: 148/211	Loss: 0.593
Batch: 169/211	Loss: 0.595
Batch: 190/211	Loss: 0.660
Batch: 211/211	Loss: 0.590
Epoch: 4	Train Loss: 0.605	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.653
Batch: 22/211	Loss: 0.563
Batch: 43/211	Loss: 0.525
Batch: 64/211	Loss: 0.533
Batch: 85/211	Loss: 0.620
Batch: 106/211	Loss: 0.525
Batch: 127/211	Loss: 0.488
Batch: 148/211	Loss: 0.542
Batch: 169/211	Loss: 0.501
Batch: 190/211	Loss: 0.635
Batch: 211/211	Loss: 0.638
Epoch: 5	Train Loss: 0.576	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.486
Batch: 22/211	Loss: 0.685
Batch: 43/211	Loss: 0.557
Batch: 64/211	Loss: 0.472
Batch: 85/211	Loss: 0.534
Batch: 106/211	Loss: 0.645
Batch: 127/211	Loss: 0.485
Batch: 148/211	Loss: 0.532
Batch: 169/211	Loss: 0.679
Batch: 190/211	Loss: 0.543
Batch: 211/211	Loss: 0.333
Epoch: 6	Train Loss: 0.554	Val F1: 0.732
Best Epoch: 6	Best Val F1: 0.732

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.601
Batch: 22/211	Loss: 0.600
Batch: 43/211	Loss: 0.543
Batch: 64/211	Loss: 0.501
Batch: 85/211	Loss: 0.565
Batch: 106/211	Loss: 0.506
Batch: 127/211	Loss: 0.695
Batch: 148/211	Loss: 0.617
Batch: 169/211	Loss: 0.565
Batch: 190/211	Loss: 0.535
Batch: 211/211	Loss: 0.633
Epoch: 7	Train Loss: 0.530	Val F1: 0.732
Best Epoch: 7	Best Val F1: 0.732

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.490
Batch: 22/211	Loss: 0.480
Batch: 43/211	Loss: 0.454
Batch: 64/211	Loss: 0.520
Batch: 85/211	Loss: 0.760
Batch: 106/211	Loss: 0.473
Batch: 127/211	Loss: 0.407
Batch: 148/211	Loss: 0.557
Batch: 169/211	Loss: 0.546
Batch: 190/211	Loss: 0.424
Batch: 211/211	Loss: 0.622
Epoch: 8	Train Loss: 0.505	Val F1: 0.736
Best Epoch: 8	Best Val F1: 0.736

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.526
Batch: 22/211	Loss: 0.441
Batch: 43/211	Loss: 0.528
Batch: 64/211	Loss: 0.525
Batch: 85/211	Loss: 0.592
Batch: 106/211	Loss: 0.384
Batch: 127/211	Loss: 0.638
Batch: 148/211	Loss: 0.491
Batch: 169/211	Loss: 0.423
Batch: 190/211	Loss: 0.477
Batch: 211/211	Loss: 0.462
Epoch: 9	Train Loss: 0.485	Val F1: 0.739
Best Epoch: 9	Best Val F1: 0.739

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.441
Batch: 22/211	Loss: 0.480
Batch: 43/211	Loss: 0.522
Batch: 64/211	Loss: 0.387
Batch: 85/211	Loss: 0.427
Batch: 106/211	Loss: 0.537
Batch: 127/211	Loss: 0.488
Batch: 148/211	Loss: 0.426
Batch: 169/211	Loss: 0.376
Batch: 190/211	Loss: 0.381
Batch: 211/211	Loss: 0.285
Epoch: 10	Train Loss: 0.456	Val F1: 0.743
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.363
Batch: 22/211	Loss: 0.366
Batch: 43/211	Loss: 0.463
Batch: 64/211	Loss: 0.501
Batch: 85/211	Loss: 0.429
Batch: 106/211	Loss: 0.428
Batch: 127/211	Loss: 0.637
Batch: 148/211	Loss: 0.409
Batch: 169/211	Loss: 0.487
Batch: 190/211	Loss: 0.483
Batch: 211/211	Loss: 0.439
Epoch: 11	Train Loss: 0.443	Val F1: 0.743
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 12 ******************************
Batch: 1/211	Loss: 0.473
Batch: 22/211	Loss: 0.399
Batch: 43/211	Loss: 0.408
Batch: 64/211	Loss: 0.468
Batch: 85/211	Loss: 0.422
Batch: 106/211	Loss: 0.517
Batch: 127/211	Loss: 0.345
Batch: 148/211	Loss: 0.588
Batch: 169/211	Loss: 0.470
Batch: 190/211	Loss: 0.316
Batch: 211/211	Loss: 0.472
Epoch: 12	Train Loss: 0.421	Val F1: 0.740
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 13 ******************************
Batch: 1/211	Loss: 0.515
Batch: 22/211	Loss: 0.439
Batch: 43/211	Loss: 0.314
Batch: 64/211	Loss: 0.306
Batch: 85/211	Loss: 0.362
Batch: 106/211	Loss: 0.535
Batch: 127/211	Loss: 0.466
Batch: 148/211	Loss: 0.556
Batch: 169/211	Loss: 0.476
Batch: 190/211	Loss: 0.299
Batch: 211/211	Loss: 0.328
Epoch: 13	Train Loss: 0.403	Val F1: 0.722
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 14 ******************************
Batch: 1/211	Loss: 0.416
Batch: 22/211	Loss: 0.601
Batch: 43/211	Loss: 0.339
Batch: 64/211	Loss: 0.333
Batch: 85/211	Loss: 0.394
Batch: 106/211	Loss: 0.356
Batch: 127/211	Loss: 0.223
Batch: 148/211	Loss: 0.401
Batch: 169/211	Loss: 0.314
Batch: 190/211	Loss: 0.324
Batch: 211/211	Loss: 0.521
Epoch: 14	Train Loss: 0.380	Val F1: 0.713
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 15 ******************************
Batch: 1/211	Loss: 0.496
Batch: 22/211	Loss: 0.255
Batch: 43/211	Loss: 0.353
Batch: 64/211	Loss: 0.418
Batch: 85/211	Loss: 0.269
Batch: 106/211	Loss: 0.302
Batch: 127/211	Loss: 0.530
Batch: 148/211	Loss: 0.457
Batch: 169/211	Loss: 0.387
Batch: 190/211	Loss: 0.467
Batch: 211/211	Loss: 0.327
Epoch: 15	Train Loss: 0.363	Val F1: 0.723
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 16 ******************************
Batch: 1/211	Loss: 0.295
Batch: 22/211	Loss: 0.422
Batch: 43/211	Loss: 0.368
Batch: 64/211	Loss: 0.304
Batch: 85/211	Loss: 0.399
Batch: 106/211	Loss: 0.261
Batch: 127/211	Loss: 0.335
Batch: 148/211	Loss: 0.283
Batch: 169/211	Loss: 0.178
Batch: 190/211	Loss: 0.341
Batch: 211/211	Loss: 0.410
Epoch: 16	Train Loss: 0.344	Val F1: 0.726
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 17 ******************************
Batch: 1/211	Loss: 0.332
Batch: 22/211	Loss: 0.365
Batch: 43/211	Loss: 0.322
Batch: 64/211	Loss: 0.311
Batch: 85/211	Loss: 0.296
Batch: 106/211	Loss: 0.331
Batch: 127/211	Loss: 0.389
Batch: 148/211	Loss: 0.179
Batch: 169/211	Loss: 0.376
Batch: 190/211	Loss: 0.460
Batch: 211/211	Loss: 0.265
Epoch: 17	Train Loss: 0.328	Val F1: 0.733
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 18 ******************************
Batch: 1/211	Loss: 0.391
Batch: 22/211	Loss: 0.282
Batch: 43/211	Loss: 0.343
Batch: 64/211	Loss: 0.244
Batch: 85/211	Loss: 0.394
Batch: 106/211	Loss: 0.266
Batch: 127/211	Loss: 0.282
Batch: 148/211	Loss: 0.380
Batch: 169/211	Loss: 0.230
Batch: 190/211	Loss: 0.220
Batch: 211/211	Loss: 0.378
Epoch: 18	Train Loss: 0.315	Val F1: 0.733
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 19 ******************************
Batch: 1/211	Loss: 0.295
Batch: 22/211	Loss: 0.373
Batch: 43/211	Loss: 0.386
Batch: 64/211	Loss: 0.529
Batch: 85/211	Loss: 0.423
Batch: 106/211	Loss: 0.182
Batch: 127/211	Loss: 0.326
Batch: 148/211	Loss: 0.364
Batch: 169/211	Loss: 0.406
Batch: 190/211	Loss: 0.357
Batch: 211/211	Loss: 0.225
Epoch: 19	Train Loss: 0.304	Val F1: 0.727
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 20 ******************************
Batch: 1/211	Loss: 0.216
Batch: 22/211	Loss: 0.179
Batch: 43/211	Loss: 0.365
Batch: 64/211	Loss: 0.327
Batch: 85/211	Loss: 0.347
Batch: 106/211	Loss: 0.346
Batch: 127/211	Loss: 0.234
Batch: 148/211	Loss: 0.336
Batch: 169/211	Loss: 0.283
Batch: 190/211	Loss: 0.314
Batch: 211/211	Loss: 0.276
Epoch: 20	Train Loss: 0.289	Val F1: 0.728
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 21 ******************************
Batch: 1/211	Loss: 0.207
Batch: 22/211	Loss: 0.190
Batch: 43/211	Loss: 0.170
Batch: 64/211	Loss: 0.189
Batch: 85/211	Loss: 0.147
Batch: 106/211	Loss: 0.341
Batch: 127/211	Loss: 0.261
Batch: 148/211	Loss: 0.216
Batch: 169/211	Loss: 0.302
Batch: 190/211	Loss: 0.364
Batch: 211/211	Loss: 0.221
Epoch: 21	Train Loss: 0.283	Val F1: 0.726
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 22 ******************************
Batch: 1/211	Loss: 0.283
Batch: 22/211	Loss: 0.240
Batch: 43/211	Loss: 0.440
Batch: 64/211	Loss: 0.241
Batch: 85/211	Loss: 0.291
Batch: 106/211	Loss: 0.211
Batch: 127/211	Loss: 0.243
Batch: 148/211	Loss: 0.285
Batch: 169/211	Loss: 0.267
Batch: 190/211	Loss: 0.201
Batch: 211/211	Loss: 0.298
Epoch: 22	Train Loss: 0.267	Val F1: 0.732
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 23 ******************************
Batch: 1/211	Loss: 0.273
Batch: 22/211	Loss: 0.188
Batch: 43/211	Loss: 0.258
Batch: 64/211	Loss: 0.177
Batch: 85/211	Loss: 0.182
Batch: 106/211	Loss: 0.152
Batch: 127/211	Loss: 0.241
Batch: 148/211	Loss: 0.233
Batch: 169/211	Loss: 0.323
Batch: 190/211	Loss: 0.234
Batch: 211/211	Loss: 0.163
Epoch: 23	Train Loss: 0.262	Val F1: 0.734
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 24 ******************************
Batch: 1/211	Loss: 0.285
Batch: 22/211	Loss: 0.156
Batch: 43/211	Loss: 0.190
Batch: 64/211	Loss: 0.195
Batch: 85/211	Loss: 0.258
Batch: 106/211	Loss: 0.161
Batch: 127/211	Loss: 0.330
Batch: 148/211	Loss: 0.330
Batch: 169/211	Loss: 0.224
Batch: 190/211	Loss: 0.203
Batch: 211/211	Loss: 0.226
Epoch: 24	Train Loss: 0.246	Val F1: 0.728
Best Epoch: 10	Best Val F1: 0.743

****************************** Epoch: 25 ******************************
Batch: 1/211	Loss: 0.236
Batch: 22/211	Loss: 0.235
Batch: 43/211	Loss: 0.268
Batch: 64/211	Loss: 0.159
Batch: 85/211	Loss: 0.166
Batch: 106/211	Loss: 0.196
Batch: 127/211	Loss: 0.269
Batch: 148/211	Loss: 0.138
Batch: 169/211	Loss: 0.206
Batch: 190/211	Loss: 0.111
Batch: 211/211	Loss: 0.251
Epoch: 25	Train Loss: 0.235	Val F1: 0.726
Best Epoch: 10	Best Val F1: 0.743

Saving the best checkpoint....
Inference...
Test F1: 0.722	Test F1_Few: 0.718	Test F1_Zero: 0.727
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 3.0566695176830026e-05, 'lr': 1.579341769330365e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/1685	Loss: 1.078
Batch: 169/1685	Loss: 1.069
Batch: 337/1685	Loss: 1.179
Batch: 505/1685	Loss: 0.698
Batch: 673/1685	Loss: 0.497
Batch: 841/1685	Loss: 0.640
Batch: 1009/1685	Loss: 0.502
Batch: 1177/1685	Loss: 0.355
Batch: 1345/1685	Loss: 0.784
Batch: 1513/1685	Loss: 0.928
Batch: 1681/1685	Loss: 0.734
Batch: 1685/1685	Loss: 0.694
Epoch: 1	Train Loss: 0.770	Val F1: 0.726
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 2 ******************************
Batch: 1/1685	Loss: 0.466
Batch: 169/1685	Loss: 1.285
Batch: 337/1685	Loss: 0.794
Batch: 505/1685	Loss: 0.396
Batch: 673/1685	Loss: 0.336
Batch: 841/1685	Loss: 0.968
Batch: 1009/1685	Loss: 0.417
Batch: 1177/1685	Loss: 0.445
Batch: 1345/1685	Loss: 0.585
Batch: 1513/1685	Loss: 0.674
Batch: 1681/1685	Loss: 0.339
Batch: 1685/1685	Loss: 0.416
Epoch: 2	Train Loss: 0.574	Val F1: 0.723
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 3 ******************************
Batch: 1/1685	Loss: 0.492
Batch: 169/1685	Loss: 0.495
Batch: 337/1685	Loss: 0.603
Batch: 505/1685	Loss: 0.400
Batch: 673/1685	Loss: 0.474
Batch: 841/1685	Loss: 1.007
Batch: 1009/1685	Loss: 0.493
Batch: 1177/1685	Loss: 0.656
Batch: 1345/1685	Loss: 0.219
Batch: 1513/1685	Loss: 0.601
Batch: 1681/1685	Loss: 0.424
Batch: 1685/1685	Loss: 0.442
Epoch: 3	Train Loss: 0.506	Val F1: 0.714
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 4 ******************************
Batch: 1/1685	Loss: 0.422
Batch: 169/1685	Loss: 0.517
Batch: 337/1685	Loss: 0.517
Batch: 505/1685	Loss: 0.686
Batch: 673/1685	Loss: 0.291
Batch: 841/1685	Loss: 0.349
Batch: 1009/1685	Loss: 0.754
Batch: 1177/1685	Loss: 0.511
Batch: 1345/1685	Loss: 0.779
Batch: 1513/1685	Loss: 0.738
Batch: 1681/1685	Loss: 0.632
Batch: 1685/1685	Loss: 1.762
Epoch: 4	Train Loss: 0.445	Val F1: 0.700
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 5 ******************************
Batch: 1/1685	Loss: 0.679
Batch: 169/1685	Loss: 0.316
Batch: 337/1685	Loss: 0.434
Batch: 505/1685	Loss: 0.086
Batch: 673/1685	Loss: 0.640
Batch: 841/1685	Loss: 0.277
Batch: 1009/1685	Loss: 0.120
Batch: 1177/1685	Loss: 1.061
Batch: 1345/1685	Loss: 0.770
Batch: 1513/1685	Loss: 0.245
Batch: 1681/1685	Loss: 0.306
Batch: 1685/1685	Loss: 0.610
Epoch: 5	Train Loss: 0.398	Val F1: 0.701
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 6 ******************************
Batch: 1/1685	Loss: 0.319
Batch: 169/1685	Loss: 0.118
Batch: 337/1685	Loss: 0.152
Batch: 505/1685	Loss: 0.462
Batch: 673/1685	Loss: 0.441
Batch: 841/1685	Loss: 0.488
Batch: 1009/1685	Loss: 0.300
Batch: 1177/1685	Loss: 0.331
Batch: 1345/1685	Loss: 0.562
Batch: 1513/1685	Loss: 0.142
Batch: 1681/1685	Loss: 0.217
Batch: 1685/1685	Loss: 0.027
Epoch: 6	Train Loss: 0.356	Val F1: 0.717
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 7 ******************************
Batch: 1/1685	Loss: 0.337
Batch: 169/1685	Loss: 0.255
Batch: 337/1685	Loss: 0.117
Batch: 505/1685	Loss: 0.166
Batch: 673/1685	Loss: 0.389
Batch: 841/1685	Loss: 0.168
Batch: 1009/1685	Loss: 0.207
Batch: 1177/1685	Loss: 0.206
Batch: 1345/1685	Loss: 0.265
Batch: 1513/1685	Loss: 0.608
Batch: 1681/1685	Loss: 0.153
Batch: 1685/1685	Loss: 0.144
Epoch: 7	Train Loss: 0.321	Val F1: 0.698
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 8 ******************************
Batch: 1/1685	Loss: 0.188
Batch: 169/1685	Loss: 0.824
Batch: 337/1685	Loss: 0.029
Batch: 505/1685	Loss: 0.371
Batch: 673/1685	Loss: 0.768
Batch: 841/1685	Loss: 0.124
Batch: 1009/1685	Loss: 0.400
Batch: 1177/1685	Loss: 0.511
Batch: 1345/1685	Loss: 0.126
Batch: 1513/1685	Loss: 0.239
Batch: 1681/1685	Loss: 0.400
Batch: 1685/1685	Loss: 0.240
Epoch: 8	Train Loss: 0.292	Val F1: 0.680
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 9 ******************************
Batch: 1/1685	Loss: 0.493
Batch: 169/1685	Loss: 0.389
Batch: 337/1685	Loss: 0.231
Batch: 505/1685	Loss: 0.350
Batch: 673/1685	Loss: 0.139
Batch: 841/1685	Loss: 0.399
Batch: 1009/1685	Loss: 0.472
Batch: 1177/1685	Loss: 0.467
Batch: 1345/1685	Loss: 0.648
Batch: 1513/1685	Loss: 0.121
Batch: 1681/1685	Loss: 0.033
Batch: 1685/1685	Loss: 0.497
Epoch: 9	Train Loss: 0.263	Val F1: 0.708
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 10 ******************************
Batch: 1/1685	Loss: 0.263
Batch: 169/1685	Loss: 0.372
Batch: 337/1685	Loss: 0.297
Batch: 505/1685	Loss: 0.205
Batch: 673/1685	Loss: 0.028
Batch: 841/1685	Loss: 0.112
Batch: 1009/1685	Loss: 0.186
Batch: 1177/1685	Loss: 0.204
Batch: 1345/1685	Loss: 0.055
Batch: 1513/1685	Loss: 0.246
Batch: 1681/1685	Loss: 0.128
Batch: 1685/1685	Loss: 0.072
Epoch: 10	Train Loss: 0.244	Val F1: 0.702
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 11 ******************************
Batch: 1/1685	Loss: 0.237
Batch: 169/1685	Loss: 0.484
Batch: 337/1685	Loss: 0.189
Batch: 505/1685	Loss: 0.058
Batch: 673/1685	Loss: 0.265
Batch: 841/1685	Loss: 0.398
Batch: 1009/1685	Loss: 0.394
Batch: 1177/1685	Loss: 0.180
Batch: 1345/1685	Loss: 0.520
Batch: 1513/1685	Loss: 0.100
Batch: 1681/1685	Loss: 0.138
Batch: 1685/1685	Loss: 0.286
Epoch: 11	Train Loss: 0.223	Val F1: 0.709
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 12 ******************************
Batch: 1/1685	Loss: 0.294
Batch: 169/1685	Loss: 0.026
Batch: 337/1685	Loss: 0.198
Batch: 505/1685	Loss: 0.358
Batch: 673/1685	Loss: 0.087
Batch: 841/1685	Loss: 0.832
Batch: 1009/1685	Loss: 0.157
Batch: 1177/1685	Loss: 0.184
Batch: 1345/1685	Loss: 0.035
Batch: 1513/1685	Loss: 0.013
Batch: 1681/1685	Loss: 0.198
Batch: 1685/1685	Loss: 0.014
Epoch: 12	Train Loss: 0.207	Val F1: 0.720
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 13 ******************************
Batch: 1/1685	Loss: 0.276
Batch: 169/1685	Loss: 0.010
Batch: 337/1685	Loss: 0.240
Batch: 505/1685	Loss: 0.018
Batch: 673/1685	Loss: 0.053
Batch: 841/1685	Loss: 0.052
Batch: 1009/1685	Loss: 0.102
Batch: 1177/1685	Loss: 0.137
Batch: 1345/1685	Loss: 0.043
Batch: 1513/1685	Loss: 0.283
Batch: 1681/1685	Loss: 0.809
Batch: 1685/1685	Loss: 0.102
Epoch: 13	Train Loss: 0.194	Val F1: 0.720
Best Epoch: 1	Best Val F1: 0.726

****************************** Epoch: 14 ******************************
Batch: 1/1685	Loss: 0.213
Batch: 169/1685	Loss: 0.042
Batch: 337/1685	Loss: 0.036
Batch: 505/1685	Loss: 0.119
Batch: 673/1685	Loss: 0.116
Batch: 841/1685	Loss: 0.311
Batch: 1009/1685	Loss: 0.385
Batch: 1177/1685	Loss: 0.098
Batch: 1345/1685	Loss: 0.177
Batch: 1513/1685	Loss: 0.149
Batch: 1681/1685	Loss: 0.668
Batch: 1685/1685	Loss: 0.006
Epoch: 14	Train Loss: 0.181	Val F1: 0.726
Best Epoch: 14	Best Val F1: 0.726

****************************** Epoch: 15 ******************************
Batch: 1/1685	Loss: 0.230
Batch: 169/1685	Loss: 0.006
Batch: 337/1685	Loss: 0.313
Batch: 505/1685	Loss: 0.181
Batch: 673/1685	Loss: 0.157
Batch: 841/1685	Loss: 0.003
Batch: 1009/1685	Loss: 0.363
Batch: 1177/1685	Loss: 0.030
Batch: 1345/1685	Loss: 0.297
Batch: 1513/1685	Loss: 0.102
Batch: 1681/1685	Loss: 0.112
Batch: 1685/1685	Loss: 0.003
Epoch: 15	Train Loss: 0.169	Val F1: 0.722
Best Epoch: 14	Best Val F1: 0.726

****************************** Epoch: 16 ******************************
Batch: 1/1685	Loss: 0.135
Batch: 169/1685	Loss: 0.162
Batch: 337/1685	Loss: 0.399
Batch: 505/1685	Loss: 0.331
Batch: 673/1685	Loss: 0.344
Batch: 841/1685	Loss: 0.517
Batch: 1009/1685	Loss: 0.027
Batch: 1177/1685	Loss: 0.106
Batch: 1345/1685	Loss: 0.007
Batch: 1513/1685	Loss: 0.077
Batch: 1681/1685	Loss: 0.115
Batch: 1685/1685	Loss: 0.117
Epoch: 16	Train Loss: 0.166	Val F1: 0.703
Best Epoch: 14	Best Val F1: 0.726

****************************** Epoch: 17 ******************************
Batch: 1/1685	Loss: 0.112
Batch: 169/1685	Loss: 0.092
Batch: 337/1685	Loss: 0.058
Batch: 505/1685	Loss: 0.086
Batch: 673/1685	Loss: 0.454
Batch: 841/1685	Loss: 0.007
Batch: 1009/1685	Loss: 0.123
Batch: 1177/1685	Loss: 0.152
Batch: 1345/1685	Loss: 1.125
Batch: 1513/1685	Loss: 0.326
Batch: 1681/1685	Loss: 0.131
Batch: 1685/1685	Loss: 0.045
Epoch: 17	Train Loss: 0.157	Val F1: 0.737
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 18 ******************************
Batch: 1/1685	Loss: 0.038
Batch: 169/1685	Loss: 0.055
Batch: 337/1685	Loss: 0.111
Batch: 505/1685	Loss: 0.064
Batch: 673/1685	Loss: 0.079
Batch: 841/1685	Loss: 0.240
Batch: 1009/1685	Loss: 0.035
Batch: 1177/1685	Loss: 0.072
Batch: 1345/1685	Loss: 0.231
Batch: 1513/1685	Loss: 0.022
Batch: 1681/1685	Loss: 0.100
Batch: 1685/1685	Loss: 0.284
Epoch: 18	Train Loss: 0.151	Val F1: 0.719
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 19 ******************************
Batch: 1/1685	Loss: 0.484
Batch: 169/1685	Loss: 0.006
Batch: 337/1685	Loss: 0.135
Batch: 505/1685	Loss: 0.200
Batch: 673/1685	Loss: 0.034
Batch: 841/1685	Loss: 0.147
Batch: 1009/1685	Loss: 0.151
Batch: 1177/1685	Loss: 0.164
Batch: 1345/1685	Loss: 0.073
Batch: 1513/1685	Loss: 0.187
Batch: 1681/1685	Loss: 0.105
Batch: 1685/1685	Loss: 0.058
Epoch: 19	Train Loss: 0.144	Val F1: 0.709
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 20 ******************************
Batch: 1/1685	Loss: 0.013
Batch: 169/1685	Loss: 0.145
Batch: 337/1685	Loss: 0.133
Batch: 505/1685	Loss: 0.043
Batch: 673/1685	Loss: 0.179
Batch: 841/1685	Loss: 0.072
Batch: 1009/1685	Loss: 0.120
Batch: 1177/1685	Loss: 0.177
Batch: 1345/1685	Loss: 0.116
Batch: 1513/1685	Loss: 0.069
Batch: 1681/1685	Loss: 0.029
Batch: 1685/1685	Loss: 0.040
Epoch: 20	Train Loss: 0.141	Val F1: 0.690
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 21 ******************************
Batch: 1/1685	Loss: 0.010
Batch: 169/1685	Loss: 0.018
Batch: 337/1685	Loss: 0.139
Batch: 505/1685	Loss: 0.086
Batch: 673/1685	Loss: 0.103
Batch: 841/1685	Loss: 0.133
Batch: 1009/1685	Loss: 0.095
Batch: 1177/1685	Loss: 0.077
Batch: 1345/1685	Loss: 0.028
Batch: 1513/1685	Loss: 0.172
Batch: 1681/1685	Loss: 0.007
Batch: 1685/1685	Loss: 0.076
Epoch: 21	Train Loss: 0.134	Val F1: 0.701
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 22 ******************************
Batch: 1/1685	Loss: 0.155
Batch: 169/1685	Loss: 0.132
Batch: 337/1685	Loss: 0.001
Batch: 505/1685	Loss: 0.003
Batch: 673/1685	Loss: 0.220
Batch: 841/1685	Loss: 0.042
Batch: 1009/1685	Loss: 0.302
Batch: 1177/1685	Loss: 0.137
Batch: 1345/1685	Loss: 0.130
Batch: 1513/1685	Loss: 0.005
Batch: 1681/1685	Loss: 0.127
Batch: 1685/1685	Loss: 0.273
Epoch: 22	Train Loss: 0.136	Val F1: 0.717
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 23 ******************************
Batch: 1/1685	Loss: 0.037
Batch: 169/1685	Loss: 0.001
Batch: 337/1685	Loss: 0.267
Batch: 505/1685	Loss: 0.003
Batch: 673/1685	Loss: 0.042
Batch: 841/1685	Loss: 0.009
Batch: 1009/1685	Loss: 0.010
Batch: 1177/1685	Loss: 0.145
Batch: 1345/1685	Loss: 0.140
Batch: 1513/1685	Loss: 0.075
Batch: 1681/1685	Loss: 0.898
Batch: 1685/1685	Loss: 0.130
Epoch: 23	Train Loss: 0.131	Val F1: 0.715
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 24 ******************************
Batch: 1/1685	Loss: 0.113
Batch: 169/1685	Loss: 0.021
Batch: 337/1685	Loss: 0.021
Batch: 505/1685	Loss: 0.002
Batch: 673/1685	Loss: 0.069
Batch: 841/1685	Loss: 0.154
Batch: 1009/1685	Loss: 0.080
Batch: 1177/1685	Loss: 0.003
Batch: 1345/1685	Loss: 0.133
Batch: 1513/1685	Loss: 0.027
Batch: 1681/1685	Loss: 0.017
Batch: 1685/1685	Loss: 0.090
Epoch: 24	Train Loss: 0.125	Val F1: 0.703
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 25 ******************************
Batch: 1/1685	Loss: 0.269
Batch: 169/1685	Loss: 0.001
Batch: 337/1685	Loss: 0.097
Batch: 505/1685	Loss: 0.105
Batch: 673/1685	Loss: 0.145
Batch: 841/1685	Loss: 0.127
Batch: 1009/1685	Loss: 0.141
Batch: 1177/1685	Loss: 0.032
Batch: 1345/1685	Loss: 0.003
Batch: 1513/1685	Loss: 0.122
Batch: 1681/1685	Loss: 0.097
Batch: 1685/1685	Loss: 0.048
Epoch: 25	Train Loss: 0.125	Val F1: 0.715
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 26 ******************************
Batch: 1/1685	Loss: 0.089
Batch: 169/1685	Loss: 0.259
Batch: 337/1685	Loss: 0.103
Batch: 505/1685	Loss: 0.410
Batch: 673/1685	Loss: 0.141
Batch: 841/1685	Loss: 0.324
Batch: 1009/1685	Loss: 0.104
Batch: 1177/1685	Loss: 0.260
Batch: 1345/1685	Loss: 0.133
Batch: 1513/1685	Loss: 0.397
Batch: 1681/1685	Loss: 0.149
Batch: 1685/1685	Loss: 0.006
Epoch: 26	Train Loss: 0.116	Val F1: 0.721
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 27 ******************************
Batch: 1/1685	Loss: 0.083
Batch: 169/1685	Loss: 0.043
Batch: 337/1685	Loss: 0.225
Batch: 505/1685	Loss: 0.102
Batch: 673/1685	Loss: 0.126
Batch: 841/1685	Loss: 0.172
Batch: 1009/1685	Loss: 0.235
Batch: 1177/1685	Loss: 0.178
Batch: 1345/1685	Loss: 0.064
Batch: 1513/1685	Loss: 0.234
Batch: 1681/1685	Loss: 0.013
Batch: 1685/1685	Loss: 0.330
Epoch: 27	Train Loss: 0.120	Val F1: 0.720
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 28 ******************************
Batch: 1/1685	Loss: 0.009
Batch: 169/1685	Loss: 0.067
Batch: 337/1685	Loss: 0.003
Batch: 505/1685	Loss: 0.091
Batch: 673/1685	Loss: 0.001
Batch: 841/1685	Loss: 0.001
Batch: 1009/1685	Loss: 0.001
Batch: 1177/1685	Loss: 0.002
Batch: 1345/1685	Loss: 0.092
Batch: 1513/1685	Loss: 0.157
Batch: 1681/1685	Loss: 0.113
Batch: 1685/1685	Loss: 0.311
Epoch: 28	Train Loss: 0.112	Val F1: 0.700
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 29 ******************************
Batch: 1/1685	Loss: 0.343
Batch: 169/1685	Loss: 0.011
Batch: 337/1685	Loss: 0.093
Batch: 505/1685	Loss: 0.002
Batch: 673/1685	Loss: 0.194
Batch: 841/1685	Loss: 0.527
Batch: 1009/1685	Loss: 0.676
Batch: 1177/1685	Loss: 0.178
Batch: 1345/1685	Loss: 0.372
Batch: 1513/1685	Loss: 0.003
Batch: 1681/1685	Loss: 0.137
Batch: 1685/1685	Loss: 0.063
Epoch: 29	Train Loss: 0.116	Val F1: 0.717
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 30 ******************************
Batch: 1/1685	Loss: 0.007
Batch: 169/1685	Loss: 0.004
Batch: 337/1685	Loss: 0.321
Batch: 505/1685	Loss: 0.091
Batch: 673/1685	Loss: 0.012
Batch: 841/1685	Loss: 0.046
Batch: 1009/1685	Loss: 0.154
Batch: 1177/1685	Loss: 0.158
Batch: 1345/1685	Loss: 0.100
Batch: 1513/1685	Loss: 0.032
Batch: 1681/1685	Loss: 0.088
Batch: 1685/1685	Loss: 0.005
Epoch: 30	Train Loss: 0.110	Val F1: 0.703
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 31 ******************************
Batch: 1/1685	Loss: 0.172
Batch: 169/1685	Loss: 0.045
Batch: 337/1685	Loss: 0.015
Batch: 505/1685	Loss: 0.002
Batch: 673/1685	Loss: 0.117
Batch: 841/1685	Loss: 0.101
Batch: 1009/1685	Loss: 0.062
Batch: 1177/1685	Loss: 0.002
Batch: 1345/1685	Loss: 0.061
Batch: 1513/1685	Loss: 0.194
Batch: 1681/1685	Loss: 0.074
Batch: 1685/1685	Loss: 0.379
Epoch: 31	Train Loss: 0.109	Val F1: 0.725
Best Epoch: 17	Best Val F1: 0.737

****************************** Epoch: 32 ******************************
Batch: 1/1685	Loss: 0.058
Batch: 169/1685	Loss: 0.075
Batch: 337/1685	Loss: 0.227
Batch: 505/1685	Loss: 0.014
Batch: 673/1685	Loss: 0.095
Batch: 841/1685	Loss: 0.003
Batch: 1009/1685	Loss: 0.070
Batch: 1177/1685	Loss: 0.120
Batch: 1345/1685	Loss: 0.004
Batch: 1513/1685	Loss: 0.122
Batch: 1681/1685	Loss: 0.009
Batch: 1685/1685	Loss: 0.287
Epoch: 32	Train Loss: 0.108	Val F1: 0.713
Best Epoch: 17	Best Val F1: 0.737

Saving the best checkpoint....
Inference...
Test F1: 0.730	Test F1_Few: 0.747	Test F1_Zero: 0.712
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 3.434774207140354e-05, 'lr': 5.518688732730609e-06, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.050
Batch: 43/211	Loss: 1.116
Batch: 64/211	Loss: 1.039
Batch: 85/211	Loss: 1.064
Batch: 106/211	Loss: 1.028
Batch: 127/211	Loss: 1.031
Batch: 148/211	Loss: 1.061
Batch: 169/211	Loss: 1.007
Batch: 190/211	Loss: 1.127
Batch: 211/211	Loss: 1.016
Epoch: 1	Train Loss: 1.043	Val F1: 0.312
Best Epoch: 1	Best Val F1: 0.312

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 1.034
Batch: 22/211	Loss: 0.983
Batch: 43/211	Loss: 0.973
Batch: 64/211	Loss: 0.917
Batch: 85/211	Loss: 0.835
Batch: 106/211	Loss: 0.863
Batch: 127/211	Loss: 0.826
Batch: 148/211	Loss: 0.737
Batch: 169/211	Loss: 0.696
Batch: 190/211	Loss: 0.658
Batch: 211/211	Loss: 0.608
Epoch: 2	Train Loss: 0.840	Val F1: 0.703
Best Epoch: 2	Best Val F1: 0.703

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.756
Batch: 22/211	Loss: 0.711
Batch: 43/211	Loss: 0.691
Batch: 64/211	Loss: 0.617
Batch: 85/211	Loss: 0.635
Batch: 106/211	Loss: 0.590
Batch: 127/211	Loss: 0.550
Batch: 148/211	Loss: 0.704
Batch: 169/211	Loss: 0.695
Batch: 190/211	Loss: 0.746
Batch: 211/211	Loss: 0.661
Epoch: 3	Train Loss: 0.659	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.717

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.516
Batch: 22/211	Loss: 0.647
Batch: 43/211	Loss: 0.592
Batch: 64/211	Loss: 0.596
Batch: 85/211	Loss: 0.599
Batch: 106/211	Loss: 0.521
Batch: 127/211	Loss: 0.570
Batch: 148/211	Loss: 0.623
Batch: 169/211	Loss: 0.674
Batch: 190/211	Loss: 0.666
Batch: 211/211	Loss: 0.595
Epoch: 4	Train Loss: 0.613	Val F1: 0.708
Best Epoch: 3	Best Val F1: 0.717

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.669
Batch: 22/211	Loss: 0.570
Batch: 43/211	Loss: 0.541
Batch: 64/211	Loss: 0.502
Batch: 85/211	Loss: 0.602
Batch: 106/211	Loss: 0.536
Batch: 127/211	Loss: 0.463
Batch: 148/211	Loss: 0.548
Batch: 169/211	Loss: 0.555
Batch: 190/211	Loss: 0.629
Batch: 211/211	Loss: 0.654
Epoch: 5	Train Loss: 0.585	Val F1: 0.723
Best Epoch: 5	Best Val F1: 0.723

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.511
Batch: 22/211	Loss: 0.693
Batch: 43/211	Loss: 0.573
Batch: 64/211	Loss: 0.492
Batch: 85/211	Loss: 0.541
Batch: 106/211	Loss: 0.648
Batch: 127/211	Loss: 0.510
Batch: 148/211	Loss: 0.538
Batch: 169/211	Loss: 0.688
Batch: 190/211	Loss: 0.545
Batch: 211/211	Loss: 0.345
Epoch: 6	Train Loss: 0.563	Val F1: 0.729
Best Epoch: 6	Best Val F1: 0.729

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.595
Batch: 22/211	Loss: 0.628
Batch: 43/211	Loss: 0.504
Batch: 64/211	Loss: 0.490
Batch: 85/211	Loss: 0.580
Batch: 106/211	Loss: 0.538
Batch: 127/211	Loss: 0.736
Batch: 148/211	Loss: 0.612
Batch: 169/211	Loss: 0.602
Batch: 190/211	Loss: 0.552
Batch: 211/211	Loss: 0.609
Epoch: 7	Train Loss: 0.540	Val F1: 0.726
Best Epoch: 6	Best Val F1: 0.729

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.487
Batch: 22/211	Loss: 0.503
Batch: 43/211	Loss: 0.452
Batch: 64/211	Loss: 0.540
Batch: 85/211	Loss: 0.784
Batch: 106/211	Loss: 0.492
Batch: 127/211	Loss: 0.414
Batch: 148/211	Loss: 0.580
Batch: 169/211	Loss: 0.621
Batch: 190/211	Loss: 0.436
Batch: 211/211	Loss: 0.618
Epoch: 8	Train Loss: 0.521	Val F1: 0.727
Best Epoch: 6	Best Val F1: 0.729

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.550
Batch: 22/211	Loss: 0.455
Batch: 43/211	Loss: 0.511
Batch: 64/211	Loss: 0.545
Batch: 85/211	Loss: 0.617
Batch: 106/211	Loss: 0.450
Batch: 127/211	Loss: 0.650
Batch: 148/211	Loss: 0.515
Batch: 169/211	Loss: 0.423
Batch: 190/211	Loss: 0.510
Batch: 211/211	Loss: 0.458
Epoch: 9	Train Loss: 0.501	Val F1: 0.734
Best Epoch: 9	Best Val F1: 0.734

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.475
Batch: 22/211	Loss: 0.493
Batch: 43/211	Loss: 0.562
Batch: 64/211	Loss: 0.360
Batch: 85/211	Loss: 0.436
Batch: 106/211	Loss: 0.575
Batch: 127/211	Loss: 0.460
Batch: 148/211	Loss: 0.449
Batch: 169/211	Loss: 0.391
Batch: 190/211	Loss: 0.379
Batch: 211/211	Loss: 0.369
Epoch: 10	Train Loss: 0.475	Val F1: 0.732
Best Epoch: 9	Best Val F1: 0.734

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.349
Batch: 22/211	Loss: 0.366
Batch: 43/211	Loss: 0.486
Batch: 64/211	Loss: 0.488
Batch: 85/211	Loss: 0.438
Batch: 106/211	Loss: 0.452
Batch: 127/211	Loss: 0.641
Batch: 148/211	Loss: 0.496
Batch: 169/211	Loss: 0.497
Batch: 190/211	Loss: 0.523
Batch: 211/211	Loss: 0.522
Epoch: 11	Train Loss: 0.462	Val F1: 0.736
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 12 ******************************
Batch: 1/211	Loss: 0.467
Batch: 22/211	Loss: 0.403
Batch: 43/211	Loss: 0.460
Batch: 64/211	Loss: 0.457
Batch: 85/211	Loss: 0.419
Batch: 106/211	Loss: 0.506
Batch: 127/211	Loss: 0.348
Batch: 148/211	Loss: 0.536
Batch: 169/211	Loss: 0.478
Batch: 190/211	Loss: 0.327
Batch: 211/211	Loss: 0.462
Epoch: 12	Train Loss: 0.440	Val F1: 0.734
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 13 ******************************
Batch: 1/211	Loss: 0.478
Batch: 22/211	Loss: 0.449
Batch: 43/211	Loss: 0.366
Batch: 64/211	Loss: 0.300
Batch: 85/211	Loss: 0.418
Batch: 106/211	Loss: 0.536
Batch: 127/211	Loss: 0.428
Batch: 148/211	Loss: 0.570
Batch: 169/211	Loss: 0.500
Batch: 190/211	Loss: 0.345
Batch: 211/211	Loss: 0.325
Epoch: 13	Train Loss: 0.423	Val F1: 0.720
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 14 ******************************
Batch: 1/211	Loss: 0.508
Batch: 22/211	Loss: 0.622
Batch: 43/211	Loss: 0.384
Batch: 64/211	Loss: 0.323
Batch: 85/211	Loss: 0.350
Batch: 106/211	Loss: 0.419
Batch: 127/211	Loss: 0.306
Batch: 148/211	Loss: 0.405
Batch: 169/211	Loss: 0.312
Batch: 190/211	Loss: 0.341
Batch: 211/211	Loss: 0.543
Epoch: 14	Train Loss: 0.399	Val F1: 0.712
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 15 ******************************
Batch: 1/211	Loss: 0.542
Batch: 22/211	Loss: 0.294
Batch: 43/211	Loss: 0.438
Batch: 64/211	Loss: 0.425
Batch: 85/211	Loss: 0.279
Batch: 106/211	Loss: 0.312
Batch: 127/211	Loss: 0.576
Batch: 148/211	Loss: 0.454
Batch: 169/211	Loss: 0.441
Batch: 190/211	Loss: 0.430
Batch: 211/211	Loss: 0.351
Epoch: 15	Train Loss: 0.381	Val F1: 0.721
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 16 ******************************
Batch: 1/211	Loss: 0.326
Batch: 22/211	Loss: 0.446
Batch: 43/211	Loss: 0.414
Batch: 64/211	Loss: 0.339
Batch: 85/211	Loss: 0.384
Batch: 106/211	Loss: 0.292
Batch: 127/211	Loss: 0.355
Batch: 148/211	Loss: 0.353
Batch: 169/211	Loss: 0.211
Batch: 190/211	Loss: 0.317
Batch: 211/211	Loss: 0.463
Epoch: 16	Train Loss: 0.366	Val F1: 0.717
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 17 ******************************
Batch: 1/211	Loss: 0.346
Batch: 22/211	Loss: 0.376
Batch: 43/211	Loss: 0.289
Batch: 64/211	Loss: 0.273
Batch: 85/211	Loss: 0.288
Batch: 106/211	Loss: 0.294
Batch: 127/211	Loss: 0.409
Batch: 148/211	Loss: 0.225
Batch: 169/211	Loss: 0.425
Batch: 190/211	Loss: 0.578
Batch: 211/211	Loss: 0.278
Epoch: 17	Train Loss: 0.349	Val F1: 0.713
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 18 ******************************
Batch: 1/211	Loss: 0.455
Batch: 22/211	Loss: 0.267
Batch: 43/211	Loss: 0.348
Batch: 64/211	Loss: 0.328
Batch: 85/211	Loss: 0.348
Batch: 106/211	Loss: 0.284
Batch: 127/211	Loss: 0.296
Batch: 148/211	Loss: 0.424
Batch: 169/211	Loss: 0.267
Batch: 190/211	Loss: 0.264
Batch: 211/211	Loss: 0.391
Epoch: 18	Train Loss: 0.337	Val F1: 0.726
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 19 ******************************
Batch: 1/211	Loss: 0.318
Batch: 22/211	Loss: 0.356
Batch: 43/211	Loss: 0.352
Batch: 64/211	Loss: 0.565
Batch: 85/211	Loss: 0.404
Batch: 106/211	Loss: 0.259
Batch: 127/211	Loss: 0.382
Batch: 148/211	Loss: 0.394
Batch: 169/211	Loss: 0.421
Batch: 190/211	Loss: 0.323
Batch: 211/211	Loss: 0.198
Epoch: 19	Train Loss: 0.322	Val F1: 0.716
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 20 ******************************
Batch: 1/211	Loss: 0.247
Batch: 22/211	Loss: 0.221
Batch: 43/211	Loss: 0.357
Batch: 64/211	Loss: 0.415
Batch: 85/211	Loss: 0.306
Batch: 106/211	Loss: 0.403
Batch: 127/211	Loss: 0.304
Batch: 148/211	Loss: 0.373
Batch: 169/211	Loss: 0.362
Batch: 190/211	Loss: 0.389
Batch: 211/211	Loss: 0.292
Epoch: 20	Train Loss: 0.308	Val F1: 0.713
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 21 ******************************
Batch: 1/211	Loss: 0.227
Batch: 22/211	Loss: 0.219
Batch: 43/211	Loss: 0.158
Batch: 64/211	Loss: 0.232
Batch: 85/211	Loss: 0.158
Batch: 106/211	Loss: 0.337
Batch: 127/211	Loss: 0.347
Batch: 148/211	Loss: 0.234
Batch: 169/211	Loss: 0.368
Batch: 190/211	Loss: 0.351
Batch: 211/211	Loss: 0.195
Epoch: 21	Train Loss: 0.301	Val F1: 0.715
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 22 ******************************
Batch: 1/211	Loss: 0.312
Batch: 22/211	Loss: 0.250
Batch: 43/211	Loss: 0.488
Batch: 64/211	Loss: 0.240
Batch: 85/211	Loss: 0.300
Batch: 106/211	Loss: 0.230
Batch: 127/211	Loss: 0.262
Batch: 148/211	Loss: 0.266
Batch: 169/211	Loss: 0.318
Batch: 190/211	Loss: 0.205
Batch: 211/211	Loss: 0.276
Epoch: 22	Train Loss: 0.287	Val F1: 0.719
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 23 ******************************
Batch: 1/211	Loss: 0.251
Batch: 22/211	Loss: 0.231
Batch: 43/211	Loss: 0.365
Batch: 64/211	Loss: 0.223
Batch: 85/211	Loss: 0.175
Batch: 106/211	Loss: 0.238
Batch: 127/211	Loss: 0.305
Batch: 148/211	Loss: 0.260
Batch: 169/211	Loss: 0.417
Batch: 190/211	Loss: 0.241
Batch: 211/211	Loss: 0.214
Epoch: 23	Train Loss: 0.283	Val F1: 0.720
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 24 ******************************
Batch: 1/211	Loss: 0.354
Batch: 22/211	Loss: 0.191
Batch: 43/211	Loss: 0.206
Batch: 64/211	Loss: 0.211
Batch: 85/211	Loss: 0.256
Batch: 106/211	Loss: 0.205
Batch: 127/211	Loss: 0.323
Batch: 148/211	Loss: 0.275
Batch: 169/211	Loss: 0.241
Batch: 190/211	Loss: 0.247
Batch: 211/211	Loss: 0.292
Epoch: 24	Train Loss: 0.268	Val F1: 0.722
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 25 ******************************
Batch: 1/211	Loss: 0.231
Batch: 22/211	Loss: 0.298
Batch: 43/211	Loss: 0.306
Batch: 64/211	Loss: 0.203
Batch: 85/211	Loss: 0.156
Batch: 106/211	Loss: 0.238
Batch: 127/211	Loss: 0.255
Batch: 148/211	Loss: 0.156
Batch: 169/211	Loss: 0.228
Batch: 190/211	Loss: 0.129
Batch: 211/211	Loss: 0.424
Epoch: 25	Train Loss: 0.255	Val F1: 0.714
Best Epoch: 11	Best Val F1: 0.736

****************************** Epoch: 26 ******************************
Batch: 1/211	Loss: 0.224
Batch: 22/211	Loss: 0.319
Batch: 43/211	Loss: 0.188
Batch: 64/211	Loss: 0.242
Batch: 85/211	Loss: 0.294
Batch: 106/211	Loss: 0.336
Batch: 127/211	Loss: 0.206
Batch: 148/211	Loss: 0.318
Batch: 169/211	Loss: 0.172
Batch: 190/211	Loss: 0.311
Batch: 211/211	Loss: 0.258
Epoch: 26	Train Loss: 0.247	Val F1: 0.720
Best Epoch: 11	Best Val F1: 0.736

Saving the best checkpoint....
Inference...
Test F1: 0.741	Test F1_Few: 0.736	Test F1_Zero: 0.746
Starting training with config: {'batch_size': 4, 'epochs': 75, 'l2_reg': 7.796991299028495e-05, 'lr': 5.032678327176544e-06, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/3370	Loss: 1.064
Batch: 338/3370	Loss: 1.206
Batch: 675/3370	Loss: 1.053
Batch: 1012/3370	Loss: 1.012
Batch: 1349/3370	Loss: 0.738
Batch: 1686/3370	Loss: 1.183
Batch: 2023/3370	Loss: 0.486
Batch: 2360/3370	Loss: 1.274
Batch: 2697/3370	Loss: 0.859
Batch: 3034/3370	Loss: 0.634
Batch: 3370/3370	Loss: 1.190
Epoch: 1	Train Loss: 0.808	Val F1: 0.715
Best Epoch: 1	Best Val F1: 0.715

****************************** Epoch: 2 ******************************
Batch: 1/3370	Loss: 0.555
Batch: 338/3370	Loss: 0.936
Batch: 675/3370	Loss: 0.573
Batch: 1012/3370	Loss: 0.693
Batch: 1349/3370	Loss: 0.571
Batch: 1686/3370	Loss: 0.430
Batch: 2023/3370	Loss: 0.555
Batch: 2360/3370	Loss: 0.451
Batch: 2697/3370	Loss: 0.607
Batch: 3034/3370	Loss: 0.601
Batch: 3370/3370	Loss: 0.107
Epoch: 2	Train Loss: 0.607	Val F1: 0.723
Best Epoch: 2	Best Val F1: 0.723

****************************** Epoch: 3 ******************************
Batch: 1/3370	Loss: 0.403
Batch: 338/3370	Loss: 0.504
Batch: 675/3370	Loss: 0.771
Batch: 1012/3370	Loss: 0.727
Batch: 1349/3370	Loss: 0.515
Batch: 1686/3370	Loss: 0.834
Batch: 2023/3370	Loss: 0.435
Batch: 2360/3370	Loss: 0.336
Batch: 2697/3370	Loss: 0.234
Batch: 3034/3370	Loss: 0.217
Batch: 3370/3370	Loss: 0.148
Epoch: 3	Train Loss: 0.554	Val F1: 0.743
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 4 ******************************
Batch: 1/3370	Loss: 0.302
Batch: 338/3370	Loss: 0.760
Batch: 675/3370	Loss: 0.396
Batch: 1012/3370	Loss: 0.950
Batch: 1349/3370	Loss: 0.251
Batch: 1686/3370	Loss: 0.201
Batch: 2023/3370	Loss: 0.468
Batch: 2360/3370	Loss: 0.419
Batch: 2697/3370	Loss: 0.677
Batch: 3034/3370	Loss: 0.669
Batch: 3370/3370	Loss: 0.008
Epoch: 4	Train Loss: 0.500	Val F1: 0.735
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 5 ******************************
Batch: 1/3370	Loss: 0.500
Batch: 338/3370	Loss: 0.488
Batch: 675/3370	Loss: 0.247
Batch: 1012/3370	Loss: 0.164
Batch: 1349/3370	Loss: 0.108
Batch: 1686/3370	Loss: 0.503
Batch: 2023/3370	Loss: 0.106
Batch: 2360/3370	Loss: 0.139
Batch: 2697/3370	Loss: 1.155
Batch: 3034/3370	Loss: 0.429
Batch: 3370/3370	Loss: 0.772
Epoch: 5	Train Loss: 0.450	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 6 ******************************
Batch: 1/3370	Loss: 0.087
Batch: 338/3370	Loss: 0.171
Batch: 675/3370	Loss: 0.530
Batch: 1012/3370	Loss: 0.215
Batch: 1349/3370	Loss: 0.223
Batch: 1686/3370	Loss: 0.687
Batch: 2023/3370	Loss: 0.087
Batch: 2360/3370	Loss: 0.524
Batch: 2697/3370	Loss: 0.448
Batch: 3034/3370	Loss: 1.283
Batch: 3370/3370	Loss: 0.003
Epoch: 6	Train Loss: 0.413	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 7 ******************************
Batch: 1/3370	Loss: 0.277
Batch: 338/3370	Loss: 0.175
Batch: 675/3370	Loss: 0.357
Batch: 1012/3370	Loss: 0.035
Batch: 1349/3370	Loss: 0.075
Batch: 1686/3370	Loss: 0.609
Batch: 2023/3370	Loss: 0.532
Batch: 2360/3370	Loss: 0.291
Batch: 2697/3370	Loss: 0.511
Batch: 3034/3370	Loss: 0.120
Batch: 3370/3370	Loss: 0.006
Epoch: 7	Train Loss: 0.375	Val F1: 0.683
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 8 ******************************
Batch: 1/3370	Loss: 0.338
Batch: 338/3370	Loss: 1.177
Batch: 675/3370	Loss: 0.009
Batch: 1012/3370	Loss: 0.208
Batch: 1349/3370	Loss: 0.147
Batch: 1686/3370	Loss: 0.225
Batch: 2023/3370	Loss: 0.327
Batch: 2360/3370	Loss: 0.539
Batch: 2697/3370	Loss: 0.096
Batch: 3034/3370	Loss: 0.335
Batch: 3370/3370	Loss: 0.027
Epoch: 8	Train Loss: 0.347	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 9 ******************************
Batch: 1/3370	Loss: 1.484
Batch: 338/3370	Loss: 0.861
Batch: 675/3370	Loss: 1.251
Batch: 1012/3370	Loss: 0.158
Batch: 1349/3370	Loss: 0.363
Batch: 1686/3370	Loss: 0.430
Batch: 2023/3370	Loss: 0.116
Batch: 2360/3370	Loss: 0.154
Batch: 2697/3370	Loss: 0.108
Batch: 3034/3370	Loss: 0.265
Batch: 3370/3370	Loss: 0.226
Epoch: 9	Train Loss: 0.315	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 10 ******************************
Batch: 1/3370	Loss: 0.355
Batch: 338/3370	Loss: 0.602
Batch: 675/3370	Loss: 0.362
Batch: 1012/3370	Loss: 0.269
Batch: 1349/3370	Loss: 0.054
Batch: 1686/3370	Loss: 0.269
Batch: 2023/3370	Loss: 0.505
Batch: 2360/3370	Loss: 0.020
Batch: 2697/3370	Loss: 0.458
Batch: 3034/3370	Loss: 0.368
Batch: 3370/3370	Loss: 0.655
Epoch: 10	Train Loss: 0.289	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 11 ******************************
Batch: 1/3370	Loss: 0.087
Batch: 338/3370	Loss: 0.096
Batch: 675/3370	Loss: 0.046
Batch: 1012/3370	Loss: 0.012
Batch: 1349/3370	Loss: 0.058
Batch: 1686/3370	Loss: 0.014
Batch: 2023/3370	Loss: 0.131
Batch: 2360/3370	Loss: 0.234
Batch: 2697/3370	Loss: 0.096
Batch: 3034/3370	Loss: 0.298
Batch: 3370/3370	Loss: 0.001
Epoch: 11	Train Loss: 0.267	Val F1: 0.701
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 12 ******************************
Batch: 1/3370	Loss: 0.521
Batch: 338/3370	Loss: 0.006
Batch: 675/3370	Loss: 0.180
Batch: 1012/3370	Loss: 0.039
Batch: 1349/3370	Loss: 0.163
Batch: 1686/3370	Loss: 0.698
Batch: 2023/3370	Loss: 0.106
Batch: 2360/3370	Loss: 0.051
Batch: 2697/3370	Loss: 0.059
Batch: 3034/3370	Loss: 0.330
Batch: 3370/3370	Loss: 0.014
Epoch: 12	Train Loss: 0.250	Val F1: 0.710
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 13 ******************************
Batch: 1/3370	Loss: 0.027
Batch: 338/3370	Loss: 0.026
Batch: 675/3370	Loss: 0.231
Batch: 1012/3370	Loss: 0.023
Batch: 1349/3370	Loss: 0.180
Batch: 1686/3370	Loss: 0.370
Batch: 2023/3370	Loss: 0.022
Batch: 2360/3370	Loss: 0.095
Batch: 2697/3370	Loss: 0.090
Batch: 3034/3370	Loss: 0.023
Batch: 3370/3370	Loss: 0.159
Epoch: 13	Train Loss: 0.230	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 14 ******************************
Batch: 1/3370	Loss: 0.361
Batch: 338/3370	Loss: 0.076
Batch: 675/3370	Loss: 0.052
Batch: 1012/3370	Loss: 0.009
Batch: 1349/3370	Loss: 0.005
Batch: 1686/3370	Loss: 0.157
Batch: 2023/3370	Loss: 0.073
Batch: 2360/3370	Loss: 0.024
Batch: 2697/3370	Loss: 0.023
Batch: 3034/3370	Loss: 0.005
Batch: 3370/3370	Loss: 0.057
Epoch: 14	Train Loss: 0.220	Val F1: 0.682
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 15 ******************************
Batch: 1/3370	Loss: 0.020
Batch: 338/3370	Loss: 0.032
Batch: 675/3370	Loss: 0.065
Batch: 1012/3370	Loss: 0.132
Batch: 1349/3370	Loss: 0.208
Batch: 1686/3370	Loss: 0.085
Batch: 2023/3370	Loss: 0.149
Batch: 2360/3370	Loss: 0.003
Batch: 2697/3370	Loss: 0.062
Batch: 3034/3370	Loss: 0.393
Batch: 3370/3370	Loss: 0.004
Epoch: 15	Train Loss: 0.201	Val F1: 0.691
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 16 ******************************
Batch: 1/3370	Loss: 0.134
Batch: 338/3370	Loss: 0.043
Batch: 675/3370	Loss: 0.129
Batch: 1012/3370	Loss: 0.010
Batch: 1349/3370	Loss: 0.210
Batch: 1686/3370	Loss: 0.006
Batch: 2023/3370	Loss: 0.249
Batch: 2360/3370	Loss: 0.883
Batch: 2697/3370	Loss: 0.057
Batch: 3034/3370	Loss: 0.643
Batch: 3370/3370	Loss: 0.092
Epoch: 16	Train Loss: 0.190	Val F1: 0.710
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 17 ******************************
Batch: 1/3370	Loss: 0.159
Batch: 338/3370	Loss: 0.008
Batch: 675/3370	Loss: 0.325
Batch: 1012/3370	Loss: 0.224
Batch: 1349/3370	Loss: 0.046
Batch: 1686/3370	Loss: 0.182
Batch: 2023/3370	Loss: 0.010
Batch: 2360/3370	Loss: 0.399
Batch: 2697/3370	Loss: 0.491
Batch: 3034/3370	Loss: 0.144
Batch: 3370/3370	Loss: 0.053
Epoch: 17	Train Loss: 0.184	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.743

****************************** Epoch: 18 ******************************
Batch: 1/3370	Loss: 0.014
Batch: 338/3370	Loss: 0.024
Batch: 675/3370	Loss: 0.282
Batch: 1012/3370	Loss: 0.300
Batch: 1349/3370	Loss: 0.008
Batch: 1686/3370	Loss: 0.179
Batch: 2023/3370	Loss: 0.032
Batch: 2360/3370	Loss: 0.004
Batch: 2697/3370	Loss: 0.044
Batch: 3034/3370	Loss: 0.148
Batch: 3370/3370	Loss: 0.003
Epoch: 18	Train Loss: 0.169	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.743

Saving the best checkpoint....
Inference...
Test F1: 0.745	Test F1_Few: 0.744	Test F1_Zero: 0.745
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 3.208310278659775e-05, 'lr': 6.399862485604724e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/422	Loss: 1.084
Batch: 43/422	Loss: 1.084
Batch: 85/422	Loss: 1.148
Batch: 127/422	Loss: 1.063
Batch: 169/422	Loss: 1.091
Batch: 211/422	Loss: 1.000
Batch: 253/422	Loss: 1.089
Batch: 295/422	Loss: 1.042
Batch: 337/422	Loss: 1.017
Batch: 379/422	Loss: 0.985
Batch: 421/422	Loss: 0.901
Batch: 422/422	Loss: 0.736
Epoch: 1	Train Loss: 1.015	Val F1: 0.588
Best Epoch: 1	Best Val F1: 0.588

****************************** Epoch: 2 ******************************
Batch: 1/422	Loss: 0.822
Batch: 43/422	Loss: 0.789
Batch: 85/422	Loss: 0.749
Batch: 127/422	Loss: 0.674
Batch: 169/422	Loss: 0.554
Batch: 211/422	Loss: 0.743
Batch: 253/422	Loss: 0.642
Batch: 295/422	Loss: 0.693
Batch: 337/422	Loss: 0.688
Batch: 379/422	Loss: 0.553
Batch: 421/422	Loss: 0.606
Batch: 422/422	Loss: 0.593
Epoch: 2	Train Loss: 0.675	Val F1: 0.735
Best Epoch: 2	Best Val F1: 0.735

****************************** Epoch: 3 ******************************
Batch: 1/422	Loss: 0.631
Batch: 43/422	Loss: 0.560
Batch: 85/422	Loss: 0.764
Batch: 127/422	Loss: 0.512
Batch: 169/422	Loss: 0.516
Batch: 211/422	Loss: 0.663
Batch: 253/422	Loss: 0.498
Batch: 295/422	Loss: 0.723
Batch: 337/422	Loss: 0.485
Batch: 379/422	Loss: 0.698
Batch: 421/422	Loss: 0.580
Batch: 422/422	Loss: 0.931
Epoch: 3	Train Loss: 0.602	Val F1: 0.744
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 4 ******************************
Batch: 1/422	Loss: 0.513
Batch: 43/422	Loss: 0.622
Batch: 85/422	Loss: 0.677
Batch: 127/422	Loss: 0.589
Batch: 169/422	Loss: 0.653
Batch: 211/422	Loss: 0.485
Batch: 253/422	Loss: 0.482
Batch: 295/422	Loss: 0.505
Batch: 337/422	Loss: 0.666
Batch: 379/422	Loss: 0.834
Batch: 421/422	Loss: 0.569
Batch: 422/422	Loss: 1.396
Epoch: 4	Train Loss: 0.558	Val F1: 0.707
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 5 ******************************
Batch: 1/422	Loss: 0.653
Batch: 43/422	Loss: 0.401
Batch: 85/422	Loss: 0.441
Batch: 127/422	Loss: 0.668
Batch: 169/422	Loss: 0.501
Batch: 211/422	Loss: 0.468
Batch: 253/422	Loss: 0.422
Batch: 295/422	Loss: 0.412
Batch: 337/422	Loss: 0.592
Batch: 379/422	Loss: 0.391
Batch: 421/422	Loss: 0.559
Batch: 422/422	Loss: 0.443
Epoch: 5	Train Loss: 0.521	Val F1: 0.704
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 6 ******************************
Batch: 1/422	Loss: 0.393
Batch: 43/422	Loss: 0.648
Batch: 85/422	Loss: 0.515
Batch: 127/422	Loss: 0.488
Batch: 169/422	Loss: 0.651
Batch: 211/422	Loss: 0.456
Batch: 253/422	Loss: 0.489
Batch: 295/422	Loss: 0.476
Batch: 337/422	Loss: 0.767
Batch: 379/422	Loss: 0.592
Batch: 421/422	Loss: 0.305
Batch: 422/422	Loss: 0.096
Epoch: 6	Train Loss: 0.488	Val F1: 0.739
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 7 ******************************
Batch: 1/422	Loss: 0.582
Batch: 43/422	Loss: 0.366
Batch: 85/422	Loss: 0.412
Batch: 127/422	Loss: 0.411
Batch: 169/422	Loss: 0.716
Batch: 211/422	Loss: 0.413
Batch: 253/422	Loss: 0.467
Batch: 295/422	Loss: 0.573
Batch: 337/422	Loss: 0.515
Batch: 379/422	Loss: 0.386
Batch: 421/422	Loss: 0.460
Batch: 422/422	Loss: 0.675
Epoch: 7	Train Loss: 0.457	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 8 ******************************
Batch: 1/422	Loss: 0.386
Batch: 43/422	Loss: 0.436
Batch: 85/422	Loss: 0.315
Batch: 127/422	Loss: 0.581
Batch: 169/422	Loss: 0.697
Batch: 211/422	Loss: 0.316
Batch: 253/422	Loss: 0.460
Batch: 295/422	Loss: 0.505
Batch: 337/422	Loss: 0.408
Batch: 379/422	Loss: 0.248
Batch: 421/422	Loss: 0.567
Batch: 422/422	Loss: 0.439
Epoch: 8	Train Loss: 0.425	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 9 ******************************
Batch: 1/422	Loss: 0.635
Batch: 43/422	Loss: 0.296
Batch: 85/422	Loss: 0.446
Batch: 127/422	Loss: 0.423
Batch: 169/422	Loss: 0.459
Batch: 211/422	Loss: 0.301
Batch: 253/422	Loss: 0.424
Batch: 295/422	Loss: 0.453
Batch: 337/422	Loss: 0.398
Batch: 379/422	Loss: 0.230
Batch: 421/422	Loss: 0.445
Batch: 422/422	Loss: 0.360
Epoch: 9	Train Loss: 0.400	Val F1: 0.743
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 10 ******************************
Batch: 1/422	Loss: 0.451
Batch: 43/422	Loss: 0.435
Batch: 85/422	Loss: 0.446
Batch: 127/422	Loss: 0.174
Batch: 169/422	Loss: 0.394
Batch: 211/422	Loss: 0.331
Batch: 253/422	Loss: 0.439
Batch: 295/422	Loss: 0.239
Batch: 337/422	Loss: 0.313
Batch: 379/422	Loss: 0.359
Batch: 421/422	Loss: 0.172
Batch: 422/422	Loss: 0.231
Epoch: 10	Train Loss: 0.373	Val F1: 0.731
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 11 ******************************
Batch: 1/422	Loss: 0.287
Batch: 43/422	Loss: 0.486
Batch: 85/422	Loss: 0.416
Batch: 127/422	Loss: 0.312
Batch: 169/422	Loss: 0.308
Batch: 211/422	Loss: 0.355
Batch: 253/422	Loss: 0.510
Batch: 295/422	Loss: 0.181
Batch: 337/422	Loss: 0.455
Batch: 379/422	Loss: 0.298
Batch: 421/422	Loss: 0.364
Batch: 422/422	Loss: 0.208
Epoch: 11	Train Loss: 0.351	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 12 ******************************
Batch: 1/422	Loss: 0.407
Batch: 43/422	Loss: 0.266
Batch: 85/422	Loss: 0.173
Batch: 127/422	Loss: 0.491
Batch: 169/422	Loss: 0.219
Batch: 211/422	Loss: 0.547
Batch: 253/422	Loss: 0.322
Batch: 295/422	Loss: 0.372
Batch: 337/422	Loss: 0.194
Batch: 379/422	Loss: 0.218
Batch: 421/422	Loss: 0.423
Batch: 422/422	Loss: 0.051
Epoch: 12	Train Loss: 0.334	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 13 ******************************
Batch: 1/422	Loss: 0.281
Batch: 43/422	Loss: 0.383
Batch: 85/422	Loss: 0.285
Batch: 127/422	Loss: 0.204
Batch: 169/422	Loss: 0.316
Batch: 211/422	Loss: 0.263
Batch: 253/422	Loss: 0.360
Batch: 295/422	Loss: 0.416
Batch: 337/422	Loss: 0.390
Batch: 379/422	Loss: 0.331
Batch: 421/422	Loss: 0.177
Batch: 422/422	Loss: 0.160
Epoch: 13	Train Loss: 0.316	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 14 ******************************
Batch: 1/422	Loss: 0.283
Batch: 43/422	Loss: 0.416
Batch: 85/422	Loss: 0.342
Batch: 127/422	Loss: 0.280
Batch: 169/422	Loss: 0.220
Batch: 211/422	Loss: 0.391
Batch: 253/422	Loss: 0.181
Batch: 295/422	Loss: 0.180
Batch: 337/422	Loss: 0.286
Batch: 379/422	Loss: 0.347
Batch: 421/422	Loss: 0.287
Batch: 422/422	Loss: 0.169
Epoch: 14	Train Loss: 0.297	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 15 ******************************
Batch: 1/422	Loss: 0.262
Batch: 43/422	Loss: 0.181
Batch: 85/422	Loss: 0.306
Batch: 127/422	Loss: 0.428
Batch: 169/422	Loss: 0.347
Batch: 211/422	Loss: 0.183
Batch: 253/422	Loss: 0.424
Batch: 295/422	Loss: 0.447
Batch: 337/422	Loss: 0.243
Batch: 379/422	Loss: 0.307
Batch: 421/422	Loss: 0.302
Batch: 422/422	Loss: 0.076
Epoch: 15	Train Loss: 0.277	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 16 ******************************
Batch: 1/422	Loss: 0.234
Batch: 43/422	Loss: 0.284
Batch: 85/422	Loss: 0.227
Batch: 127/422	Loss: 0.236
Batch: 169/422	Loss: 0.488
Batch: 211/422	Loss: 0.181
Batch: 253/422	Loss: 0.196
Batch: 295/422	Loss: 0.191
Batch: 337/422	Loss: 0.131
Batch: 379/422	Loss: 0.376
Batch: 421/422	Loss: 0.275
Batch: 422/422	Loss: 0.463
Epoch: 16	Train Loss: 0.266	Val F1: 0.713
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 17 ******************************
Batch: 1/422	Loss: 0.251
Batch: 43/422	Loss: 0.188
Batch: 85/422	Loss: 0.296
Batch: 127/422	Loss: 0.276
Batch: 169/422	Loss: 0.146
Batch: 211/422	Loss: 0.297
Batch: 253/422	Loss: 0.287
Batch: 295/422	Loss: 0.243
Batch: 337/422	Loss: 0.233
Batch: 379/422	Loss: 0.234
Batch: 421/422	Loss: 0.242
Batch: 422/422	Loss: 0.058
Epoch: 17	Train Loss: 0.244	Val F1: 0.736
Best Epoch: 3	Best Val F1: 0.744

****************************** Epoch: 18 ******************************
Batch: 1/422	Loss: 0.333
Batch: 43/422	Loss: 0.204
Batch: 85/422	Loss: 0.200
Batch: 127/422	Loss: 0.171
Batch: 169/422	Loss: 0.229
Batch: 211/422	Loss: 0.195
Batch: 253/422	Loss: 0.281
Batch: 295/422	Loss: 0.338
Batch: 337/422	Loss: 0.181
Batch: 379/422	Loss: 0.190
Batch: 421/422	Loss: 0.368
Batch: 422/422	Loss: 0.402
Epoch: 18	Train Loss: 0.241	Val F1: 0.733
Best Epoch: 3	Best Val F1: 0.744

Saving the best checkpoint....
Inference...
Test F1: 0.744	Test F1_Few: 0.739	Test F1_Zero: 0.749
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 4.64787936312592e-05, 'lr': 5.723049692144186e-06, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.120
Batch: 169/843	Loss: 1.211
Batch: 253/843	Loss: 1.007
Batch: 337/843	Loss: 1.067
Batch: 421/843	Loss: 0.834
Batch: 505/843	Loss: 0.921
Batch: 589/843	Loss: 0.642
Batch: 673/843	Loss: 0.770
Batch: 757/843	Loss: 0.673
Batch: 841/843	Loss: 0.780
Batch: 843/843	Loss: 0.602
Epoch: 1	Train Loss: 0.914	Val F1: 0.649
Best Epoch: 1	Best Val F1: 0.649

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.642
Batch: 85/843	Loss: 0.866
Batch: 169/843	Loss: 0.823
Batch: 253/843	Loss: 0.574
Batch: 337/843	Loss: 0.721
Batch: 421/843	Loss: 0.823
Batch: 505/843	Loss: 0.644
Batch: 589/843	Loss: 0.777
Batch: 673/843	Loss: 0.646
Batch: 757/843	Loss: 0.653
Batch: 841/843	Loss: 0.625
Batch: 843/843	Loss: 0.420
Epoch: 2	Train Loss: 0.635	Val F1: 0.724
Best Epoch: 2	Best Val F1: 0.724

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.511
Batch: 85/843	Loss: 0.621
Batch: 169/843	Loss: 0.510
Batch: 253/843	Loss: 0.440
Batch: 337/843	Loss: 0.484
Batch: 421/843	Loss: 0.652
Batch: 505/843	Loss: 0.400
Batch: 589/843	Loss: 0.709
Batch: 673/843	Loss: 0.382
Batch: 757/843	Loss: 0.893
Batch: 841/843	Loss: 0.509
Batch: 843/843	Loss: 0.888
Epoch: 3	Train Loss: 0.582	Val F1: 0.746
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.392
Batch: 85/843	Loss: 0.490
Batch: 169/843	Loss: 0.499
Batch: 253/843	Loss: 0.692
Batch: 337/843	Loss: 0.471
Batch: 421/843	Loss: 0.607
Batch: 505/843	Loss: 0.575
Batch: 589/843	Loss: 0.484
Batch: 673/843	Loss: 0.717
Batch: 757/843	Loss: 0.637
Batch: 841/843	Loss: 0.511
Batch: 843/843	Loss: 1.343
Epoch: 4	Train Loss: 0.531	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.654
Batch: 85/843	Loss: 0.403
Batch: 169/843	Loss: 0.591
Batch: 253/843	Loss: 0.366
Batch: 337/843	Loss: 0.603
Batch: 421/843	Loss: 0.422
Batch: 505/843	Loss: 0.523
Batch: 589/843	Loss: 0.619
Batch: 673/843	Loss: 0.395
Batch: 757/843	Loss: 0.558
Batch: 841/843	Loss: 0.392
Batch: 843/843	Loss: 0.537
Epoch: 5	Train Loss: 0.492	Val F1: 0.711
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.395
Batch: 85/843	Loss: 0.340
Batch: 169/843	Loss: 0.479
Batch: 253/843	Loss: 0.604
Batch: 337/843	Loss: 0.518
Batch: 421/843	Loss: 0.338
Batch: 505/843	Loss: 0.463
Batch: 589/843	Loss: 0.506
Batch: 673/843	Loss: 0.568
Batch: 757/843	Loss: 0.623
Batch: 841/843	Loss: 0.324
Batch: 843/843	Loss: 0.066
Epoch: 6	Train Loss: 0.457	Val F1: 0.731
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.292
Batch: 85/843	Loss: 0.173
Batch: 169/843	Loss: 0.384
Batch: 253/843	Loss: 0.354
Batch: 337/843	Loss: 0.792
Batch: 421/843	Loss: 0.199
Batch: 505/843	Loss: 0.217
Batch: 589/843	Loss: 0.268
Batch: 673/843	Loss: 0.588
Batch: 757/843	Loss: 0.444
Batch: 841/843	Loss: 0.539
Batch: 843/843	Loss: 0.305
Epoch: 7	Train Loss: 0.420	Val F1: 0.720
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.281
Batch: 85/843	Loss: 0.478
Batch: 169/843	Loss: 0.172
Batch: 253/843	Loss: 0.682
Batch: 337/843	Loss: 0.505
Batch: 421/843	Loss: 0.392
Batch: 505/843	Loss: 0.624
Batch: 589/843	Loss: 0.490
Batch: 673/843	Loss: 0.318
Batch: 757/843	Loss: 0.310
Batch: 841/843	Loss: 0.276
Batch: 843/843	Loss: 0.344
Epoch: 8	Train Loss: 0.386	Val F1: 0.707
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 9 ******************************
Batch: 1/843	Loss: 0.772
Batch: 85/843	Loss: 0.199
Batch: 169/843	Loss: 0.475
Batch: 253/843	Loss: 0.421
Batch: 337/843	Loss: 0.114
Batch: 421/843	Loss: 0.437
Batch: 505/843	Loss: 0.436
Batch: 589/843	Loss: 0.276
Batch: 673/843	Loss: 0.519
Batch: 757/843	Loss: 0.256
Batch: 841/843	Loss: 0.311
Batch: 843/843	Loss: 0.443
Epoch: 9	Train Loss: 0.361	Val F1: 0.738
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 10 ******************************
Batch: 1/843	Loss: 0.560
Batch: 85/843	Loss: 0.420
Batch: 169/843	Loss: 0.255
Batch: 253/843	Loss: 0.173
Batch: 337/843	Loss: 0.208
Batch: 421/843	Loss: 0.240
Batch: 505/843	Loss: 0.295
Batch: 589/843	Loss: 0.191
Batch: 673/843	Loss: 0.244
Batch: 757/843	Loss: 0.260
Batch: 841/843	Loss: 0.121
Batch: 843/843	Loss: 0.171
Epoch: 10	Train Loss: 0.339	Val F1: 0.744
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 11 ******************************
Batch: 1/843	Loss: 0.453
Batch: 85/843	Loss: 0.507
Batch: 169/843	Loss: 0.386
Batch: 253/843	Loss: 0.176
Batch: 337/843	Loss: 0.211
Batch: 421/843	Loss: 0.260
Batch: 505/843	Loss: 0.593
Batch: 589/843	Loss: 0.101
Batch: 673/843	Loss: 0.568
Batch: 757/843	Loss: 0.204
Batch: 841/843	Loss: 0.216
Batch: 843/843	Loss: 0.140
Epoch: 11	Train Loss: 0.313	Val F1: 0.720
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 12 ******************************
Batch: 1/843	Loss: 0.386
Batch: 85/843	Loss: 0.107
Batch: 169/843	Loss: 0.183
Batch: 253/843	Loss: 0.498
Batch: 337/843	Loss: 0.224
Batch: 421/843	Loss: 0.845
Batch: 505/843	Loss: 0.334
Batch: 589/843	Loss: 0.168
Batch: 673/843	Loss: 0.166
Batch: 757/843	Loss: 0.383
Batch: 841/843	Loss: 0.168
Batch: 843/843	Loss: 0.032
Epoch: 12	Train Loss: 0.294	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 13 ******************************
Batch: 1/843	Loss: 0.279
Batch: 85/843	Loss: 0.325
Batch: 169/843	Loss: 0.276
Batch: 253/843	Loss: 0.190
Batch: 337/843	Loss: 0.202
Batch: 421/843	Loss: 0.181
Batch: 505/843	Loss: 0.369
Batch: 589/843	Loss: 0.421
Batch: 673/843	Loss: 0.123
Batch: 757/843	Loss: 0.300
Batch: 841/843	Loss: 0.308
Batch: 843/843	Loss: 0.177
Epoch: 13	Train Loss: 0.273	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 14 ******************************
Batch: 1/843	Loss: 0.134
Batch: 85/843	Loss: 0.293
Batch: 169/843	Loss: 0.088
Batch: 253/843	Loss: 0.214
Batch: 337/843	Loss: 0.232
Batch: 421/843	Loss: 0.224
Batch: 505/843	Loss: 0.112
Batch: 589/843	Loss: 0.081
Batch: 673/843	Loss: 0.237
Batch: 757/843	Loss: 0.069
Batch: 841/843	Loss: 0.409
Batch: 843/843	Loss: 0.084
Epoch: 14	Train Loss: 0.254	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 15 ******************************
Batch: 1/843	Loss: 0.096
Batch: 85/843	Loss: 0.144
Batch: 169/843	Loss: 0.146
Batch: 253/843	Loss: 0.147
Batch: 337/843	Loss: 0.249
Batch: 421/843	Loss: 0.063
Batch: 505/843	Loss: 0.125
Batch: 589/843	Loss: 0.163
Batch: 673/843	Loss: 0.510
Batch: 757/843	Loss: 0.301
Batch: 841/843	Loss: 0.189
Batch: 843/843	Loss: 0.059
Epoch: 15	Train Loss: 0.241	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 16 ******************************
Batch: 1/843	Loss: 0.334
Batch: 85/843	Loss: 0.198
Batch: 169/843	Loss: 0.227
Batch: 253/843	Loss: 0.355
Batch: 337/843	Loss: 0.519
Batch: 421/843	Loss: 0.240
Batch: 505/843	Loss: 0.079
Batch: 589/843	Loss: 0.113
Batch: 673/843	Loss: 0.309
Batch: 757/843	Loss: 0.087
Batch: 841/843	Loss: 0.255
Batch: 843/843	Loss: 0.105
Epoch: 16	Train Loss: 0.225	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 17 ******************************
Batch: 1/843	Loss: 0.125
Batch: 85/843	Loss: 0.126
Batch: 169/843	Loss: 0.463
Batch: 253/843	Loss: 0.202
Batch: 337/843	Loss: 0.107
Batch: 421/843	Loss: 0.659
Batch: 505/843	Loss: 0.383
Batch: 589/843	Loss: 0.176
Batch: 673/843	Loss: 0.198
Batch: 757/843	Loss: 0.222
Batch: 841/843	Loss: 0.157
Batch: 843/843	Loss: 0.030
Epoch: 17	Train Loss: 0.213	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.746

****************************** Epoch: 18 ******************************
Batch: 1/843	Loss: 0.112
Batch: 85/843	Loss: 0.094
Batch: 169/843	Loss: 0.166
Batch: 253/843	Loss: 0.075
Batch: 337/843	Loss: 0.117
Batch: 421/843	Loss: 0.266
Batch: 505/843	Loss: 0.047
Batch: 589/843	Loss: 0.316
Batch: 673/843	Loss: 0.087
Batch: 757/843	Loss: 0.164
Batch: 841/843	Loss: 0.239
Batch: 843/843	Loss: 0.143
Epoch: 18	Train Loss: 0.201	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.746

Saving the best checkpoint....
Inference...
Test F1: 0.749	Test F1_Few: 0.746	Test F1_Zero: 0.753
Starting training with config: {'batch_size': 64, 'epochs': 50, 'l2_reg': 7.100413728874671e-05, 'lr': 9.797141633819474e-06, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/211	Loss: 1.079
Batch: 22/211	Loss: 1.031
Batch: 43/211	Loss: 1.129
Batch: 64/211	Loss: 1.037
Batch: 85/211	Loss: 1.066
Batch: 106/211	Loss: 1.032
Batch: 127/211	Loss: 0.996
Batch: 148/211	Loss: 1.026
Batch: 169/211	Loss: 0.975
Batch: 190/211	Loss: 0.841
Batch: 211/211	Loss: 0.797
Epoch: 1	Train Loss: 0.998	Val F1: 0.693
Best Epoch: 1	Best Val F1: 0.693

****************************** Epoch: 2 ******************************
Batch: 1/211	Loss: 0.793
Batch: 22/211	Loss: 0.781
Batch: 43/211	Loss: 0.758
Batch: 64/211	Loss: 0.729
Batch: 85/211	Loss: 0.636
Batch: 106/211	Loss: 0.787
Batch: 127/211	Loss: 0.744
Batch: 148/211	Loss: 0.699
Batch: 169/211	Loss: 0.626
Batch: 190/211	Loss: 0.584
Batch: 211/211	Loss: 0.622
Epoch: 2	Train Loss: 0.675	Val F1: 0.713
Best Epoch: 2	Best Val F1: 0.713

****************************** Epoch: 3 ******************************
Batch: 1/211	Loss: 0.630
Batch: 22/211	Loss: 0.662
Batch: 43/211	Loss: 0.628
Batch: 64/211	Loss: 0.530
Batch: 85/211	Loss: 0.565
Batch: 106/211	Loss: 0.552
Batch: 127/211	Loss: 0.477
Batch: 148/211	Loss: 0.662
Batch: 169/211	Loss: 0.672
Batch: 190/211	Loss: 0.695
Batch: 211/211	Loss: 0.629
Epoch: 3	Train Loss: 0.611	Val F1: 0.738
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 4 ******************************
Batch: 1/211	Loss: 0.464
Batch: 22/211	Loss: 0.608
Batch: 43/211	Loss: 0.570
Batch: 64/211	Loss: 0.575
Batch: 85/211	Loss: 0.561
Batch: 106/211	Loss: 0.496
Batch: 127/211	Loss: 0.466
Batch: 148/211	Loss: 0.551
Batch: 169/211	Loss: 0.577
Batch: 190/211	Loss: 0.647
Batch: 211/211	Loss: 0.567
Epoch: 4	Train Loss: 0.571	Val F1: 0.702
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 5 ******************************
Batch: 1/211	Loss: 0.628
Batch: 22/211	Loss: 0.499
Batch: 43/211	Loss: 0.419
Batch: 64/211	Loss: 0.491
Batch: 85/211	Loss: 0.516
Batch: 106/211	Loss: 0.486
Batch: 127/211	Loss: 0.401
Batch: 148/211	Loss: 0.403
Batch: 169/211	Loss: 0.409
Batch: 190/211	Loss: 0.594
Batch: 211/211	Loss: 0.591
Epoch: 5	Train Loss: 0.532	Val F1: 0.736
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 6 ******************************
Batch: 1/211	Loss: 0.452
Batch: 22/211	Loss: 0.598
Batch: 43/211	Loss: 0.530
Batch: 64/211	Loss: 0.452
Batch: 85/211	Loss: 0.486
Batch: 106/211	Loss: 0.625
Batch: 127/211	Loss: 0.474
Batch: 148/211	Loss: 0.452
Batch: 169/211	Loss: 0.641
Batch: 190/211	Loss: 0.483
Batch: 211/211	Loss: 0.265
Epoch: 6	Train Loss: 0.502	Val F1: 0.733
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 7 ******************************
Batch: 1/211	Loss: 0.515
Batch: 22/211	Loss: 0.519
Batch: 43/211	Loss: 0.575
Batch: 64/211	Loss: 0.467
Batch: 85/211	Loss: 0.537
Batch: 106/211	Loss: 0.424
Batch: 127/211	Loss: 0.668
Batch: 148/211	Loss: 0.544
Batch: 169/211	Loss: 0.531
Batch: 190/211	Loss: 0.539
Batch: 211/211	Loss: 0.484
Epoch: 7	Train Loss: 0.469	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 8 ******************************
Batch: 1/211	Loss: 0.438
Batch: 22/211	Loss: 0.428
Batch: 43/211	Loss: 0.383
Batch: 64/211	Loss: 0.453
Batch: 85/211	Loss: 0.702
Batch: 106/211	Loss: 0.384
Batch: 127/211	Loss: 0.372
Batch: 148/211	Loss: 0.599
Batch: 169/211	Loss: 0.507
Batch: 190/211	Loss: 0.374
Batch: 211/211	Loss: 0.501
Epoch: 8	Train Loss: 0.433	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 9 ******************************
Batch: 1/211	Loss: 0.463
Batch: 22/211	Loss: 0.325
Batch: 43/211	Loss: 0.414
Batch: 64/211	Loss: 0.506
Batch: 85/211	Loss: 0.490
Batch: 106/211	Loss: 0.395
Batch: 127/211	Loss: 0.547
Batch: 148/211	Loss: 0.420
Batch: 169/211	Loss: 0.389
Batch: 190/211	Loss: 0.435
Batch: 211/211	Loss: 0.431
Epoch: 9	Train Loss: 0.404	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 10 ******************************
Batch: 1/211	Loss: 0.419
Batch: 22/211	Loss: 0.372
Batch: 43/211	Loss: 0.510
Batch: 64/211	Loss: 0.296
Batch: 85/211	Loss: 0.281
Batch: 106/211	Loss: 0.390
Batch: 127/211	Loss: 0.369
Batch: 148/211	Loss: 0.363
Batch: 169/211	Loss: 0.386
Batch: 190/211	Loss: 0.336
Batch: 211/211	Loss: 0.219
Epoch: 10	Train Loss: 0.374	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 11 ******************************
Batch: 1/211	Loss: 0.327
Batch: 22/211	Loss: 0.280
Batch: 43/211	Loss: 0.363
Batch: 64/211	Loss: 0.442
Batch: 85/211	Loss: 0.299
Batch: 106/211	Loss: 0.318
Batch: 127/211	Loss: 0.505
Batch: 148/211	Loss: 0.281
Batch: 169/211	Loss: 0.335
Batch: 190/211	Loss: 0.356
Batch: 211/211	Loss: 0.340
Epoch: 11	Train Loss: 0.356	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 12 ******************************
Batch: 1/211	Loss: 0.367
Batch: 22/211	Loss: 0.321
Batch: 43/211	Loss: 0.366
Batch: 64/211	Loss: 0.397
Batch: 85/211	Loss: 0.234
Batch: 106/211	Loss: 0.345
Batch: 127/211	Loss: 0.273
Batch: 148/211	Loss: 0.571
Batch: 169/211	Loss: 0.456
Batch: 190/211	Loss: 0.280
Batch: 211/211	Loss: 0.291
Epoch: 12	Train Loss: 0.331	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 13 ******************************
Batch: 1/211	Loss: 0.368
Batch: 22/211	Loss: 0.346
Batch: 43/211	Loss: 0.309
Batch: 64/211	Loss: 0.177
Batch: 85/211	Loss: 0.320
Batch: 106/211	Loss: 0.376
Batch: 127/211	Loss: 0.395
Batch: 148/211	Loss: 0.366
Batch: 169/211	Loss: 0.368
Batch: 190/211	Loss: 0.284
Batch: 211/211	Loss: 0.252
Epoch: 13	Train Loss: 0.309	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 14 ******************************
Batch: 1/211	Loss: 0.256
Batch: 22/211	Loss: 0.446
Batch: 43/211	Loss: 0.266
Batch: 64/211	Loss: 0.253
Batch: 85/211	Loss: 0.237
Batch: 106/211	Loss: 0.203
Batch: 127/211	Loss: 0.192
Batch: 148/211	Loss: 0.276
Batch: 169/211	Loss: 0.283
Batch: 190/211	Loss: 0.238
Batch: 211/211	Loss: 0.242
Epoch: 14	Train Loss: 0.295	Val F1: 0.715
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 15 ******************************
Batch: 1/211	Loss: 0.418
Batch: 22/211	Loss: 0.162
Batch: 43/211	Loss: 0.265
Batch: 64/211	Loss: 0.408
Batch: 85/211	Loss: 0.183
Batch: 106/211	Loss: 0.202
Batch: 127/211	Loss: 0.383
Batch: 148/211	Loss: 0.345
Batch: 169/211	Loss: 0.258
Batch: 190/211	Loss: 0.251
Batch: 211/211	Loss: 0.262
Epoch: 15	Train Loss: 0.276	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 16 ******************************
Batch: 1/211	Loss: 0.196
Batch: 22/211	Loss: 0.278
Batch: 43/211	Loss: 0.365
Batch: 64/211	Loss: 0.113
Batch: 85/211	Loss: 0.359
Batch: 106/211	Loss: 0.251
Batch: 127/211	Loss: 0.190
Batch: 148/211	Loss: 0.207
Batch: 169/211	Loss: 0.109
Batch: 190/211	Loss: 0.237
Batch: 211/211	Loss: 0.366
Epoch: 16	Train Loss: 0.258	Val F1: 0.707
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 17 ******************************
Batch: 1/211	Loss: 0.241
Batch: 22/211	Loss: 0.274
Batch: 43/211	Loss: 0.246
Batch: 64/211	Loss: 0.231
Batch: 85/211	Loss: 0.275
Batch: 106/211	Loss: 0.229
Batch: 127/211	Loss: 0.329
Batch: 148/211	Loss: 0.146
Batch: 169/211	Loss: 0.214
Batch: 190/211	Loss: 0.316
Batch: 211/211	Loss: 0.242
Epoch: 17	Train Loss: 0.248	Val F1: 0.716
Best Epoch: 3	Best Val F1: 0.738

****************************** Epoch: 18 ******************************
Batch: 1/211	Loss: 0.303
Batch: 22/211	Loss: 0.150
Batch: 43/211	Loss: 0.295
Batch: 64/211	Loss: 0.219
Batch: 85/211	Loss: 0.249
Batch: 106/211	Loss: 0.255
Batch: 127/211	Loss: 0.259
Batch: 148/211	Loss: 0.278
Batch: 169/211	Loss: 0.214
Batch: 190/211	Loss: 0.142
Batch: 211/211	Loss: 0.302
Epoch: 18	Train Loss: 0.231	Val F1: 0.717
Best Epoch: 3	Best Val F1: 0.738

Saving the best checkpoint....
Inference...
Test F1: 0.741	Test F1_Few: 0.735	Test F1_Zero: 0.747
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 6.944414012776496e-05, 'lr': 5.182462246759728e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/843	Loss: 1.099
Batch: 85/843	Loss: 1.116
Batch: 169/843	Loss: 1.212
Batch: 253/843	Loss: 1.006
Batch: 337/843	Loss: 1.078
Batch: 421/843	Loss: 0.847
Batch: 505/843	Loss: 1.026
Batch: 589/843	Loss: 0.962
Batch: 673/843	Loss: 0.961
Batch: 757/843	Loss: 0.729
Batch: 841/843	Loss: 0.701
Batch: 843/843	Loss: 0.618
Epoch: 1	Train Loss: 0.965	Val F1: 0.601
Best Epoch: 1	Best Val F1: 0.601

****************************** Epoch: 2 ******************************
Batch: 1/843	Loss: 0.679
Batch: 85/843	Loss: 0.852
Batch: 169/843	Loss: 0.781
Batch: 253/843	Loss: 0.606
Batch: 337/843	Loss: 0.601
Batch: 421/843	Loss: 0.849
Batch: 505/843	Loss: 0.649
Batch: 589/843	Loss: 0.830
Batch: 673/843	Loss: 0.629
Batch: 757/843	Loss: 0.600
Batch: 841/843	Loss: 0.557
Batch: 843/843	Loss: 0.530
Epoch: 2	Train Loss: 0.651	Val F1: 0.730
Best Epoch: 2	Best Val F1: 0.730

****************************** Epoch: 3 ******************************
Batch: 1/843	Loss: 0.534
Batch: 85/843	Loss: 0.608
Batch: 169/843	Loss: 0.512
Batch: 253/843	Loss: 0.472
Batch: 337/843	Loss: 0.423
Batch: 421/843	Loss: 0.661
Batch: 505/843	Loss: 0.404
Batch: 589/843	Loss: 0.723
Batch: 673/843	Loss: 0.372
Batch: 757/843	Loss: 0.887
Batch: 841/843	Loss: 0.551
Batch: 843/843	Loss: 0.930
Epoch: 3	Train Loss: 0.588	Val F1: 0.748
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 4 ******************************
Batch: 1/843	Loss: 0.414
Batch: 85/843	Loss: 0.536
Batch: 169/843	Loss: 0.576
Batch: 253/843	Loss: 0.728
Batch: 337/843	Loss: 0.551
Batch: 421/843	Loss: 0.693
Batch: 505/843	Loss: 0.763
Batch: 589/843	Loss: 0.570
Batch: 673/843	Loss: 0.718
Batch: 757/843	Loss: 0.688
Batch: 841/843	Loss: 0.523
Batch: 843/843	Loss: 1.238
Epoch: 4	Train Loss: 0.546	Val F1: 0.732
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 5 ******************************
Batch: 1/843	Loss: 0.665
Batch: 85/843	Loss: 0.386
Batch: 169/843	Loss: 0.490
Batch: 253/843	Loss: 0.383
Batch: 337/843	Loss: 0.610
Batch: 421/843	Loss: 0.478
Batch: 505/843	Loss: 0.527
Batch: 589/843	Loss: 0.633
Batch: 673/843	Loss: 0.439
Batch: 757/843	Loss: 0.554
Batch: 841/843	Loss: 0.390
Batch: 843/843	Loss: 0.521
Epoch: 5	Train Loss: 0.504	Val F1: 0.721
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 6 ******************************
Batch: 1/843	Loss: 0.353
Batch: 85/843	Loss: 0.513
Batch: 169/843	Loss: 0.542
Batch: 253/843	Loss: 0.639
Batch: 337/843	Loss: 0.586
Batch: 421/843	Loss: 0.352
Batch: 505/843	Loss: 0.431
Batch: 589/843	Loss: 0.483
Batch: 673/843	Loss: 0.535
Batch: 757/843	Loss: 0.598
Batch: 841/843	Loss: 0.343
Batch: 843/843	Loss: 0.060
Epoch: 6	Train Loss: 0.470	Val F1: 0.741
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 7 ******************************
Batch: 1/843	Loss: 0.304
Batch: 85/843	Loss: 0.146
Batch: 169/843	Loss: 0.410
Batch: 253/843	Loss: 0.366
Batch: 337/843	Loss: 0.759
Batch: 421/843	Loss: 0.222
Batch: 505/843	Loss: 0.250
Batch: 589/843	Loss: 0.253
Batch: 673/843	Loss: 0.648
Batch: 757/843	Loss: 0.520
Batch: 841/843	Loss: 0.546
Batch: 843/843	Loss: 0.254
Epoch: 7	Train Loss: 0.431	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 8 ******************************
Batch: 1/843	Loss: 0.255
Batch: 85/843	Loss: 0.520
Batch: 169/843	Loss: 0.163
Batch: 253/843	Loss: 0.544
Batch: 337/843	Loss: 0.519
Batch: 421/843	Loss: 0.441
Batch: 505/843	Loss: 0.606
Batch: 589/843	Loss: 0.499
Batch: 673/843	Loss: 0.287
Batch: 757/843	Loss: 0.291
Batch: 841/843	Loss: 0.479
Batch: 843/843	Loss: 0.201
Epoch: 8	Train Loss: 0.404	Val F1: 0.697
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 9 ******************************
Batch: 1/843	Loss: 0.707
Batch: 85/843	Loss: 0.242
Batch: 169/843	Loss: 0.575
Batch: 253/843	Loss: 0.430
Batch: 337/843	Loss: 0.178
Batch: 421/843	Loss: 0.444
Batch: 505/843	Loss: 0.453
Batch: 589/843	Loss: 0.348
Batch: 673/843	Loss: 0.743
Batch: 757/843	Loss: 0.319
Batch: 841/843	Loss: 0.437
Batch: 843/843	Loss: 0.379
Epoch: 9	Train Loss: 0.379	Val F1: 0.741
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 10 ******************************
Batch: 1/843	Loss: 0.550
Batch: 85/843	Loss: 0.330
Batch: 169/843	Loss: 0.222
Batch: 253/843	Loss: 0.175
Batch: 337/843	Loss: 0.246
Batch: 421/843	Loss: 0.213
Batch: 505/843	Loss: 0.349
Batch: 589/843	Loss: 0.311
Batch: 673/843	Loss: 0.213
Batch: 757/843	Loss: 0.287
Batch: 841/843	Loss: 0.110
Batch: 843/843	Loss: 0.179
Epoch: 10	Train Loss: 0.358	Val F1: 0.737
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 11 ******************************
Batch: 1/843	Loss: 0.478
Batch: 85/843	Loss: 0.485
Batch: 169/843	Loss: 0.299
Batch: 253/843	Loss: 0.234
Batch: 337/843	Loss: 0.168
Batch: 421/843	Loss: 0.307
Batch: 505/843	Loss: 0.607
Batch: 589/843	Loss: 0.122
Batch: 673/843	Loss: 0.581
Batch: 757/843	Loss: 0.327
Batch: 841/843	Loss: 0.175
Batch: 843/843	Loss: 0.177
Epoch: 11	Train Loss: 0.331	Val F1: 0.731
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 12 ******************************
Batch: 1/843	Loss: 0.485
Batch: 85/843	Loss: 0.111
Batch: 169/843	Loss: 0.167
Batch: 253/843	Loss: 0.518
Batch: 337/843	Loss: 0.255
Batch: 421/843	Loss: 0.820
Batch: 505/843	Loss: 0.332
Batch: 589/843	Loss: 0.193
Batch: 673/843	Loss: 0.137
Batch: 757/843	Loss: 0.284
Batch: 841/843	Loss: 0.195
Batch: 843/843	Loss: 0.047
Epoch: 12	Train Loss: 0.313	Val F1: 0.724
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 13 ******************************
Batch: 1/843	Loss: 0.176
Batch: 85/843	Loss: 0.430
Batch: 169/843	Loss: 0.329
Batch: 253/843	Loss: 0.174
Batch: 337/843	Loss: 0.345
Batch: 421/843	Loss: 0.292
Batch: 505/843	Loss: 0.457
Batch: 589/843	Loss: 0.415
Batch: 673/843	Loss: 0.270
Batch: 757/843	Loss: 0.147
Batch: 841/843	Loss: 0.350
Batch: 843/843	Loss: 0.285
Epoch: 13	Train Loss: 0.296	Val F1: 0.723
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 14 ******************************
Batch: 1/843	Loss: 0.181
Batch: 85/843	Loss: 0.150
Batch: 169/843	Loss: 0.123
Batch: 253/843	Loss: 0.213
Batch: 337/843	Loss: 0.377
Batch: 421/843	Loss: 0.208
Batch: 505/843	Loss: 0.096
Batch: 589/843	Loss: 0.096
Batch: 673/843	Loss: 0.208
Batch: 757/843	Loss: 0.117
Batch: 841/843	Loss: 0.425
Batch: 843/843	Loss: 0.191
Epoch: 14	Train Loss: 0.277	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 15 ******************************
Batch: 1/843	Loss: 0.341
Batch: 85/843	Loss: 0.075
Batch: 169/843	Loss: 0.201
Batch: 253/843	Loss: 0.311
Batch: 337/843	Loss: 0.289
Batch: 421/843	Loss: 0.054
Batch: 505/843	Loss: 0.366
Batch: 589/843	Loss: 0.296
Batch: 673/843	Loss: 0.429
Batch: 757/843	Loss: 0.293
Batch: 841/843	Loss: 0.297
Batch: 843/843	Loss: 0.036
Epoch: 15	Train Loss: 0.260	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 16 ******************************
Batch: 1/843	Loss: 0.198
Batch: 85/843	Loss: 0.242
Batch: 169/843	Loss: 0.177
Batch: 253/843	Loss: 0.275
Batch: 337/843	Loss: 0.448
Batch: 421/843	Loss: 0.211
Batch: 505/843	Loss: 0.130
Batch: 589/843	Loss: 0.148
Batch: 673/843	Loss: 0.268
Batch: 757/843	Loss: 0.342
Batch: 841/843	Loss: 0.217
Batch: 843/843	Loss: 0.159
Epoch: 16	Train Loss: 0.248	Val F1: 0.734
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 17 ******************************
Batch: 1/843	Loss: 0.381
Batch: 85/843	Loss: 0.104
Batch: 169/843	Loss: 0.283
Batch: 253/843	Loss: 0.259
Batch: 337/843	Loss: 0.087
Batch: 421/843	Loss: 0.172
Batch: 505/843	Loss: 0.579
Batch: 589/843	Loss: 0.115
Batch: 673/843	Loss: 0.165
Batch: 757/843	Loss: 0.237
Batch: 841/843	Loss: 0.264
Batch: 843/843	Loss: 0.132
Epoch: 17	Train Loss: 0.228	Val F1: 0.725
Best Epoch: 3	Best Val F1: 0.748

****************************** Epoch: 18 ******************************
Batch: 1/843	Loss: 0.087
Batch: 85/843	Loss: 0.088
Batch: 169/843	Loss: 0.157
Batch: 253/843	Loss: 0.135
Batch: 337/843	Loss: 0.170
Batch: 421/843	Loss: 0.373
Batch: 505/843	Loss: 0.216
Batch: 589/843	Loss: 0.466
Batch: 673/843	Loss: 0.168
Batch: 757/843	Loss: 0.159
Batch: 841/843	Loss: 0.215
Batch: 843/843	Loss: 0.241
Epoch: 18	Train Loss: 0.218	Val F1: 0.730
Best Epoch: 3	Best Val F1: 0.748

Saving the best checkpoint....
Inference...
Test F1: 0.750	Test F1_Few: 0.745	Test F1_Zero: 0.755
Starting training with config: {'batch_size': 4, 'epochs': 25, 'l2_reg': 9.308580747959692e-05, 'lr': 4.985979820315001e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/3370	Loss: 1.064
Batch: 338/3370	Loss: 1.246
Batch: 675/3370	Loss: 1.076
Batch: 1012/3370	Loss: 1.084
Batch: 1349/3370	Loss: 0.925
Batch: 1686/3370	Loss: 1.325
Batch: 2023/3370	Loss: 1.065
Batch: 2360/3370	Loss: 1.271
Batch: 2697/3370	Loss: 0.953
Batch: 3034/3370	Loss: 1.092
Batch: 3370/3370	Loss: 0.899
Epoch: 1	Train Loss: 1.051	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 2 ******************************
Batch: 1/3370	Loss: 1.092
Batch: 338/3370	Loss: 0.899
Batch: 675/3370	Loss: 0.904
Batch: 1012/3370	Loss: 0.842
Batch: 1349/3370	Loss: 1.105
Batch: 1686/3370	Loss: 0.910
Batch: 2023/3370	Loss: 0.894
Batch: 2360/3370	Loss: 1.082
Batch: 2697/3370	Loss: 0.908
Batch: 3034/3370	Loss: 0.905
Batch: 3370/3370	Loss: 1.526
Epoch: 2	Train Loss: 1.050	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 3 ******************************
Batch: 1/3370	Loss: 1.077
Batch: 338/3370	Loss: 1.102
Batch: 675/3370	Loss: 0.903
Batch: 1012/3370	Loss: 0.911
Batch: 1349/3370	Loss: 0.951
Batch: 1686/3370	Loss: 1.114
Batch: 2023/3370	Loss: 1.100
Batch: 2360/3370	Loss: 1.313
Batch: 2697/3370	Loss: 0.883
Batch: 3034/3370	Loss: 1.095
Batch: 3370/3370	Loss: 0.862
Epoch: 3	Train Loss: 1.049	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 4 ******************************
Batch: 1/3370	Loss: 0.873
Batch: 338/3370	Loss: 0.883
Batch: 675/3370	Loss: 1.072
Batch: 1012/3370	Loss: 0.889
Batch: 1349/3370	Loss: 0.910
Batch: 1686/3370	Loss: 1.069
Batch: 2023/3370	Loss: 1.109
Batch: 2360/3370	Loss: 1.125
Batch: 2697/3370	Loss: 0.915
Batch: 3034/3370	Loss: 1.115
Batch: 3370/3370	Loss: 1.675
Epoch: 4	Train Loss: 1.049	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 5 ******************************
Batch: 1/3370	Loss: 1.309
Batch: 338/3370	Loss: 1.123
Batch: 675/3370	Loss: 0.881
Batch: 1012/3370	Loss: 1.103
Batch: 1349/3370	Loss: 0.922
Batch: 1686/3370	Loss: 0.948
Batch: 2023/3370	Loss: 1.256
Batch: 2360/3370	Loss: 1.260
Batch: 2697/3370	Loss: 1.064
Batch: 3034/3370	Loss: 0.941
Batch: 3370/3370	Loss: 0.942
Epoch: 5	Train Loss: 1.049	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 6 ******************************
Batch: 1/3370	Loss: 1.489
Batch: 338/3370	Loss: 1.108
Batch: 675/3370	Loss: 0.894
Batch: 1012/3370	Loss: 1.094
Batch: 1349/3370	Loss: 1.070
Batch: 1686/3370	Loss: 1.112
Batch: 2023/3370	Loss: 1.039
Batch: 2360/3370	Loss: 0.908
Batch: 2697/3370	Loss: 1.088
Batch: 3034/3370	Loss: 0.925
Batch: 3370/3370	Loss: 1.670
Epoch: 6	Train Loss: 1.049	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 7 ******************************
Batch: 1/3370	Loss: 1.126
Batch: 338/3370	Loss: 1.071
Batch: 675/3370	Loss: 0.922
Batch: 1012/3370	Loss: 1.088
Batch: 1349/3370	Loss: 0.914
Batch: 1686/3370	Loss: 1.108
Batch: 2023/3370	Loss: 1.230
Batch: 2360/3370	Loss: 1.073
Batch: 2697/3370	Loss: 0.940
Batch: 3034/3370	Loss: 1.089
Batch: 3370/3370	Loss: 1.696
Epoch: 7	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 8 ******************************
Batch: 1/3370	Loss: 1.087
Batch: 338/3370	Loss: 1.286
Batch: 675/3370	Loss: 1.515
Batch: 1012/3370	Loss: 0.896
Batch: 1349/3370	Loss: 0.882
Batch: 1686/3370	Loss: 1.088
Batch: 2023/3370	Loss: 1.089
Batch: 2360/3370	Loss: 0.889
Batch: 2697/3370	Loss: 0.970
Batch: 3034/3370	Loss: 1.081
Batch: 3370/3370	Loss: 0.900
Epoch: 8	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 9 ******************************
Batch: 1/3370	Loss: 0.886
Batch: 338/3370	Loss: 0.900
Batch: 675/3370	Loss: 1.125
Batch: 1012/3370	Loss: 0.936
Batch: 1349/3370	Loss: 1.115
Batch: 1686/3370	Loss: 0.925
Batch: 2023/3370	Loss: 0.913
Batch: 2360/3370	Loss: 1.081
Batch: 2697/3370	Loss: 1.098
Batch: 3034/3370	Loss: 0.862
Batch: 3370/3370	Loss: 0.882
Epoch: 9	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 10 ******************************
Batch: 1/3370	Loss: 0.873
Batch: 338/3370	Loss: 0.907
Batch: 675/3370	Loss: 1.098
Batch: 1012/3370	Loss: 1.276
Batch: 1349/3370	Loss: 0.948
Batch: 1686/3370	Loss: 0.889
Batch: 2023/3370	Loss: 0.871
Batch: 2360/3370	Loss: 1.064
Batch: 2697/3370	Loss: 0.913
Batch: 3034/3370	Loss: 1.078
Batch: 3370/3370	Loss: 0.833
Epoch: 10	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 11 ******************************
Batch: 1/3370	Loss: 1.337
Batch: 338/3370	Loss: 1.120
Batch: 675/3370	Loss: 1.071
Batch: 1012/3370	Loss: 1.286
Batch: 1349/3370	Loss: 1.080
Batch: 1686/3370	Loss: 0.883
Batch: 2023/3370	Loss: 1.365
Batch: 2360/3370	Loss: 1.079
Batch: 2697/3370	Loss: 1.140
Batch: 3034/3370	Loss: 1.079
Batch: 3370/3370	Loss: 1.625
Epoch: 11	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 12 ******************************
Batch: 1/3370	Loss: 0.943
Batch: 338/3370	Loss: 1.272
Batch: 675/3370	Loss: 1.117
Batch: 1012/3370	Loss: 1.303
Batch: 1349/3370	Loss: 1.222
Batch: 1686/3370	Loss: 0.903
Batch: 2023/3370	Loss: 0.954
Batch: 2360/3370	Loss: 1.109
Batch: 2697/3370	Loss: 0.923
Batch: 3034/3370	Loss: 0.878
Batch: 3370/3370	Loss: 0.884
Epoch: 12	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 13 ******************************
Batch: 1/3370	Loss: 0.953
Batch: 338/3370	Loss: 0.924
Batch: 675/3370	Loss: 1.086
Batch: 1012/3370	Loss: 0.912
Batch: 1349/3370	Loss: 0.929
Batch: 1686/3370	Loss: 0.918
Batch: 2023/3370	Loss: 1.097
Batch: 2360/3370	Loss: 0.893
Batch: 2697/3370	Loss: 1.337
Batch: 3034/3370	Loss: 1.079
Batch: 3370/3370	Loss: 0.887
Epoch: 13	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 14 ******************************
Batch: 1/3370	Loss: 0.886
Batch: 338/3370	Loss: 1.119
Batch: 675/3370	Loss: 1.283
Batch: 1012/3370	Loss: 1.069
Batch: 1349/3370	Loss: 1.076
Batch: 1686/3370	Loss: 1.122
Batch: 2023/3370	Loss: 0.886
Batch: 2360/3370	Loss: 1.082
Batch: 2697/3370	Loss: 1.093
Batch: 3034/3370	Loss: 1.282
Batch: 3370/3370	Loss: 0.898
Epoch: 14	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 15 ******************************
Batch: 1/3370	Loss: 1.525
Batch: 338/3370	Loss: 0.919
Batch: 675/3370	Loss: 1.225
Batch: 1012/3370	Loss: 0.899
Batch: 1349/3370	Loss: 0.874
Batch: 1686/3370	Loss: 1.049
Batch: 2023/3370	Loss: 0.898
Batch: 2360/3370	Loss: 1.310
Batch: 2697/3370	Loss: 1.102
Batch: 3034/3370	Loss: 1.092
Batch: 3370/3370	Loss: 0.887
Epoch: 15	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

****************************** Epoch: 16 ******************************
Batch: 1/3370	Loss: 0.902
Batch: 338/3370	Loss: 1.104
Batch: 675/3370	Loss: 0.884
Batch: 1012/3370	Loss: 1.304
Batch: 1349/3370	Loss: 1.304
Batch: 1686/3370	Loss: 1.097
Batch: 2023/3370	Loss: 0.915
Batch: 2360/3370	Loss: 1.101
Batch: 2697/3370	Loss: 1.282
Batch: 3034/3370	Loss: 0.899
Batch: 3370/3370	Loss: 0.963
Epoch: 16	Train Loss: 1.048	Val F1: 0.166
Best Epoch: 1	Best Val F1: 0.166

Saving the best checkpoint....
Inference...
Test F1: 0.169	Test F1_Few: 0.170	Test F1_Zero: 0.168
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 1.437528295315638e-05, 'lr': 1.6351942547645442e-05, 'n_layers_freeze': 3, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/106	Loss: 1.091
Batch: 11/106	Loss: 1.059
Batch: 21/106	Loss: 1.077
Batch: 31/106	Loss: 1.025
Batch: 41/106	Loss: 1.023
Batch: 51/106	Loss: 1.013
Batch: 61/106	Loss: 1.043
Batch: 71/106	Loss: 1.025
Batch: 81/106	Loss: 0.977
Batch: 91/106	Loss: 0.997
Batch: 101/106	Loss: 1.013
Batch: 106/106	Loss: 1.012
Epoch: 1	Train Loss: 1.027	Val F1: 0.320
Best Epoch: 1	Best Val F1: 0.320

****************************** Epoch: 2 ******************************
Batch: 1/106	Loss: 1.014
Batch: 11/106	Loss: 0.884
Batch: 21/106	Loss: 0.827
Batch: 31/106	Loss: 0.745
Batch: 41/106	Loss: 0.758
Batch: 51/106	Loss: 0.636
Batch: 61/106	Loss: 0.751
Batch: 71/106	Loss: 0.644
Batch: 81/106	Loss: 0.555
Batch: 91/106	Loss: 0.651
Batch: 101/106	Loss: 0.770
Batch: 106/106	Loss: 0.575
Epoch: 2	Train Loss: 0.734	Val F1: 0.728
Best Epoch: 2	Best Val F1: 0.728

****************************** Epoch: 3 ******************************
Batch: 1/106	Loss: 0.633
Batch: 11/106	Loss: 0.616
Batch: 21/106	Loss: 0.588
Batch: 31/106	Loss: 0.568
Batch: 41/106	Loss: 0.620
Batch: 51/106	Loss: 0.608
Batch: 61/106	Loss: 0.594
Batch: 71/106	Loss: 0.578
Batch: 81/106	Loss: 0.529
Batch: 91/106	Loss: 0.621
Batch: 101/106	Loss: 0.600
Batch: 106/106	Loss: 0.582
Epoch: 3	Train Loss: 0.589	Val F1: 0.745
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 4 ******************************
Batch: 1/106	Loss: 0.469
Batch: 11/106	Loss: 0.552
Batch: 21/106	Loss: 0.612
Batch: 31/106	Loss: 0.540
Batch: 41/106	Loss: 0.501
Batch: 51/106	Loss: 0.561
Batch: 61/106	Loss: 0.532
Batch: 71/106	Loss: 0.606
Batch: 81/106	Loss: 0.562
Batch: 91/106	Loss: 0.654
Batch: 101/106	Loss: 0.540
Batch: 106/106	Loss: 0.643
Epoch: 4	Train Loss: 0.546	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 5 ******************************
Batch: 1/106	Loss: 0.526
Batch: 11/106	Loss: 0.496
Batch: 21/106	Loss: 0.492
Batch: 31/106	Loss: 0.412
Batch: 41/106	Loss: 0.507
Batch: 51/106	Loss: 0.502
Batch: 61/106	Loss: 0.429
Batch: 71/106	Loss: 0.538
Batch: 81/106	Loss: 0.534
Batch: 91/106	Loss: 0.509
Batch: 101/106	Loss: 0.610
Batch: 106/106	Loss: 0.605
Epoch: 5	Train Loss: 0.503	Val F1: 0.740
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 6 ******************************
Batch: 1/106	Loss: 0.558
Batch: 11/106	Loss: 0.531
Batch: 21/106	Loss: 0.539
Batch: 31/106	Loss: 0.552
Batch: 41/106	Loss: 0.477
Batch: 51/106	Loss: 0.409
Batch: 61/106	Loss: 0.447
Batch: 71/106	Loss: 0.458
Batch: 81/106	Loss: 0.422
Batch: 91/106	Loss: 0.426
Batch: 101/106	Loss: 0.472
Batch: 106/106	Loss: 0.314
Epoch: 6	Train Loss: 0.467	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 7 ******************************
Batch: 1/106	Loss: 0.461
Batch: 11/106	Loss: 0.475
Batch: 21/106	Loss: 0.474
Batch: 31/106	Loss: 0.410
Batch: 41/106	Loss: 0.374
Batch: 51/106	Loss: 0.379
Batch: 61/106	Loss: 0.527
Batch: 71/106	Loss: 0.491
Batch: 81/106	Loss: 0.470
Batch: 91/106	Loss: 0.421
Batch: 101/106	Loss: 0.459
Batch: 106/106	Loss: 0.501
Epoch: 7	Train Loss: 0.423	Val F1: 0.727
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 8 ******************************
Batch: 1/106	Loss: 0.350
Batch: 11/106	Loss: 0.508
Batch: 21/106	Loss: 0.336
Batch: 31/106	Loss: 0.354
Batch: 41/106	Loss: 0.470
Batch: 51/106	Loss: 0.386
Batch: 61/106	Loss: 0.363
Batch: 71/106	Loss: 0.419
Batch: 81/106	Loss: 0.452
Batch: 91/106	Loss: 0.342
Batch: 101/106	Loss: 0.408
Batch: 106/106	Loss: 0.510
Epoch: 8	Train Loss: 0.400	Val F1: 0.729
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 9 ******************************
Batch: 1/106	Loss: 0.355
Batch: 11/106	Loss: 0.318
Batch: 21/106	Loss: 0.309
Batch: 31/106	Loss: 0.382
Batch: 41/106	Loss: 0.361
Batch: 51/106	Loss: 0.396
Batch: 61/106	Loss: 0.382
Batch: 71/106	Loss: 0.413
Batch: 81/106	Loss: 0.367
Batch: 91/106	Loss: 0.394
Batch: 101/106	Loss: 0.352
Batch: 106/106	Loss: 0.341
Epoch: 9	Train Loss: 0.374	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 10 ******************************
Batch: 1/106	Loss: 0.329
Batch: 11/106	Loss: 0.282
Batch: 21/106	Loss: 0.289
Batch: 31/106	Loss: 0.435
Batch: 41/106	Loss: 0.332
Batch: 51/106	Loss: 0.354
Batch: 61/106	Loss: 0.412
Batch: 71/106	Loss: 0.260
Batch: 81/106	Loss: 0.330
Batch: 91/106	Loss: 0.254
Batch: 101/106	Loss: 0.330
Batch: 106/106	Loss: 0.225
Epoch: 10	Train Loss: 0.345	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 11 ******************************
Batch: 1/106	Loss: 0.341
Batch: 11/106	Loss: 0.287
Batch: 21/106	Loss: 0.295
Batch: 31/106	Loss: 0.308
Batch: 41/106	Loss: 0.341
Batch: 51/106	Loss: 0.357
Batch: 61/106	Loss: 0.230
Batch: 71/106	Loss: 0.250
Batch: 81/106	Loss: 0.282
Batch: 91/106	Loss: 0.306
Batch: 101/106	Loss: 0.364
Batch: 106/106	Loss: 0.318
Epoch: 11	Train Loss: 0.325	Val F1: 0.728
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 12 ******************************
Batch: 1/106	Loss: 0.248
Batch: 11/106	Loss: 0.267
Batch: 21/106	Loss: 0.318
Batch: 31/106	Loss: 0.327
Batch: 41/106	Loss: 0.312
Batch: 51/106	Loss: 0.297
Batch: 61/106	Loss: 0.261
Batch: 71/106	Loss: 0.363
Batch: 81/106	Loss: 0.233
Batch: 91/106	Loss: 0.254
Batch: 101/106	Loss: 0.327
Batch: 106/106	Loss: 0.238
Epoch: 12	Train Loss: 0.290	Val F1: 0.719
Best Epoch: 3	Best Val F1: 0.745

****************************** Epoch: 13 ******************************
Batch: 1/106	Loss: 0.299
Batch: 11/106	Loss: 0.320
Batch: 21/106	Loss: 0.230
Batch: 31/106	Loss: 0.236
Batch: 41/106	Loss: 0.327
Batch: 51/106	Loss: 0.309
Batch: 61/106	Loss: 0.215
Batch: 71/106	Loss: 0.240
Batch: 81/106	Loss: 0.391
Batch: 91/106	Loss: 0.234
Batch: 101/106	Loss: 0.425
Batch: 106/106	Loss: 0.269
Epoch: 13	Train Loss: 0.276	Val F1: 0.710
Best Epoch: 3	Best Val F1: 0.745

Saving the best checkpoint....
Inference...
Test F1: 0.745	Test F1_Few: 0.740	Test F1_Zero: 0.751
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 4.434033172753466e-05, 'lr': 8.12936395143613e-06, 'n_layers_freeze': 1, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Batch: 1/106	Loss: 1.091
Batch: 11/106	Loss: 1.068
Batch: 21/106	Loss: 1.074
Batch: 31/106	Loss: 1.035
Batch: 41/106	Loss: 1.033
Batch: 51/106	Loss: 1.025
Batch: 61/106	Loss: 1.067
Batch: 71/106	Loss: 1.061
Batch: 81/106	Loss: 1.014
Batch: 91/106	Loss: 1.010
Batch: 101/106	Loss: 1.036
Batch: 106/106	Loss: 1.022
Epoch: 1	Train Loss: 1.043	Val F1: 0.308
Best Epoch: 1	Best Val F1: 0.308

****************************** Epoch: 2 ******************************
Batch: 1/106	Loss: 1.041
Batch: 11/106	Loss: 0.982
Batch: 21/106	Loss: 1.015
Batch: 31/106	Loss: 0.985
Batch: 41/106	Loss: 0.971
Batch: 51/106	Loss: 0.976
Batch: 61/106	Loss: 1.011
Batch: 71/106	Loss: 1.000
Batch: 81/106	Loss: 0.867
Batch: 91/106	Loss: 0.798
Batch: 101/106	Loss: 0.820
Batch: 106/106	Loss: 0.703
Epoch: 2	Train Loss: 0.943	Val F1: 0.719
Best Epoch: 2	Best Val F1: 0.719

****************************** Epoch: 3 ******************************
Batch: 1/106	Loss: 0.717
Batch: 11/106	Loss: 0.712
Batch: 21/106	Loss: 0.679
Batch: 31/106	Loss: 0.637
Batch: 41/106	Loss: 0.680
Batch: 51/106	Loss: 0.680
Batch: 61/106	Loss: 0.668
Batch: 71/106	Loss: 0.661
Batch: 81/106	Loss: 0.584
Batch: 91/106	Loss: 0.620
Batch: 101/106	Loss: 0.619
Batch: 106/106	Loss: 0.674
Epoch: 3	Train Loss: 0.660	Val F1: 0.726
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 4 ******************************
Batch: 1/106	Loss: 0.548
Batch: 11/106	Loss: 0.654
Batch: 21/106	Loss: 0.715
Batch: 31/106	Loss: 0.581
Batch: 41/106	Loss: 0.573
Batch: 51/106	Loss: 0.577
Batch: 61/106	Loss: 0.504
Batch: 71/106	Loss: 0.622
Batch: 81/106	Loss: 0.578
Batch: 91/106	Loss: 0.682
Batch: 101/106	Loss: 0.555
Batch: 106/106	Loss: 0.676
Epoch: 4	Train Loss: 0.591	Val F1: 0.718
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 5 ******************************
Batch: 1/106	Loss: 0.569
Batch: 11/106	Loss: 0.536
Batch: 21/106	Loss: 0.563
Batch: 31/106	Loss: 0.463
Batch: 41/106	Loss: 0.554
Batch: 51/106	Loss: 0.547
Batch: 61/106	Loss: 0.444
Batch: 71/106	Loss: 0.572
Batch: 81/106	Loss: 0.532
Batch: 91/106	Loss: 0.525
Batch: 101/106	Loss: 0.684
Batch: 106/106	Loss: 0.635
Epoch: 5	Train Loss: 0.545	Val F1: 0.722
Best Epoch: 3	Best Val F1: 0.726

****************************** Epoch: 6 ******************************
Batch: 1/106	Loss: 0.570
Batch: 11/106	Loss: 0.575
Batch: 21/106	Loss: 0.586
Batch: 31/106	Loss: 0.567
Batch: 41/106	Loss: 0.530
Batch: 51/106	Loss: 0.494
Batch: 61/106	Loss: 0.517
Batch: 71/106	Loss: 0.527
Batch: 81/106	Loss: 0.517
Batch: 91/106	Loss: 0.433
Batch: 101/106	Loss: 0.490
Batch: 106/106	Loss: 0.321
Epoch: 6	Train Loss: 0.517	Val F1: 0.745
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 7 ******************************
Batch: 1/106	Loss: 0.537
Batch: 11/106	Loss: 0.570
Batch: 21/106	Loss: 0.522
Batch: 31/106	Loss: 0.496
Batch: 41/106	Loss: 0.420
Batch: 51/106	Loss: 0.438
Batch: 61/106	Loss: 0.562
Batch: 71/106	Loss: 0.555
Batch: 81/106	Loss: 0.606
Batch: 91/106	Loss: 0.423
Batch: 101/106	Loss: 0.493
Batch: 106/106	Loss: 0.520
Epoch: 7	Train Loss: 0.483	Val F1: 0.702
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 8 ******************************
Batch: 1/106	Loss: 0.386
Batch: 11/106	Loss: 0.569
Batch: 21/106	Loss: 0.384
Batch: 31/106	Loss: 0.447
Batch: 41/106	Loss: 0.545
Batch: 51/106	Loss: 0.452
Batch: 61/106	Loss: 0.374
Batch: 71/106	Loss: 0.515
Batch: 81/106	Loss: 0.485
Batch: 91/106	Loss: 0.359
Batch: 101/106	Loss: 0.469
Batch: 106/106	Loss: 0.631
Epoch: 8	Train Loss: 0.460	Val F1: 0.714
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 9 ******************************
Batch: 1/106	Loss: 0.442
Batch: 11/106	Loss: 0.378
Batch: 21/106	Loss: 0.382
Batch: 31/106	Loss: 0.479
Batch: 41/106	Loss: 0.420
Batch: 51/106	Loss: 0.510
Batch: 61/106	Loss: 0.521
Batch: 71/106	Loss: 0.459
Batch: 81/106	Loss: 0.418
Batch: 91/106	Loss: 0.421
Batch: 101/106	Loss: 0.390
Batch: 106/106	Loss: 0.445
Epoch: 9	Train Loss: 0.438	Val F1: 0.729
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 10 ******************************
Batch: 1/106	Loss: 0.397
Batch: 11/106	Loss: 0.425
Batch: 21/106	Loss: 0.356
Batch: 31/106	Loss: 0.461
Batch: 41/106	Loss: 0.345
Batch: 51/106	Loss: 0.526
Batch: 61/106	Loss: 0.434
Batch: 71/106	Loss: 0.298
Batch: 81/106	Loss: 0.391
Batch: 91/106	Loss: 0.328
Batch: 101/106	Loss: 0.414
Batch: 106/106	Loss: 0.261
Epoch: 10	Train Loss: 0.415	Val F1: 0.726
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 11 ******************************
Batch: 1/106	Loss: 0.349
Batch: 11/106	Loss: 0.365
Batch: 21/106	Loss: 0.287
Batch: 31/106	Loss: 0.403
Batch: 41/106	Loss: 0.499
Batch: 51/106	Loss: 0.480
Batch: 61/106	Loss: 0.280
Batch: 71/106	Loss: 0.385
Batch: 81/106	Loss: 0.332
Batch: 91/106	Loss: 0.439
Batch: 101/106	Loss: 0.408
Batch: 106/106	Loss: 0.359
Epoch: 11	Train Loss: 0.397	Val F1: 0.734
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 12 ******************************
Batch: 1/106	Loss: 0.332
Batch: 11/106	Loss: 0.366
Batch: 21/106	Loss: 0.399
Batch: 31/106	Loss: 0.384
Batch: 41/106	Loss: 0.414
Batch: 51/106	Loss: 0.386
Batch: 61/106	Loss: 0.341
Batch: 71/106	Loss: 0.376
Batch: 81/106	Loss: 0.326
Batch: 91/106	Loss: 0.362
Batch: 101/106	Loss: 0.398
Batch: 106/106	Loss: 0.310
Epoch: 12	Train Loss: 0.375	Val F1: 0.724
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 13 ******************************
Batch: 1/106	Loss: 0.370
Batch: 11/106	Loss: 0.426
Batch: 21/106	Loss: 0.322
Batch: 31/106	Loss: 0.322
Batch: 41/106	Loss: 0.432
Batch: 51/106	Loss: 0.339
Batch: 61/106	Loss: 0.372
Batch: 71/106	Loss: 0.312
Batch: 81/106	Loss: 0.481
Batch: 91/106	Loss: 0.332
Batch: 101/106	Loss: 0.539
Batch: 106/106	Loss: 0.368
Epoch: 13	Train Loss: 0.364	Val F1: 0.719
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 14 ******************************
Batch: 1/106	Loss: 0.335
Batch: 11/106	Loss: 0.319
Batch: 21/106	Loss: 0.309
Batch: 31/106	Loss: 0.275
Batch: 41/106	Loss: 0.310
Batch: 51/106	Loss: 0.342
Batch: 61/106	Loss: 0.341
Batch: 71/106	Loss: 0.296
Batch: 81/106	Loss: 0.344
Batch: 91/106	Loss: 0.407
Batch: 101/106	Loss: 0.334
Batch: 106/106	Loss: 0.588
Epoch: 14	Train Loss: 0.343	Val F1: 0.703
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 15 ******************************
Batch: 1/106	Loss: 0.409
Batch: 11/106	Loss: 0.294
Batch: 21/106	Loss: 0.397
Batch: 31/106	Loss: 0.295
Batch: 41/106	Loss: 0.208
Batch: 51/106	Loss: 0.412
Batch: 61/106	Loss: 0.277
Batch: 71/106	Loss: 0.345
Batch: 81/106	Loss: 0.298
Batch: 91/106	Loss: 0.256
Batch: 101/106	Loss: 0.409
Batch: 106/106	Loss: 0.270
Epoch: 15	Train Loss: 0.332	Val F1: 0.702
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 16 ******************************
Batch: 1/106	Loss: 0.337
Batch: 11/106	Loss: 0.396
Batch: 21/106	Loss: 0.323
Batch: 31/106	Loss: 0.298
Batch: 41/106	Loss: 0.364
Batch: 51/106	Loss: 0.324
Batch: 61/106	Loss: 0.377
Batch: 71/106	Loss: 0.307
Batch: 81/106	Loss: 0.337
Batch: 91/106	Loss: 0.389
Batch: 101/106	Loss: 0.309
Batch: 106/106	Loss: 0.529
Epoch: 16	Train Loss: 0.313	Val F1: 0.713
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 17 ******************************
Batch: 1/106	Loss: 0.331
Batch: 11/106	Loss: 0.317
Batch: 21/106	Loss: 0.244
Batch: 31/106	Loss: 0.346
Batch: 41/106	Loss: 0.388
Batch: 51/106	Loss: 0.264
Batch: 61/106	Loss: 0.288
Batch: 71/106	Loss: 0.303
Batch: 81/106	Loss: 0.284
Batch: 91/106	Loss: 0.294
Batch: 101/106	Loss: 0.238
Batch: 106/106	Loss: 0.298
Epoch: 17	Train Loss: 0.295	Val F1: 0.726
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 18 ******************************
Batch: 1/106	Loss: 0.281
Batch: 11/106	Loss: 0.243
Batch: 21/106	Loss: 0.223
Batch: 31/106	Loss: 0.256
Batch: 41/106	Loss: 0.293
Batch: 51/106	Loss: 0.228
Batch: 61/106	Loss: 0.217
Batch: 71/106	Loss: 0.298
Batch: 81/106	Loss: 0.291
Batch: 91/106	Loss: 0.290
Batch: 101/106	Loss: 0.296
Batch: 106/106	Loss: 0.454
Epoch: 18	Train Loss: 0.293	Val F1: 0.726
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 19 ******************************
Batch: 1/106	Loss: 0.260
Batch: 11/106	Loss: 0.328
Batch: 21/106	Loss: 0.265
Batch: 31/106	Loss: 0.212
Batch: 41/106	Loss: 0.370
Batch: 51/106	Loss: 0.259
Batch: 61/106	Loss: 0.406
Batch: 71/106	Loss: 0.293
Batch: 81/106	Loss: 0.246
Batch: 91/106	Loss: 0.351
Batch: 101/106	Loss: 0.257
Batch: 106/106	Loss: 0.232
Epoch: 19	Train Loss: 0.279	Val F1: 0.719
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 20 ******************************
Batch: 1/106	Loss: 0.235
Batch: 11/106	Loss: 0.246
Batch: 21/106	Loss: 0.268
Batch: 31/106	Loss: 0.183
Batch: 41/106	Loss: 0.215
Batch: 51/106	Loss: 0.188
Batch: 61/106	Loss: 0.226
Batch: 71/106	Loss: 0.287
Batch: 81/106	Loss: 0.301
Batch: 91/106	Loss: 0.275
Batch: 101/106	Loss: 0.254
Batch: 106/106	Loss: 0.227
Epoch: 20	Train Loss: 0.257	Val F1: 0.719
Best Epoch: 6	Best Val F1: 0.745

****************************** Epoch: 21 ******************************
Batch: 1/106	Loss: 0.241
Batch: 11/106	Loss: 0.177
Batch: 21/106	Loss: 0.234
Batch: 31/106	Loss: 0.248
Batch: 41/106	Loss: 0.237
Batch: 51/106	Loss: 0.318
Batch: 61/106	Loss: 0.241
Batch: 71/106	Loss: 0.271
Batch: 81/106	Loss: 0.228
Batch: 91/106	Loss: 0.208
Batch: 101/106	Loss: 0.168
Batch: 106/106	Loss: 0.187
Epoch: 21	Train Loss: 0.252	Val F1: 0.731
Best Epoch: 6	Best Val F1: 0.745

Saving the best checkpoint....
Inference...
Test F1: 0.732	Test F1_Few: 0.735	Test F1_Zero: 0.730
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 1.3553048023825074e-05, 'lr': 5.389384726339378e-06, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
****************************** Epoch: 1 ******************************
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 5.378536752028828e-05, 'lr': 2.4824335680138062e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 8.534944535288185e-05, 'lr': 7.310939918407751e-06, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 3.5083475746921346e-05, 'lr': 7.506492088838504e-06, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 5.453762379514319e-05, 'lr': 7.76086292392101e-06, 'n_layers_freeze': 1, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 25, 'l2_reg': 2.7745588704973584e-05, 'lr': 1.631123887121325e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 5.412874318000286e-05, 'lr': 1.6972231469040918e-05, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 25, 'l2_reg': 9.088765133422395e-05, 'lr': 3.8240681946957705e-05, 'n_layers_freeze': 1, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 50, 'l2_reg': 6.657256690053658e-05, 'lr': 5.521209579453673e-06, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 9.965958906012944e-05, 'lr': 3.979281687067081e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 8.643213318824036e-05, 'lr': 2.252259298186523e-05, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 50, 'l2_reg': 9.09951323520668e-05, 'lr': 2.6027074095902408e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 50, 'l2_reg': 2.0087139097913457e-05, 'lr': 2.9697840259034332e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 3.769127702547605e-05, 'lr': 2.4204228134810827e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 50, 'l2_reg': 7.525122728956934e-05, 'lr': 4.094089037262557e-05, 'n_layers_freeze': 2, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 8.20548967071845e-05, 'lr': 1.3105694749699105e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 25, 'l2_reg': 8.110277182898062e-05, 'lr': 4.263459496235173e-05, 'n_layers_freeze': 1, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 5.49728991983748e-05, 'lr': 2.7942188131684996e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 7.988976927140796e-05, 'lr': 2.9824556762008744e-05, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 2.0995160159668884e-05, 'lr': 7.112374665834755e-06, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 50, 'l2_reg': 8.892858314363998e-05, 'lr': 1.5450024353488782e-05, 'n_layers_freeze': 1, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 5.3996770466089185e-05, 'lr': 2.861624739920091e-05, 'n_layers_freeze': 3, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 9.442934479064562e-05, 'lr': 4.2697383315439666e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 8.762499551841136e-05, 'lr': 3.873719435945431e-05, 'n_layers_freeze': 1, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 9.508209731411314e-05, 'lr': 3.7320858029099616e-05, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 7.154757717697567e-05, 'lr': 2.0023394109685145e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 3.458657117781888e-05, 'lr': 2.2277222082437015e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 6.4516314257963e-05, 'lr': 4.0302538765320024e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 8.101552068190298e-05, 'lr': 4.598691815304714e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 6.610101646539949e-05, 'lr': 3.592422970007838e-05, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 25, 'l2_reg': 4.223595274974337e-05, 'lr': 4.8758380411120705e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 4.9703772224031155e-05, 'lr': 3.896892923632482e-05, 'n_layers_freeze': 3, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 9.599832565307095e-05, 'lr': 4.487897448504871e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 8.804867745655106e-05, 'lr': 2.51815865937778e-05, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 25, 'l2_reg': 8.730647716160163e-05, 'lr': 1.5856642071656275e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 75, 'l2_reg': 3.2521436887605936e-05, 'lr': 2.3887721291412337e-05, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 6.948251635593858e-05, 'lr': 7.054067542193316e-06, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 50, 'l2_reg': 3.742581291248168e-05, 'lr': 3.582381504568585e-05, 'n_layers_freeze': 4, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 1.4119428059123077e-05, 'lr': 3.871980945577228e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 25, 'l2_reg': 8.700993836767399e-05, 'lr': 5.335189223120284e-06, 'n_layers_freeze': 4, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 9.715668273170954e-05, 'lr': 4.0920148954538165e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 5.8806092059456886e-05, 'lr': 2.9549792007571815e-05, 'n_layers_freeze': 1, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 1.4027681446049613e-05, 'lr': 2.5255639784329472e-05, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 25, 'l2_reg': 6.436577637956605e-05, 'lr': 1.6500878439841133e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 6.810362910336849e-05, 'lr': 2.2628116732256508e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 1.4765494775394128e-05, 'lr': 1.4177698479903888e-05, 'n_layers_freeze': 4, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 9.595878399883756e-05, 'lr': 2.039873960706727e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 9.135790050706343e-05, 'lr': 5.956167912815196e-06, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 25, 'l2_reg': 8.137752952889594e-05, 'lr': 4.430377540189464e-05, 'n_layers_freeze': 3, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 8.454867862990377e-05, 'lr': 1.8752524551986987e-05, 'n_layers_freeze': 2, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 50, 'l2_reg': 8.449147445379052e-05, 'lr': 3.821703559540823e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 8.143411051482276e-05, 'lr': 4.085995929992756e-05, 'n_layers_freeze': 2, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 3.97488537567464e-05, 'lr': 4.45293780279408e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 5.1542914484537834e-05, 'lr': 2.2631791882810932e-05, 'n_layers_freeze': 3, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 50, 'l2_reg': 4.643049782979197e-05, 'lr': 2.6556345177768544e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 25, 'l2_reg': 6.045208803534989e-05, 'lr': 2.1024864409713564e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 25, 'l2_reg': 7.246006920923282e-05, 'lr': 1.7872555071562035e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 2.164442650038909e-05, 'lr': 2.590173518803814e-05, 'n_layers_freeze': 2, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 4.754465867554969e-05, 'lr': 8.807284273096733e-06, 'n_layers_freeze': 0, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 25, 'l2_reg': 7.426209799335307e-05, 'lr': 9.67069784475196e-06, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 75, 'l2_reg': 3.646564513294047e-05, 'lr': 3.006596483749496e-05, 'n_layers_freeze': 3, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 50, 'l2_reg': 9.9216227935587e-05, 'lr': 3.2961566194487886e-05, 'n_layers_freeze': 1, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 5.3516965763974035e-05, 'lr': 4.102605135415334e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 7.122999949737384e-05, 'lr': 2.303969481147342e-05, 'n_layers_freeze': 1, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 9.434118732215724e-05, 'lr': 8.625789501737118e-06, 'n_layers_freeze': 1, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 25, 'l2_reg': 6.316025649590111e-05, 'lr': 3.993481379702697e-05, 'n_layers_freeze': 4, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 32, 'epochs': 50, 'l2_reg': 1.3765520751389116e-05, 'lr': 4.384536951029869e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 75, 'l2_reg': 8.187582561500884e-05, 'lr': 2.9271606214178585e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 3.2776317499681676e-05, 'lr': 2.4248963852706343e-05, 'n_layers_freeze': 4, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 25, 'l2_reg': 8.800468185777216e-05, 'lr': 4.40446337395838e-05, 'n_layers_freeze': 2, 'patience': 5}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 75, 'l2_reg': 6.6934159660223e-05, 'lr': 3.7566645936384525e-05, 'n_layers_freeze': 3, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 75, 'l2_reg': 8.76680148538092e-05, 'lr': 3.689929931417024e-05, 'n_layers_freeze': 0, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 4, 'epochs': 75, 'l2_reg': 7.298661470006947e-05, 'lr': 3.2205677338186225e-05, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 64, 'epochs': 50, 'l2_reg': 2.719576210275753e-05, 'lr': 9.86754105818486e-06, 'n_layers_freeze': 4, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 128, 'epochs': 75, 'l2_reg': 3.019297413845636e-05, 'lr': 2.0778334321897196e-05, 'n_layers_freeze': 1, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 8, 'epochs': 75, 'l2_reg': 2.053526614820333e-05, 'lr': 2.3958839864575272e-05, 'n_layers_freeze': 1, 'patience': 10}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 50, 'l2_reg': 5.423071499791766e-05, 'lr': 1.618654131765119e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
Starting training with config: {'batch_size': 16, 'epochs': 75, 'l2_reg': 2.219425469197279e-05, 'lr': 3.551814138704574e-05, 'n_layers_freeze': 0, 'patience': 15}
Let's use 1 GPUs!
Preparing data....
Training data....
# of train examples: 13477
max len: 512, max len wiki: 1
Val data....
# of val examples: 2062
max len: 512, max len wiki: 1
Test data....
# of test examples: 3006
max len: 512, max len wiki: 1
Done

Initializing model....
