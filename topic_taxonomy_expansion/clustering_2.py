#!/usr/bin/env python
"""
Clustering - Second Level

This script performs clustering on the second-level subtopics generated by topic_generation_2.py.
It groups similar second-level subtopics together and generates representative labels for each 
cluster using GPT-4o-mini, creating the third level of the taxonomy.

The script:
1. Loads predicted second-level subtopics from step_3.csv
2. Computes embeddings for each subtopic using Sentence Transformers
3. Performs dimensionality reduction with UMAP
4. Clusters the reduced embeddings with HDBSCAN
5. Generates cluster labels with GPT-4o-mini
6. Outputs the clustering results to step_4.csv

Input:
- step_3.csv: Data with first and second-level subtopics

Output:
- step_4.csv: Contains policy areas, first-level subtopics, second-level subtopics, and source indices
- Visualization plots in the cluster_viz_2 directory
"""

import os
import pandas as pd
import numpy as np
import hdbscan
import umap
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import seaborn as sns
import matplotlib.pyplot as plt
from hdbscan.validity import validity_index
from openai import OpenAI
import api_keys
from collections import defaultdict, Counter
from tqdm.auto import tqdm
import torch.nn.functional as F
import torch

def enforce_majority_cluster(embeddings, cluster_labels, misc_embedding):
    """
    Ensures identical embeddings are assigned to the most frequent cluster they appeared in.
    
    Args:
        embeddings (numpy.ndarray): Array of embeddings
        cluster_labels (numpy.ndarray): Array of cluster labels
        misc_embedding (tuple): Embedding of the "Misc." label
    
    Returns:
        numpy.ndarray: Corrected cluster labels
    """
    embedding_dict = defaultdict(lambda: Counter())  # Maps embedding -> cluster frequency counter
    corrected_labels = np.copy(cluster_labels)

    # First pass: Count occurrences of each embedding in different clusters
    for i, emb in enumerate(embeddings):
        emb_tuple = tuple(emb)  # Convert to a hashable type
        embedding_dict[emb_tuple][cluster_labels[i]] += 1  # Count occurrences

    # Second pass: Reassign labels based on majority occurrence
    for i, emb in enumerate(embeddings):
        emb_tuple = tuple(emb)
        if len(embedding_dict[emb_tuple]) > 1:
            # Find the most frequent cluster for this embedding
            most_frequent_cluster = max(embedding_dict[emb_tuple], key=embedding_dict[emb_tuple].get)
            corrected_labels[i] = most_frequent_cluster  # Assign the most common cluster
        # remove misc
        if emb_tuple == misc_embedding:
            corrected_labels[i] = -1

    return corrected_labels

def compute_medoid(texts, embeddings):
    """
    Given a list of texts and their embeddings, compute the medoid text.
    The medoid is the text whose embedding is closest to the cluster centroid.
    
    Args:
        texts (list): List of text strings
        embeddings (numpy.ndarray): Array of embeddings for the texts
    
    Returns:
        str: The medoid text
    """
    centroid = np.mean(embeddings, axis=0, keepdims=True)
    similarities = cosine_similarity(embeddings, centroid).flatten()
    medoid_index = np.argmax(similarities)
    return texts[medoid_index]

def generate_cluster_label(subtopics, parent_topic, client, max_attempts=5):
    """
    Uses the OpenAI API (gpt-4o-mini) to generate a succinct label that represents
    the common theme among the given list of subtopics.
    
    Args:
        subtopics (list): List of subtopics in the cluster
        parent_topic (str): The parent topic (subtopic_1)
        client (OpenAI): OpenAI client instance
        max_attempts (int, optional): Maximum number of API call attempts
    
    Returns:
        str: Generated label for the cluster
    """
    system_message = (
        "You are a helpful assistant constructing a three-level topic taxonomy. "
        "When given a second-level parent topic and a list of topics of documents clustered under the parent topic, you must generate a concise and representative third-level topic (2-5 words) "
        "that describes the common topic of the document topics clustered under the parent topic.\n"
        "The generated third-level subtopic should be a distinct child topic but still distinct from the parent topic, and should be as specific as possible."
        "For example, a good three-level taxonomy would be 1. Defense; 2. Naval Expansion; 3. Asia-Pacific Naval Buildup. "
        "The outputted topic should have an inherent stance someone could take a for or against position on, e.g. Energy Subsidies instead of Energy Policy, and Federal Workforce Expansion instead of Federal Workforce Size. "
        "It must be in alignment with the political stance of its parent topic, so if the parent topic is Expanding Voter Access, two possible subtopics are Expanding Mail-In Voting and Removing Voter ID Laws."
    )

    prompt = (
        "Given the following parent topic and list of topics from documents clustered under a second-level parent topic, generate a third-level subtopic that is representative of "
        "the common topic the document topics represent. It should be as specific as possible, and a subtopic of the parent topic. It must be a policy controversy one can take a stance on (for or against). Output the topic as an inherently stanced issue, such as Expanding Refugee Visa Programs instead of Refugee Policies or Refugee Controversy. It must be in alignment with the political stance of its parent topic, so if the parent topic is Expanding Clean Energy, the subtopic should be Reducing Fossil Fuel Subsidies instead of Increasing Fossil Fuel Subsidies (wrong directional stance) or Fossil Fuel Subsidied (unstanced).\n\n"
        "Second-Level Parent Topic: " + parent_topic + "\n\n"
        "Clustered Document Topics: " + "; ".join(subtopics) + "\n\n"
        "Answer only with the generated third-level subtopic (2-5 words):\n\n"
    )

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]
    
    attempt = 0
    while attempt < max_attempts:
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
            )
            content = response.choices[0].message.content.strip()
            return content
        except Exception as e:
            print(f"Attempt {attempt+1} failed: {e}")
            attempt += 1
    return "Label not generated"


def main():
    """
    Main function to perform clustering on the second-level subtopics.
    """
    # UMAP and HDBSCAN parameters
    n_components = 5          # Dimensionality for UMAP reduction
    n_neighbors = 15          # Number of neighbors for UMAP
    min_cluster_size = 10     # Minimum cluster size for HDBSCAN
    
    # Statistics tracking
    weighted_score = 0
    doc_count = 0

    # Create directory for visualizations if it doesn't exist
    if not os.path.exists('cluster_viz_2'):
        os.makedirs('cluster_viz_2')

    # Initialize OpenAI client
    print("Initializing OpenAI client...")
    client = OpenAI(api_key=api_keys.OPENAI_API_KEY)

    # Load data from previous step
    print("Loading data from step_3.csv...")
    df = pd.read_csv('step_3.csv')
    original_length = len(df)
    
    # Convert matching_idx to string for consistent handling
    df['matching_idx'] = df['matching_idx'].astype(str)
    
    # Note: Commenting out the dropping of rows with no subtopic to maintain original logic
    # df = df.dropna(subset=['subtopic_2'])
    # print(f"{original_length - len(df)} rows dropped ({(original_length - len(df)) / original_length * 100:.2f}%) due to no predicted subtopic or policy area.")

    # Initialize the Sentence Transformer model
    print("Loading Sentence Transformer model...")
    model = SentenceTransformer('all-MiniLM-L6-v2').to('cuda')

    # Create source mapping (policy_area, subtopic_1, subtopic_2) -> list of matching_idx
    source_mapping = {}
    for _, row in df.iterrows():
        path = (row['policy_area'], row['subtopic_1'], row['subtopic_2'])
        # We convert the index to a string for later concatenation.
        source_mapping.setdefault(path, []).append(row['matching_idx'])

    # Generate embeddings for all unique subtopics
    print("Generating embeddings for subtopics...")
    embeddings = {}
    for subtopic in tqdm(df['subtopic_2'].unique(), desc="Generating embeddings"):
        embeddings[subtopic] = model.encode(subtopic, convert_to_numpy=True)
        # Normalize embedding
        tensor_emb = torch.tensor(embeddings[subtopic])
        normalized_emb = F.normalize(tensor_emb, p=2, dim=0)
        embeddings[subtopic] = normalized_emb.numpy()
    
    # Generate and normalize the "Misc." embedding
    embeddings["Misc."] = model.encode("Misc.", convert_to_numpy=True)
    tensor_misc = torch.tensor(embeddings["Misc."])
    normalized_misc = F.normalize(tensor_misc, p=2, dim=0)
    embeddings["Misc."] = normalized_misc.numpy()
    misc_embedding = tuple(embeddings["Misc."])

    # List to store cluster taxonomy information
    taxonomy = []

    # Process each policy area and first-level subtopic pair
    for policy_area in df['policy_area'].unique():
        new_df = df[df['policy_area'] == policy_area]
        for parent_topic in new_df['subtopic_1'].unique():
            print(f"\nProcessing parent topic: {parent_topic}")
            curr_doc_count = 0
            
            # Filter by current parent topic
            sub_df = new_df[new_df['subtopic_1'] == parent_topic]
            
            # Get unique predicted subtopics
            subtopics = sub_df['subtopic_2'].tolist()
            print(f"Total subtopics: {len(subtopics)}")
            
            # Skip if too few subtopics for clustering or if parent topic is "Misc."
            if len(subtopics) < n_components+2 or parent_topic=="Misc.":
                print("Too few subtopics or Misc. subtopics, skipping...")
                source_indices = set()
                for t in set(subtopics):
                    source_indices.update(source_mapping[(policy_area, parent_topic, t)])
                doc_count += len(source_indices)
                
                taxonomy.append({
                    'policy_area': policy_area,
                    'subtopic_1': parent_topic,
                    'subtopic_2': "" if parent_topic == "Misc." else "Misc.",
                    'cluster_length': len(subtopics),
                    'source_subtopics': "",
                    'source_indices': ";".join(source_indices)
                })
                continue

            # Get embeddings for the subtopics
            original_embeddings = np.array([embeddings[subtopic] for subtopic in subtopics])
            
            # Perform dimensionality reduction with UMAP
            print("Performing UMAP dimensionality reduction...")
            umap_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, metric="cosine")
            reduced_embeddings = umap_model.fit_transform(original_embeddings)
            d = reduced_embeddings.shape[1]  # Number of features after reduction
            
            # Cluster using HDBSCAN on the reduced embeddings
            print("Clustering with HDBSCAN...")
            clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)
            cluster_labels = clusterer.fit_predict(reduced_embeddings)
            cluster_labels = enforce_majority_cluster(original_embeddings, cluster_labels, misc_embedding)
            
            # Print clustering summary
            unique_labels = set(cluster_labels)
            num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            noise_count = list(cluster_labels).count(-1)
            print(f"Clusters (excluding noise): {num_clusters}, Noise points: {noise_count}")
            
            # Compute DBCV score to evaluate clustering quality
            try:
                dbcv_score = validity_index(
                    reduced_embeddings.astype(np.float64),
                    cluster_labels,
                    d=d
                )
                print(f"DBCV score: {dbcv_score}")
                weighted_score += dbcv_score * len(subtopics)
            except Exception as e:
                print(f"Failed to compute DBCV score: {e}")
                dbcv_score = float('nan')

            # Save UMAP projection visualization
            plt.figure(figsize=(20, 12))
            plt.scatter(
                reduced_embeddings[:, 0],
                reduced_embeddings[:, 1],
                c=cluster_labels,
                cmap='viridis',
                s=50,
                alpha=0.7
            )
            plt.colorbar(label='Cluster')
            policy_topic = f"{policy_area} - {parent_topic}"
            plt.title(f"UMAP projection of {policy_topic} subtopics (Clusters: {num_clusters})")
            plt.savefig(f"cluster_viz_2/{policy_area.replace('/', '_')}_{parent_topic.replace('/', '_')}_umap.png")
            plt.close()

            # Process each cluster
            cluster_count = 0
            for label in tqdm(sorted(list(set(cluster_labels))), desc="Processing clusters"):
                if label == -1:
                    # Handle noise points
                    indices = [i for i, x in enumerate(cluster_labels) if x == label]
                    noise_texts = [subtopics[i] for i in indices]
                    source_indices = set()
                    for t in noise_texts:
                        source_indices.update(source_mapping[(policy_area, parent_topic, t)])
                    curr_doc_count += len(source_indices)
                    
                    taxonomy.append({
                        'policy_area': policy_area,
                        'subtopic_1': parent_topic,
                        'subtopic_2': "Misc.",
                        'cluster_length': len(indices),
                        'source_subtopics': "; ".join(noise_texts),
                        'source_indices': ";".join(source_indices)
                    })
                    continue
                
                # Get texts and embeddings for this cluster
                indices = [i for i, x in enumerate(cluster_labels) if x == label]
                cluster_texts = [subtopics[i] for i in indices]
                
                # Generate cluster label with GPT
                gpt_label = generate_cluster_label(cluster_texts, parent_topic, client)
                print(f"Cluster {cluster_count} ({len(indices)} items): {gpt_label}")
                cluster_count += 1
                
                # Collect source indices for this cluster
                source_indices = set()
                for t in cluster_texts:
                    source_indices.update(source_mapping[(policy_area, parent_topic, t)])
                curr_doc_count += len(source_indices)
                
                # Save cluster information
                taxonomy.append({
                    'policy_area': policy_area,
                    'subtopic_1': parent_topic,
                    'subtopic_2': gpt_label,
                    'cluster_length': len(indices),
                    'source_subtopics': "; ".join(cluster_texts),
                    'source_indices': ";".join(source_indices)
                })
            
            doc_count += curr_doc_count
            print(f"Documents processed: {curr_doc_count}")

    # Save taxonomy to CSV
    taxonomy_df = pd.DataFrame(taxonomy)
    taxonomy_df.to_csv('step_4.csv', index=False)
    
    # Print final statistics
    if doc_count > 0:
        print(f"Average DBCV score (weighted): {weighted_score / doc_count}")
    print(f"Total documents processed: {doc_count}")
    print("Taxonomy saved to step_4.csv")

if __name__ == "__main__":
    main()